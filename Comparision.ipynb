{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "103f896e99c34773bde3bf5e19b67eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": false,
            "description": "Episode:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_114fbe1fa67d401aa1a356e8d67824db",
            "max": 999,
            "min": 0,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 1,
            "style": "IPY_MODEL_738c5d2a42b94dec9e1498ef2f1b9733",
            "value": 999
          }
        },
        "114fbe1fa67d401aa1a356e8d67824db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738c5d2a42b94dec9e1498ef2f1b9733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "10cf1cc989b048c78e0c66cf923d70f0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_36eb284f1e684cdf9ad5dc00a480839c",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "stream",
                "name": "stderr",
                "text": [
                  "/usr/local/lib/python3.11/dist-packages/ipywidgets/widgets/widget_output.py:111: DeprecationWarning: Kernel._parent_header is deprecated in ipykernel 6. Use .get_parent()\n",
                  "  if ip and hasattr(ip, 'kernel') and hasattr(ip.kernel, '_parent_header'):\n"
                ]
              },
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "<IPython.core.display.HTML object>",
                  "text/html": "\n<link rel=\"stylesheet\"\nhref=\"https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css\">\n<script language=\"javascript\">\n  function isInternetExplorer() {\n    ua = navigator.userAgent;\n    /* MSIE used to detect old browsers and Trident used to newer ones*/\n    return ua.indexOf(\"MSIE \") > -1 || ua.indexOf(\"Trident/\") > -1;\n  }\n\n  /* Define the Animation class */\n  function Animation(frames, img_id, slider_id, interval, loop_select_id){\n    this.img_id = img_id;\n    this.slider_id = slider_id;\n    this.loop_select_id = loop_select_id;\n    this.interval = interval;\n    this.current_frame = 0;\n    this.direction = 0;\n    this.timer = null;\n    this.frames = new Array(frames.length);\n\n    for (var i=0; i<frames.length; i++)\n    {\n     this.frames[i] = new Image();\n     this.frames[i].src = frames[i];\n    }\n    var slider = document.getElementById(this.slider_id);\n    slider.max = this.frames.length - 1;\n    if (isInternetExplorer()) {\n        // switch from oninput to onchange because IE <= 11 does not conform\n        // with W3C specification. It ignores oninput and onchange behaves\n        // like oninput. In contrast, Microsoft Edge behaves correctly.\n        slider.setAttribute('onchange', slider.getAttribute('oninput'));\n        slider.setAttribute('oninput', null);\n    }\n    this.set_frame(this.current_frame);\n  }\n\n  Animation.prototype.get_loop_state = function(){\n    var button_group = document[this.loop_select_id].state;\n    for (var i = 0; i < button_group.length; i++) {\n        var button = button_group[i];\n        if (button.checked) {\n            return button.value;\n        }\n    }\n    return undefined;\n  }\n\n  Animation.prototype.set_frame = function(frame){\n    this.current_frame = frame;\n    document.getElementById(this.img_id).src =\n            this.frames[this.current_frame].src;\n    document.getElementById(this.slider_id).value = this.current_frame;\n  }\n\n  Animation.prototype.next_frame = function()\n  {\n    this.set_frame(Math.min(this.frames.length - 1, this.current_frame + 1));\n  }\n\n  Animation.prototype.previous_frame = function()\n  {\n    this.set_frame(Math.max(0, this.current_frame - 1));\n  }\n\n  Animation.prototype.first_frame = function()\n  {\n    this.set_frame(0);\n  }\n\n  Animation.prototype.last_frame = function()\n  {\n    this.set_frame(this.frames.length - 1);\n  }\n\n  Animation.prototype.slower = function()\n  {\n    this.interval /= 0.7;\n    if(this.direction > 0){this.play_animation();}\n    else if(this.direction < 0){this.reverse_animation();}\n  }\n\n  Animation.prototype.faster = function()\n  {\n    this.interval *= 0.7;\n    if(this.direction > 0){this.play_animation();}\n    else if(this.direction < 0){this.reverse_animation();}\n  }\n\n  Animation.prototype.anim_step_forward = function()\n  {\n    this.current_frame += 1;\n    if(this.current_frame < this.frames.length){\n      this.set_frame(this.current_frame);\n    }else{\n      var loop_state = this.get_loop_state();\n      if(loop_state == \"loop\"){\n        this.first_frame();\n      }else if(loop_state == \"reflect\"){\n        this.last_frame();\n        this.reverse_animation();\n      }else{\n        this.pause_animation();\n        this.last_frame();\n      }\n    }\n  }\n\n  Animation.prototype.anim_step_reverse = function()\n  {\n    this.current_frame -= 1;\n    if(this.current_frame >= 0){\n      this.set_frame(this.current_frame);\n    }else{\n      var loop_state = this.get_loop_state();\n      if(loop_state == \"loop\"){\n        this.last_frame();\n      }else if(loop_state == \"reflect\"){\n        this.first_frame();\n        this.play_animation();\n      }else{\n        this.pause_animation();\n        this.first_frame();\n      }\n    }\n  }\n\n  Animation.prototype.pause_animation = function()\n  {\n    this.direction = 0;\n    if (this.timer){\n      clearInterval(this.timer);\n      this.timer = null;\n    }\n  }\n\n  Animation.prototype.play_animation = function()\n  {\n    this.pause_animation();\n    this.direction = 1;\n    var t = this;\n    if (!this.timer) this.timer = setInterval(function() {\n        t.anim_step_forward();\n    }, this.interval);\n  }\n\n  Animation.prototype.reverse_animation = function()\n  {\n    this.pause_animation();\n    this.direction = -1;\n    var t = this;\n    if (!this.timer) this.timer = setInterval(function() {\n        t.anim_step_reverse();\n    }, this.interval);\n  }\n</script>\n\n<style>\n.animation {\n    display: inline-block;\n    text-align: center;\n}\ninput[type=range].anim-slider {\n    width: 374px;\n    margin-left: auto;\n    margin-right: auto;\n}\n.anim-buttons {\n    margin: 8px 0px;\n}\n.anim-buttons button {\n    padding: 0;\n    width: 36px;\n}\n.anim-state label {\n    margin-right: 8px;\n}\n.anim-state input {\n    margin: 0;\n    vertical-align: middle;\n}\n</style>\n\n<div class=\"animation\">\n  <img id=\"_anim_imgf4556bc6aa314543bb7500385c371eeb\">\n  <div class=\"anim-controls\">\n    <input id=\"_anim_sliderf4556bc6aa314543bb7500385c371eeb\" type=\"range\" class=\"anim-slider\"\n           name=\"points\" min=\"0\" max=\"1\" step=\"1\" value=\"0\"\n           oninput=\"animf4556bc6aa314543bb7500385c371eeb.set_frame(parseInt(this.value));\">\n    <div class=\"anim-buttons\">\n      <button title=\"Decrease speed\" aria-label=\"Decrease speed\" onclick=\"animf4556bc6aa314543bb7500385c371eeb.slower()\">\n          <i class=\"fa fa-minus\"></i></button>\n      <button title=\"First frame\" aria-label=\"First frame\" onclick=\"animf4556bc6aa314543bb7500385c371eeb.first_frame()\">\n        <i class=\"fa fa-fast-backward\"></i></button>\n      <button title=\"Previous frame\" aria-label=\"Previous frame\" onclick=\"animf4556bc6aa314543bb7500385c371eeb.previous_frame()\">\n          <i class=\"fa fa-step-backward\"></i></button>\n      <button title=\"Play backwards\" aria-label=\"Play backwards\" onclick=\"animf4556bc6aa314543bb7500385c371eeb.reverse_animation()\">\n          <i class=\"fa fa-play fa-flip-horizontal\"></i></button>\n      <button title=\"Pause\" aria-label=\"Pause\" onclick=\"animf4556bc6aa314543bb7500385c371eeb.pause_animation()\">\n          <i class=\"fa fa-pause\"></i></button>\n      <button title=\"Play\" aria-label=\"Play\" onclick=\"animf4556bc6aa314543bb7500385c371eeb.play_animation()\">\n          <i class=\"fa fa-play\"></i></button>\n      <button title=\"Next frame\" aria-label=\"Next frame\" onclick=\"animf4556bc6aa314543bb7500385c371eeb.next_frame()\">\n          <i class=\"fa fa-step-forward\"></i></button>\n      <button title=\"Last frame\" aria-label=\"Last frame\" onclick=\"animf4556bc6aa314543bb7500385c371eeb.last_frame()\">\n          <i class=\"fa fa-fast-forward\"></i></button>\n      <button title=\"Increase speed\" aria-label=\"Increase speed\" onclick=\"animf4556bc6aa314543bb7500385c371eeb.faster()\">\n          <i class=\"fa fa-plus\"></i></button>\n    </div>\n    <form title=\"Repetition mode\" aria-label=\"Repetition mode\" action=\"#n\" name=\"_anim_loop_selectf4556bc6aa314543bb7500385c371eeb\"\n          class=\"anim-state\">\n      <input type=\"radio\" name=\"state\" value=\"once\" id=\"_anim_radio1_f4556bc6aa314543bb7500385c371eeb\"\n             >\n      <label for=\"_anim_radio1_f4556bc6aa314543bb7500385c371eeb\">Once</label>\n      <input type=\"radio\" name=\"state\" value=\"loop\" id=\"_anim_radio2_f4556bc6aa314543bb7500385c371eeb\"\n             checked>\n      <label for=\"_anim_radio2_f4556bc6aa314543bb7500385c371eeb\">Loop</label>\n      <input type=\"radio\" name=\"state\" value=\"reflect\" id=\"_anim_radio3_f4556bc6aa314543bb7500385c371eeb\"\n             >\n      <label for=\"_anim_radio3_f4556bc6aa314543bb7500385c371eeb\">Reflect</label>\n    </form>\n  </div>\n</div>\n\n\n<script language=\"javascript\">\n  /* Instantiate the Animation class. */\n  /* The IDs given should match those used in the template above. */\n  (function() {\n    var img_id = \"_anim_imgf4556bc6aa314543bb7500385c371eeb\";\n    var slider_id = \"_anim_sliderf4556bc6aa314543bb7500385c371eeb\";\n    var loop_select_id = \"_anim_loop_selectf4556bc6aa314543bb7500385c371eeb\";\n    var frames = new Array(2);\n    \n  frames[0] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAyAAAAMgCAYAAADbcAZoAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90\\\nbGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAP\\\nYQAAD2EBqD+naQAAUE5JREFUeJzt3XmYXvPh/+H3TJbJnliyyCKCEEES+9YSa4haiqrWTu20yk9b\\\nVVRpVW2t2n2pltZaFLXEWmvVrrEliJ0kyCaRbeb5/ZFmapogieQzkdz3dc0185xznnM+ZzJiXjlb\\\nVaVSqQQAAKCA6sYeAAAAsPgQIAAAQDECBAAAKEaAAAAAxQgQAACgGAECAAAUI0AAAIBiBAgAAFCM\\\nAAEAAIoRIAAAQDECBAAAKEaAAAAAxQgQAACgGAECAAAUI0AAAIBiBAgAAFCMAAEAAIoRIAAAQDEC\\\nBAAAKEaAAAAAxQgQAACgGAECAAAUI0AAAIBiBAgAAFCMAAEAAIoRIAAAQDECBAAAKEaAAAAAxQgQ\\\nAACgGAECAAAUI0AAAIBiBAgAAFCMAAEAAIoRIAAAQDECBAAAKEaAAAAAxQgQAACgGAECAAAUI0AA\\\nAIBiBAgAAFCMAAEAAIoRIAAAQDECBAAAKEaAAAAAxQgQAACgGAECAAAUI0AAAIBiBAgAAFCMAAEA\\\nAIoRIAAAQDECBAAAKEaAAAAAxQgQAACgGAECAAAUI0AAAIBiBAgAAFCMAAEAAIoRIAAAQDECBAAA\\\nKEaAAAAAxQgQAACgGAECAAAUI0AAAIBiBAgAAFCMAAEAAIoRIAAAQDECBAAAKEaAAAAAxQgQAACg\\\nGAECAAAUI0AAAIBiBAgAAFCMAAEAAIoRIACLsKqqqhx++OELfDu/+c1v0qdPn9TV1S2wbbz++uup\\\nqqrK5ZdfvsC2MTsDBw7MwIEDi25zYfCTn/wk6623XmMPA1gECRBgoXT55Zenqqqq/qNFixbp2rVr\\\nBg0alHPOOScTJkxo7CF+Kcstt1yD/evUqVO+/vWv58Ybb5zrdT3yyCP5+c9/nrFjx87/gc6B8ePH\\\n57TTTsuPf/zjVFf/938rn96///04+OCDG2WsC4OXX345P/zhD7PhhhumRYsWqaqqyuuvv/6Zy998\\\n881Zc80106JFiyy77LI58cQTM3369FmWGzt2bA488MB07NgxrVu3zqabbpqnnnpqntd55JFH5tln\\\nn83NN9/8pfYX4H81bewBAHyeX/ziF+nVq1emTZuW999/P/fff3+OPPLInHXWWbn55pvTr1+/xh7i\\\nPBswYECOPvroJMm7776biy66KDvttFMuuOCCufoF/ZFHHslJJ52UffbZJx06dFhAo/1sl112WaZP\\\nn57vfOc7s8zbcssts9dee80yfaWVVprr7fTs2TOffPJJmjVrNk/jXFg8+uijOeecc9K3b9+sssoq\\\neeaZZz5z2dtvvz077rhjBg4cmN///vf597//nVNOOSWjRo3KBRdcUL9cXV1dtt122zz77LM55phj\\\nsvTSS+f888/PwIED8+STT6Z3795zvc4uXbpkhx12yBlnnJHtt99+gXwvgMVUBWAh9Ic//KGSpPL4\\\n44/PMu+ee+6ptGzZstKzZ8/KpEmTPnc9H3/88YIa4pfSs2fPyrbbbttg2nvvvVdp3bp1ZaWVVpqr\\\ndZ1++umVJJURI0bMMi9J5bDDDvsyQ/1C/fr1q+yxxx6Nsu0SNtlkk8omm2wy39b34YcfVsaPH1+p\\\nVD7/z65SqVT69u1b6d+/f2XatGn104477rhKVVVV5cUXX6yfds0111SSVK677rr6aaNGjap06NCh\\\n8p3vfGee1lmpVCrXX399paqqqvLqq6/O8/4C/C+nYAFfOZtttlmOP/74vPHGG7nyyivrp++zzz5p\\\n06ZNXn311QwePDht27bN7rvvniSZOHFijj766PTo0SM1NTVZeeWVc8YZZ6RSqTRY98xrJm666aas\\\nttpqqampyaqrrpo77rhjlnG888472W+//dK5c+f65S677LJ53q8uXbpklVVWyYgRI5Ikzz33XPbZ\\\nZ58sv/zyadGiRbp06ZL99tsvH374Yf17fv7zn+eYY45JkvTq1av+FKf/PaXni/ZnwoQJOfLII7Pc\\\ncsulpqYmnTp1ypZbbvmZp/DMNGLEiDz33HPZYost5nm/Bw4cmNVWWy1PPvlkNtxww7Rs2TK9evXK\\\nhRde2GC52V0D8v7772ffffdN9+7dU1NTk2WWWSY77LDDLPt//vnnZ9VVV01NTU26du2aww47bLan\\\nrF188cVZYYUV0rJly6y77rp58MEHZzvmKVOm5MQTT8yKK66Ympqa9OjRIz/60Y8yZcqUL9zfJZdc\\\nMm3btv3C5V544YW88MILOfDAA9O06X9PWDj00ENTqVRy/fXX10+7/vrr07lz5+y000710zp27Jhd\\\nd901f/vb3+rHNTfrTFL/5/q3v/3tC8cLMKecggV8Je2555756U9/miFDhuSAAw6onz59+vQMGjQo\\\nX/va13LGGWekVatWqVQq2X777XPfffdl//33z4ABA3LnnXfmmGOOyTvvvJOzzz67wbofeuih3HDD\\\nDTn00EPTtm3bnHPOOdl5553z5ptvZqmllkqSjBw5Muuvv359sHTs2DG333579t9//4wfPz5HHnnk\\\nXO/TtGnT8tZbb9Vv46677sprr72WfffdN126dMnzzz+fiy++OM8//3z++c9/pqqqKjvttFOGDRuW\\\nq666KmeffXaWXnrpJDN++Zyb/Tn44INz/fXX5/DDD0/fvn3z4Ycf5qGHHsqLL76YNddc8zPH/Mgj\\\njyTJZy4zefLkfPDBB7NMb9euXZo3b17/esyYMRk8eHB23XXXfOc738m1116bQw45JM2bN89+++33\\\nmdvfeeed8/zzz+eII47Icsstl1GjRuWuu+7Km2++meWWWy7JjEg76aSTssUWW+SQQw7Jyy+/nAsu\\\nuCCPP/54Hn744fpTui699NIcdNBB2XDDDXPkkUfmtddey/bbb58ll1wyPXr0qN9mXV1dtt9++zz0\\\n0EM58MADs8oqq+Tf//53zj777AwbNiw33XTTZ453bjz99NNJkrXXXrvB9K5du6Z79+7182cuu+aa\\\naza4BidJ1l133Vx88cUZNmxYVl999blaZ5K0b98+K6ywQh5++OH88Ic/nC/7BeAULGCh9HmnYM3U\\\nvn37yhprrFH/eu+9964kqfzkJz9psNxNN91USVI55ZRTGkzfZZddKlVVVZVXXnmlflqSSvPmzRtM\\\ne/bZZytJKr///e/rp+2///6VZZZZpvLBBx80WOduu+1Wad++/ReeGtazZ8/KVlttVRk9enRl9OjR\\\nlWeffbay2267VZJUjjjiiEqlUpntOq666qpKksoDDzxQP+2LTsGak/1p3779PJ0u9bOf/aySpDJh\\\nwoTZbvuzPq666qr65TbZZJNKksqZZ55ZP23KlCmVAQMGVDp16lSZOnVqpVKpVEaMGFFJUvnDH/5Q\\\nqVQqlTFjxlSSVE4//fTPHN+oUaMqzZs3r2y11VaV2tra+unnnntuJUnlsssuq1QqlcrUqVMrnTp1\\\nqgwYMKAyZcqU+uUuvvjiSpIGp2BdccUVlerq6sqDDz7YYFsXXnhhJUnl4YcfnoPv3Ayf92c3c96b\\\nb745y7x11lmnsv7669e/bt26dWW//fabZbm///3vlSSVO+64Y67XOdNWW21VWWWVVeZ4nwC+iFOw\\\ngK+sNm3azPZuWIccckiD17fddluaNGmS73//+w2mH3300alUKrn99tsbTN9iiy2ywgor1L/u169f\\\n2rVrl9deey1JUqlU8te//jXbbbddKpVKPvjgg/qPQYMGZdy4cV946lKSDBkyJB07dkzHjh3Tv3//\\\nXHfdddlzzz1z2mmnJUlatmxZv+zMIwnrr79+kszR+ud0f5KkQ4cOeeyxx/Luu+/O8XqT5MMPP0zT\\\npk3Tpk2b2c7fYYcdctddd83ysemmmzZYrmnTpjnooIPqXzdv3jwHHXRQRo0alSeffHK2627ZsmWa\\\nN2+e+++/P2PGjJntMnfffXemTp2aI488ssHRgQMOOCDt2rXL3//+9yTJE088kVGjRuXggw9ucGRm\\\nn332Sfv27Rus87rrrssqq6ySPn36NPiz32yzzZIk991332d9u+bKJ598kiSpqamZZV6LFi3q589c\\\n9rOW+/S65madMy2xxBKzPYoFMK+cggV8ZX388cfp1KlTg2lNmzZN9+7dG0x744030rVr11nOu19l\\\nlVXq53/asssuO8u2llhiifpfckePHp2xY8fm4osvzsUXXzzbsY0aNeoLx7/eeuvllFNOSVVVVVq1\\\napVVVlmlwV2sPvroo5x00km5+uqrZ1nfuHHjvnD9c7o/yYzneOy9997p0aNH1lprrQwePDh77bVX\\\nll9++Tnezux07959jq4P6dq1a1q3bt1g2sw7Zb3++uv14fVpNTU1Oe2003L00Uenc+fOWX/99fON\\\nb3wje+21V7p06ZLkv3+2K6+8coP3Nm/ePMsvv3z9/JmfP323qCRp1qzZLN+D4cOH58UXX2xwmtun\\\nzcmf/ZyYGaCzu65k8uTJDQK1ZcuWn7ncp9c1N+ucqVKppKqqah72AGD2BAjwlfT2229n3LhxWXHF\\\nFRtMr6mpmeU8+LnVpEmT2U6v/OeC9ZkP29tjjz2y9957z3bZObk98NJLL/25v5zvuuuueeSRR3LM\\\nMcdkwIABadOmTerq6rL11lvP1QP/vmh/Zm5r5nNIhgwZktNPPz2nnXZabrjhhmyzzTafue6llloq\\\n06dPz4QJE+bowur57cgjj8x2222Xm266KXfeeWeOP/74nHrqqbn33nuzxhprLJBt1tXVZfXVV89Z\\\nZ5012/mfvl7ky1hmmWWSJO+9994s63zvvfey7rrrNlj2vffem2UdM6d17dp1rtc505gxY+qvLQKY\\\nHwQI8JV0xRVXJEkGDRr0hcv27Nkzd9999yy/JL/00kv18+dGx44d07Zt29TW1n6puz99njFjxuSe\\\ne+7JSSedlBNOOKF++vDhw2dZdn796/QyyyyTQw89NIceemhGjRqVNddcM7/85S8/N0D69OmTZMbd\\\nsL7MM1nefffdTJw4scFRkGHDhiVJ/cXkn2WFFVbI0UcfnaOPPjrDhw/PgAEDcuaZZ+bKK6+s/7N9\\\n+eWXGxzJmDp1akaMGFH/5zdzueHDh9efSpXMuDHAiBEj0r9//wbbe/bZZ7P55psv0CMDAwYMSDLj\\\n9LBPh8G7776bt99+OwceeGCDZR988MHU1dU1CPDHHnssrVq1qj+aNDfrnOl/9x/gy3INCPCVc++9\\\n9+bkk09Or1696m+z+3kGDx6c2tranHvuuQ2mn3322amqqvrcX7Bnp0mTJtl5553z17/+NUOHDp1l\\\n/ujRo+dqfZ+1jSSz3Cb4t7/97SzLzvylfV6fhF5bWzvLKV2dOnVK165dv/C2shtssEGSGb/QfhnT\\\np0/PRRddVP966tSpueiii9KxY8estdZas33PpEmT6k8xmmmFFVZI27Zt68e9xRZbpHnz5jnnnHMa\\\nfC8vvfTSjBs3Lttuu22SGXeF6tixYy688MJMnTq1frnLL798lu/rrrvumnfeeSeXXHLJLGP65JNP\\\nMnHixLnb+c+w6qqrpk+fPrn44otTW1tbP/2CCy5IVVVVdtlll/ppu+yyS0aOHJkbbrihftoHH3yQ\\\n6667Ltttt139NR9zs85kxql+r776ajbccMP5sk8AiSMgwELu9ttvz0svvZTp06dn5MiRuffee3PX\\\nXXelZ8+eufnmm+svsv082223XTbddNMcd9xxef3119O/f/8MGTIkf/vb33LkkUc2uEB7Tv3617/O\\\nfffdl/XWWy8HHHBA+vbtm48++ihPPfVU7r777nz00Ufzsrv12rVrl4033ji/+c1vMm3atHTr1i1D\\\nhgypf0bIp838Bf24447LbrvtlmbNmmW77bab5ZqKzzJhwoR07949u+yyS/r37582bdrk7rvvzuOP\\\nP54zzzzzc9+7/PLLZ7XVVsvdd98929vlDhs2rMGzWmbq3Llzttxyy/rXXbt2zWmnnZbXX389K620\\\nUq655po888wzufjiiz/zyefDhg3L5ptvnl133TV9+/ZN06ZNc+ONN2bkyJHZbbfdksw4WnXsscfm\\\npJNOytZbb53tt98+L7/8cs4///yss8462WOPPZLMuNbjlFNOyUEHHZTNNtss3/72tzNixIj84Q9/\\\nmOUakD333DPXXnttDj744Nx3333ZaKONUltbm5deeinXXntt7rzzzlluc/tp48aNy+9///skycMP\\\nP5wkOffcc9OhQ4d06NAhhx9+eP2yp59+erbffvtstdVW2W233TJ06NCce+65+d73vld/DVMyI0DW\\\nX3/97LvvvnnhhRfqn4ReW1ubk046qcH253SdyYyL+CuVSnbYYYfP3B+AudZo998C+Bwzb8M786N5\\\n8+aVLl26VLbccsvK7373u/onSX/a3nvvXWnduvVs1zdhwoTKD3/4w0rXrl0rzZo1q/Tu3bty+umn\\\nV+rq6hosl894enfPnj0re++9d4NpI0eOrBx22GGVHj16VJo1a1bp0qVLZfPNN69cfPHFX7h/s3sS\\\n+v96++23K9/85jcrHTp0qLRv377yrW99q/Luu+9WklROPPHEBsuefPLJlW7dulWqq6sb3NZ1TvZn\\\nypQplWOOOabSv3//Stu2bSutW7eu9O/fv3L++ed/4X5UKpXKWWedVWnTps0stw3O59yG99O3td1k\\\nk00qq666auWJJ56obLDBBpUWLVpUevbsWTn33HMbrO9/b8P7wQcfVA477LBKnz59Kq1bt660b9++\\\nst5661WuvfbaWcZ47rnnVvr06VNp1qxZpXPnzpVDDjmkMmbMmFmWO//88yu9evWq1NTUVNZee+3K\\\nAw88MNsnoU+dOrVy2mmnVVZdddVKTU1NZYkllqistdZalZNOOqkybty4z/1+zdyP2X307NlzluVv\\\nvPHGyoABAyo1NTWV7t27V372s5/V35r40z766KPK/vvvX1lqqaUqrVq1qmyyySafeRvrOV3nt7/9\\\n7crXvva1z90fgLlVVan8z/F9AJgL48aNy/LLL5/f/OY32X///ef6/QMHDswHH3ww29PZaDzvv/9+\\\nevXqlauvvtoREGC+cg0IAF9K+/bt86Mf/Sinn376XN2di4Xbb3/726y++uriA5jvHAEBoFE5AgKw\\\neHEEBAAAKMYREAAAoBhHQAAAgGIECAAAUIwHES6E6urq8u6776Zt27apqqpq7OEAACz2KpVKJkyY\\\nkK5du6a62r/hfxkCZCH07rvvpkePHo09DAAA/sdbb72V7t27N/YwvtIEyEKobdu2SWb8gLdr166R\\\nR8Oiatq0aRkyZEi22mqrNGvWrLGHw6JsyKmNPQIWcdMq1RkyfbVs1XRomlV5Fg0LxvhJU9Jj77Pr\\\nf09j3gmQhdDM067atWsnQFhgpk2bllatWqVdu3YChAWrVYvGHgGLuGmV6rSa3irtmrYQICxwTo//\\\n8pzABgAAFCNAAACAYgQIAABQjGtAAAC+hNqqJplW7Vqnr7pmdZPTpFLb2MNYLAgQAIB5UEnyfusV\\\nM7Z1z6S6SWMPhy+rrjYdJr6RLhNficvMFywBAgAwD95vvWLGtuudTksvmVY1zdwd6SusUqlk0pRp\\\nGfVB8yTJMhNfaeQRLdoECADAXKqtapqxrXum09JLZql2rRp7OMwHLWtm3JJ+VG3PdJo0wulYC5CL\\\n0AEA5tK06pqkukla1XiO0qKkVU2zpNo1PQuaAAEAmEdOu1q0+PMswylYAAAFDH/1tUyYMHGu39e2\\\nbev0XmH5BTAiaBwCBABgARv+6mtZaa2vz/P7hz35YPEIWW719XLkId/LkYceUHS7n+f1N95Kr/7r\\\n5+kH7syAfqs19nCYR07BAgBYwOblyMf8fP//euvtd7LfYUela58107zjcum52rr5wY9PyIcffTTf\\\ntrHc6uvlt+dfMt/Wx6JDgAAALEZee/2NrL3p4Ax/bUSu+r/z8spTD+fCs3+dex54KBtsuX0+GjOm\\\nsYfIIk6AAAAsRg77f8elefNmGXLDX7LJ1zbIsj26ZZstN8vdN12dd957P8edfFr9shM+/jjf2f/Q\\\ntO66YrqtslbOu+Ty+nmVSiU/P/XMLLvaOqnp1Ctd+6yZ7//o+CTJwG13yRtvvZ0f/vTnqerQLVUd\\\nuiVJPvzoo3xn/0PTbZW10mqZFbL6hpvnqutvajC+urq6/OZ352fFNTZKTadeWXa1dfLLM373mfsz\\\n9IWXss0ue6RNt97p3Lt/9jzwiHzw4X+P5Fz/t1uz+oabp2WXFbJUr1WzxQ7fzsSJk+bDd5J5JUAA\\\nABYTH40ZkzvvuT+H7r93WrZs2WBel86dsvu3dso1N9ySSqWSJDn99xem/2p98/QDd+YnRx6WH/zk\\\nhNx13wNJkr/e/PecfcEluejs0zL8yYdy058vzeqr9kmS3HDlJenebZn84qf/L++9/HTee/npJMnk\\\nyVOy1oB++fs1f8zQR+/NgXvvnj0P+n7+9eTT9eM49qRT8+uzz8vxx/wgLzx2X/5yyXnp3KnjbPdn\\\n7Nhx2Wz7XbNGv1XzxH23547r/5yRoz/IrvsclCR57/2R+c7+h2W/3b+dFx+7P/ffen122m6b+v2j\\\ncbgIHQBgMTH81RGpVCpZZaXes52/ykorZszYsRn9wYdJko3WWyc/+eHhSZKVVlwhDz/2eM4+/5Js\\\nuenGefOtd9KlU8dsMfDradasWZbt0S3rrrVGkmTJJZZIk+omadumTbp07lS//m5dl8n/O+Lg+tdH\\\nHLRf7rz3/lx74y1Zd601MmHCx/ndhZfm3NNPyd7f3TVJskKv5fK1Ddad7XjPveQPWaPfavnVCcfW\\\nT7vs3DPTY9V1MuyVV/Pxx5Myffr07LTd4PRctnuSZPVVV5nXbx/ziSMgAACLmTk9ArDBOmvN8vrF\\\nl4cnSb614zfyyeTJWX7ABjng+8fkxltuz/Tp0z93fbW1tTn5N2dn9Q03z5LLrZo23Xrnznv+kTff\\\nfidJ8uKw4ZkyZUo23/hrczS+Z4e+kPsefCRtuvWu/+iz7iZJkldHvJH+q/fN5pt8LatvtHm+tfeB\\\nueSPf86YsWPnaN0sOAIEAGAxseLyy6WqqiovDhs+2/kvDnslS3TokI5LL/WF6+rRvVtefvyBnH/G\\\nr9KyRYsc+v9+mo0H75Rp06Z95ntOP+eC/O7CS/PjHxya+265Ns88OCSDNt8kU6fOeE/LFnP3BPKP\\\nJ07KdltvmWceHNLgY/hTD2XjDddPkyZNctdNV+f2665M35VXyu8v+kNWXnvjjHj9zbnaDvOXAAEA\\\nWEwsteSS2XLTjXP+pX/MJ5980mDe+yNH5c/X3ZBv77Rd/RPB//nEUw2W+ecTT2WVlf97+lbLli2z\\\n3TZb5ZzfnJz7b70uj/7ryfz7+ZeSJM2bN0ttbW2D9z/8z8ezw+BB2ePbO6f/6qtm+eV6Ztgrr9XP\\\n771Cr7Rs2SL3PPDQHO3Pmv1Xy/MvvZzllu2RFZfv1eCjdetWSWY83Xyj9dfJST/9f3n6wTvTvHmz\\\n3Hjr7XP4HWNBECAAAIuRc08/JVOmTM2gnXbPAw//M2+9/U7uuPu+bPnN76TbMl3yy+N/XL/sw489\\\nnt/87vwMe+XVnHfJ5bnuplvzg4P3T5Jc/udrcumfrsrQF17Ka6+/kSuvuSEtW7ZIz2Vn3PFquWV7\\\n5IFHHss7775Xf1eq3iv0yl33P5BHHns8L748PAcd+eOMHP1B/fZatGiRH//gsPzohF/mT1ddl1dH\\\nvJ5/Pv5kLv3TVbPdl8O+t08+GjM239n/0Dz+1DN5dcTrufOe+7PvoT9MbW1tHnviqfzqzHPyxNPP\\\n5s233skNt9yW0R981CCiKM9F6AAAi5HeKyyfJ+67PSeeekZ23ffgfDRmbLp07pgdB2+dE3/ywyy5\\\nxBL1yx592EF54ulnc9JpZ6Vd27Y565cnZtDmA5MkHdq3z69/e26O+tlJqa2tzep9++SWqy/PUksu\\\nmST5xU//Xw468sdZYY2NMmXKlFTGvpOfHfODvPb6mxm08+5p1bJlDtx79+w4eFDGjZ9Qv83jf3Rk\\\nmjZtkhN+dUbefX9kluncKQfvt+ds96XrMl3y8J035ccn/ipbffO7mTJ1Snr26J6tNx+Y6urqtGvb\\\nNg888lh+e8H/ZfyEj9OzR7ececoJ2WbLzRbcN5gvVFVxH7KFzvjx49O+ffuMGzcu7dq1a+zhsIia\\\nNm1abrvttgwePDjNmjVr7OGwKLvtpMYeAYu4aZXq3Da9XwY3fS7NquqKbHNyk9YZ0XGj9OrRLS2a\\\nf/G/5z71zL+z1sCt53l7T95/R9YcsPo8v585M3nq9Ix46530Gv1wWtQ2fPr8+EmT0/5bv/b72Xzg\\\nFCwAAKAYAQIAABQjQAAAFrC2bVs36vthYeIidACABaz3Cstn2JMPZsKEiV+88P9o27Z1eq+w/AIY\\\nFTQOAQIAUICIgBmcggUAABQjQAAAGtGwV5qkS+/OGfZKk8YeChQhQAAAGtFfrmuVkaOb5KrrWzb2\\\nUKAIAQIA0IiuuaHFfz4v2gFy/4OPpKpDt4wdO66xh0IjEyAAAI3k5eFN8tLwZkmSF4c1K3Ia1ugP\\\nPswhR/0ky662Tmo69UqXlQZk0E7fzcP/fHyBbnfD9dbOey8/nfbtPUV8cecuWAAAjeSvN7dMk+pK\\\nauuqUl1dyV9vbpljj/p4gW5z5z0PyNRpU/PH83+b5ZfrmZGjR+eefzyUDz8aM0/rq1Qqqa2tTdOm\\\nn/9rZfPmzdOlc6d52sacmjp1apo3b75At8GX5wgIAEAjueaGlqmrzPi6rm7Bn4Y1duy4PPjoYznt\\\n58dl0403Ss9lu2fdtdbIsUcdke0Hb5XX33grVR265ZnnhjZ4T1WHbrn/wUeS/PdUqtvvujdrbbJ1\\\najr1ymVXXp2qDt3y0rBXGmzv7PMuzgoDNmzwvrFjx2X8+Alp2WWF3H7XvQ2Wv/GW29O2+0qZNOmT\\\nJMm/n38xm233rbTsskKW6rVqDvzBj/Lxx/99lso+hxyZHb+7X355xu/Stc+aWXntjZMk5//f5em9\\\n5kZp0Xn5dO7dP7vsdcD8/2YyzxwBAQBYQCZPTp56tlkqlapZ5n00pirPPd/sU1Oq8uzQZrnl9pos\\\nuURlluWrqipZs/+0tGgx7+Np06Z12rRpnZv+fkfWX2fN1NTUzPO6fvLzX+WMU07I8sstmyU6tM8l\\\nf/xL/nztDTn5Zz+qX+bP192Y7+6y4yzvbdeubb4xaPP85bobs82Wm31q+Ruy4+BBadWqZSZOnJRB\\\nO++eDdZZK4/f+/eMGv1Bvvf9Y3L4Mcfl8gt+W/+eex54KO3atsldN16VJHni6Wfz/R+fkCsuOicb\\\nrrt2PhozNg8++tg87yfznwABAFhALvlj63z/x+0/c351dSV1dVUNXm//naU+c/lzThuXIw6a+6ep\\\nz9S0adNcft7ZOeAHP8qFf7gya/ZfLZtstH5222mH9Fut71yt6xc/PSZbbrpx/evdv/XNnHvJ5fUB\\\nMuyVV/PkM8/lyot/P9v37/6tnbLnwd/PpEmfpFWrlhk/fkL+PuTe3Hjl/yVJ/nL9jZk8eUr+dOHv\\\n0rp1qyTJuaefku122yennXRcOnfqmCRp3apV/u/3Z9SfenXDzbeldetW+cagLdK2bZv0XLZ71ui/\\\n2tx9o1ignIIFALCAHLD3xBx+4IxrOqqqZj2q8en4mN3rT7/viAM/zgF7z3t8zLTzDtvm3ZeezM1X\\\n/SFbbz4w9z/0aNbcZOtc/udr5mo9a6/Rr8Hr3XbeIa+/+Vb++fiTSZI/X3tj1uy/evqstOJs3z94\\\nq83SrGmz3Hz7kCTJX2++Le3atskWA7+eJHnx5eHpv9oq9fGRJButt07q6ury8vBX66et3rdPg+s+\\\nttx04/Ts3j3LD9ggex54RP587Q31p3SxcBAgAAALSIsWye9/Mz5/+8uHad+ukqZNZ42Qz9O0SSXt\\\n21Vy81Uf5pzfjP9Sp181HFeLbLnpxjn+Rz/MI0Nuzj7f3TUnnnpmqqtn/GpY+dQwp02fPtt1tG7V\\\nqsHrLp07ZbONN8pfrrspyYwjGLt/65ufOYbmzZtnlx22zV+uu7F++W9/c/svvJj9i8bRtm2bPPXA\\\nHbnq0vOyTJfOOeFXZ6T/17Zw+9+FiAABAFjAth88JUMfHZUN1pmaZE4jpJIN15uaoY+OynbbTFmQ\\\nw0vflXtn4qRJ6bj0kkmS90aOrJ/3zL+fn+P17P6tb+aaG2/Oo/96Iq+9/mZ223mHL1z+jnvuz/Mv\\\nvpx7H3g4u+/632BZZeXeeXboi5k4cVL9tIcfezzV1dVZufcKn7vepk2bZouBG+c3v/hZnnv47rz+\\\n5tu594GH53g/WLAECABAAd261uW+Wz/ML4+fMNvTsT6tqqqSXx4/Iffe8mG6da2bb2P48KOPstl2\\\n38qV1/w1zw19ISNefzPX3XRLfnPOBdlh8KC0bNky66+zZn599nl58eXh+cdDj+Znp/xmjte/03aD\\\nM+Hjj3PIUcdm069vmK7LdPnc5TfeaP106dwxux9weHr1XDbrrb1m/bzdv7VTWrSoyd6H/CBDX3gp\\\n9z3wcI740fHZ89s711//MTu33nFXzrnw0jzz3NC88ebb+dPV16Wuru4Lo4VyBAgAQCFNmiTf22vS\\\nFy+Y5IC9J6XJfH4uYZvWrbPe2mvm7PMvycaDd85qG26W4395eg7Y67s59/RTkiSXnXtWptdOz1oD\\\nt86Rx56YUz51V6sv0rZtm2y39ZZ5dugL2f1bO33h8lVVVfnOzjv+Z/mGp2u1atUyd/71z/lozNis\\\ns9m22WXvA7P5Jl/Luaf/8nPX2aF9+9xwy+3ZbPtvZ5X1NsmFl12Rqy49L6uusvIc7wcLVlWlUpm7\\\nkxFZ4MaPH5/27dtn3LhxadfO00JZMKZNm5bbbrstgwcPTrNmzb74DTCvbjupsUfAIm5apTq3Te+X\\\nwU2fS7Oq+Xe04PNMbtI6IzpulF49uqVF87m7ZuHiy1vl4B+2n+2teWeqSiUX/W5cDth7zmKF+WPy\\\n1OkZ8dY76TX64bSobXjB//hJk9P+W7/2+9l84AgIAEBB193YIlWfao+mTSoNPidJdZPk2hvn0xXn\\\nsJARIAAAhXw0pir3PVRTf7vd6qpKVll5em695sOsvNL0VP/n2pDa2qrc92BNxoz97KMk8FUlQAAA\\\nCrn5thapra3677M9DpqYx+8bnW0HTckT942uf8hgVVUltbVVufk2R0FY9AgQAIBCrvtbyyRJh3aV\\\n3HrNh/ntr8enpmbGvBYtkt/+enxuufrDdGhXabA8LEoECABAIUNfaJqBX5uS5x8blW0Hzf7ZHt/Y\\\nekqG/nNUNvnalAx9Ye4ucIevAj/VAACFPP/P0WndutLgIvTZ6bpMXe675cNMnOgaEBY9AgQAoJA2\\\nbeb86QdVVXO3PHxVOAULAAAoRoAAAADFCBAAAKAYAQIAsJh59F9PpMmSPbLtrns22hhef+OtVHXo\\\nlmeeG/qFy7751jvZdtc902qZFdJpxX455viTM3369AKjZEFwEToAQGOprU0eeSwZOSrp3CnZcL2k\\\nSZMFvtlLr7g6Rxy4by698uq8+9776bpMlwW+zXlVW1ubbb+9V7p06phH7vxb3hs5Knsd/IM0a9Y0\\\nvzrh2MYeHvPAERAAgMZw823J6usl230r+d5hMz6vvt6M6QvQxx9PzDU33pxD9t8r2261eS7/y7Wz\\\nGdqQ9F5zo7TovHw2/cYu+eNfrk1Vh24ZO3Zc/TIPPfqvfH2bb6ZllxXSY9W18/0fHZ+JEyfVz19u\\\n9fXyqzPPyX6HHZW23VfKsqutk4svv7J+fq/+6ydJ1th4UKo6dMvAbXeZ7XiH3PuPvPDSsFx58e8z\\\noN9q2WbLzXLyccfkvP/7Y6ZOnTq/vi0UJEAAAEq7+bZk7wOTd99rOP2992dMX4ARcu2Nt6RP7xWz\\\ncu8Vs8euO+WyK69JpfLf2/2OeP3N7LL3gdlx263z7ENDctC+e+a4U05rsI5XR7yerXfZPTtvNzjP\\\nPXxXrrnsgjz0z3/l8GOOa7DcmedelLXX6JenH7gzh+6/dw456ti8PPyVJMm/7v17kuTuv12d915+\\\nOjdceclsx/vov57M6n37pHOnjvXTBm02MOPHT8jzLw6bL98TyhIgAABfVqWSTJw0Zx/jJyQ/Pn7G\\\ne2a3niT5yQkzlpuT9c1uPZ/j0iuvyh677pQk2XqLTTNu/Pj846FH6+dfdPmVWbn3Cjn95OOzcu8V\\\ns9vOO2Sf7+7aYB2nnnVudv/WN3PkoQek9wrLZ8P11sk5p52cP119fSZPnly/3OAtN8uh39snKy7f\\\nKz8+8rAsvdSSue/BR5IkHZdaKkmy1BJLpEvnTllyiSVmO973R41uEB9J6l+/P2rUXO07CwfXgAAA\\\nfFmTPkm69Z4/66pUZhwZWbbPnC3/zvCkdas5WvTl4a/kX08+kxuvvDRJ0rRp03x7p+1z6RVXZeDX\\\nN/zPMq9mnTX6N3jfumuu0eD1s0NfyHPPv5g/X3fjp4ZdSV1dXUa88VZWWXnG96Lfan3r51dVVaVL\\\np44ZNfrDOdsvFlkCBABgMXHpFVdn+vTp6dpnzfpplUolNTXNc+64X6Z9+3ZztJ6PJ07MQfvske8f\\\nvN8s85bt3q3+62ZNG/6qWVVVlbq6urkac5dOHfOvJ59uMG3kqNH/mddprtbFwkGAAAB8Wa1azjgS\\\nMSceeSz51h5fvNx1V864K9acbHsOTJ8+PX+6+vqcecoJ2WqzTRrM23H3/XPVX2/KwfvtlZV7r5Db\\\nhtzbYP7jTz3T4PWa/VfPCy8Py4rL95qjbc9O8+bNkiS1XxAkG6y7Vn555jkZNfqDdOq4dJLkrvsf\\\nSLt2bdO3z3w66kRRrgEBAPiyqqpmnAY1Jx+bbZx0XWbGez5rXd26zlhuTtb3Wev5H7fecXfGjB2X\\\n/ff8Tlbr26fBx87bD86lV1ydJDlonz3y0vBX8uMTf5lhr7yaa2+8OZdfde1/hjZjWz/+waF55F9P\\\n5PBjjsszzw3N8Fdfy9/+fucsF6F/nk4dl07Lli1yx933ZeSo0Rk3bvxsl9tqs03St89K2fOg7+fZ\\\nfz+fO++5Pz875Tc57Ht7p6amZo63x8JDgAAAlNSkSfLrX8z4+n/jYebrU0+a788DufSKq7LFwK/N\\\n9jSrnbcfnCeefjbPDX0hvZZbNtf/8eLccMtt6bfRlrng0j/luKO/nySpqWmeZMa1Hf/4+18z7JXX\\\n8vXBO2WNjQflhF+dnq5dOs/xeJo2bZpzTjs5F11+Zbr2WTM7fHfW07mSpEmTJrn16j+mSZMm2WCr\\\n7bPHgUdkr912yS9+esw8fBdYGFRVKnN56wQWuPHjx6d9+/YZN25c2rWbs3MxYW5NmzYtt912WwYP\\\nHpxmzZo19nBYlN12UmOPgEXctEp1bpveL4ObPpdmVXN3fcG8mtykdUZ03Ci9enRLi+bzeEb7zbfN\\\nuNvVp2/F263rjPjYfvD8Geh88sszfpcL/3BF3nr+icYeygI1eer0jHjrnfQa/XBa1E5sMG/8pMlp\\\n/61f+/1sPnANCABAY9h+cLLtoEZ5EvoXOf//Ls86aw7IUksukYf/+XhOP+fCHH7gPo09LBYRAgQA\\\noLE0aZL85/a3C5Phr47IKWeck4/GjM2y3bvm6MMPzLFHHdHYw2IRIUAAAGjg7FNPytmnOn2SBcNF\\\n6AAAQDECBAAAKEaAAAAAxQgQAACgGAECAAAUI0AAAIBiBAgAAFCMAAEAaCS1dbW5/61HctVLN+X+\\\ntx5JbV1tke0++q8n0mTJHtl21z2LbG92Xn/jrVR16JZnnhv6hct+/0fHZ61Ntk5Np14Z8LUtC4yO\\\nBcmDCAEAGsENr9yWH9x/Qt7++L36ad3bLJPfDfxFdlpx8ALd9qVXXJ0jDtw3l155dd597/10XabL\\\nAt3e/LDfHrvlsSeeynPPv9jYQ+FLcgQEAKCwG165LbvcemCD+EiSdz5+P7vcemBueOW2Bbbtjz+e\\\nmGtuvDmH7L9Xtt1q81z+l2tnWebm24ak95obpUXn5bPpN3bJH/9ybao6dMvYsePql3no0X/l69t8\\\nMy27rJAeq66d7//o+EycOKl+/nKrr5dfnXlO9jvsqLTtvlKWXW2dXHz5lfXze/VfP0myxsaDUtWh\\\nWwZuu8tnjvmc35ycww7YJ8sv13N+fAtoZAIEAOBLqlQqmTht0hx9jJ8yId+/7/hUUpl1Pf+Z9oP7\\\nT8j4KRPmaH2Vyqzr+TzX3nhL+vReMSv3XjF77LpTLrvymgbrGPH6m9ll7wOz47Zb59mHhuSgfffM\\\ncaec1mAdr454PVvvsnt23m5wnnv4rlxz2QV56J//yuHHHNdguTPPvShrr9EvTz9wZw7df+8cctSx\\\neXn4K0mSf9379yTJ3X+7Ou+9/HRuuPKSudoPvrqcggUA8CVNmv5J2pzXe76sq5JK3v74vbS/oM8c\\\nLf/xYcPTulmrOV7/pVdelT123SlJsvUWm2bcYUflHw89moFf3zBJctHlV2bl3ivk9JOPT5Ks3HvF\\\nDH3xpfzyjHPq13HqWedm9299M0ceekCSpPcKy+ec007OJtvunAvOOjUtWrRIkgzecrMc+r19kiQ/\\\nPvKwnH3+JbnvwUeycu8V03GppZIkSy2xRLp07jTH4+erT4AAACwmXh7+Sv715DO58cpLkyRNmzbN\\\nt3faPpdecVV9gLw8/NWss0b/Bu9bd801Grx+dugLee75F/Pn626sn1apVFJXV5cRb7yVVVaeEWP9\\\nVutbP7+qqipdOnXMqNEfLpB946tDgAAAi5XXp4zJHeNeycGd1plv62zVtGU+Pmz4HC37wDuPZfBN\\\ne3zhcrfteGU27rbeHG17Tl16xdWZPn16uvZZs35apVJJTU3znDvul2nfvt0crefjiRNz0D575PsH\\\n7zfLvGW7d6v/ulnThr9qVlVVpa6ubo7Hy6JJgAAAi5XXp4zNhaOemK8BUlVVNcenQW217Mbp3maZ\\\nvPPx+7O9DqQqVeneZplstezGaVLdZL6Ncfr06fnT1dfnzFNOyFabbdJg3o6775+r/npTDt5vr6zc\\\ne4XcNuTeBvMff+qZBq/X7L96Xnh5WFZcvtc8j6d582ZJklpBsthxEToAsND4pG5avv3Kden773PT\\\nf+gF2erlPyVJ/jD66QwYekH6D70gaz9/UV6fMiZJcsUHz2bN58/LD176QXYYfkXemTo+SXL5B09n\\\nx+FX1a/31rEvZ+BLf0iSHPzGrXl58ocZMPSCbD/8L4X3MGlS3SS/G/iLJDNi49Nmvv7twJPma3wk\\\nya133J0xY8dl/z2/k9X69mnwsfP2g3PpFVcnSQ7aZ4+8NPyV/PjEX2bYK6/m2htvzuVXzbhTVlXV\\\njPH9+AeH5pF/PZHDjzkuzzw3NMNffS1/+/uds1yE/nk6dVw6LVu2yB1335eRo0Zn3Ljxn7nsK6+N\\\nyDPPDc37o0blk8mT88xzQ/PMc0MzderUL/EdobEIEABgoXHHuFcytnZyXlj98Dy72iG5eoVdcv/4\\\nEfnFu//I7SvtkWdXOyQP9Nk3nZq2ztBJI3PMW0NyS+8987s+v8v6rZfN916/+Qu3cWHPb2TlFkvl\\\nmdUOyc29v1tgr2a104qDc/03Lk63Ng2fv9G9zTK5/hsXL5DngFx6xVXZYuDXZnua1c7bD84TTz+b\\\n54a+kF7LLZvr/3hxbrjltvTbaMtccOmfctzR30+S1NQ0TzLj2o5//P2vGfbKa/n64J2yxsaDcsKv\\\nTk/XLp3neDxNmzbNOaednIsuvzJd+6yZHb476+lcM33viGOyxsaDctEfrsywV17LGhsPyhobD8q7\\\n742cy+8CCwOnYAEAC43+LTvnxcmjc+jrt2aTtstlcIfe+fu44dlz6X5ZpnnbJEmrJjN+Cb5vwuvZ\\\nuv2K6da8XZ6dnhzcad388tn7U1v5apzSs9OKg7PD8oPy4DuP5b2Jo7JM6075erf15vuRj5luueaP\\\nnzlv3bXWSGXsO/Wvtx+8VbYfvFX961+e8bt077ZM/d2tkmSdNQdkyI1X5bO8/u/HZpn2zEN3NXj9\\\nvb2+m+/t9cUReP/fr//CZfjqECAAwEJj+RZL5oXVDsu940fk7vGv5Udv35Wt2q2Qls3bfOF7P30y\\\nU9NUp/ZTz7aYXDd9AYz2y2tS3SQDe2zY2MOYxfn/d3nWWXNAllpyiTz8z8dz+jkX5vAD92nsYbGI\\\ncAoWALDQeHvquFSlKtsv0Sdn9NgqlVSy59L9cuWHz+W9qROSJJNqp2ZS7dRs2na53DHulbz7n+s+\\\nLh79eDZv1ytNqqqzYosl89wnI/NJ3bRMr9TmLx/+u34b7ZrUZFztlEbZv6+K4a+OyA7f3S9919s0\\\nJ5/+2xx9+IH5+U+ObuxhsYhwBAQAWGj8e9KoHPv23akkmV6py55L9c/GbZfLiV0HZtCwK1KVqjSv\\\napLrV9w1q7XqnNN7bJVvDL8iEyrXpW9N81yy3PZJkvXb9Mjg9r2z2tDzs0yzNtmozbJ5bOLbSZJ+\\\nrTpn1ZYds9rQ87J8zRKNdh3IwuzsU0/K2aee1NjDYBElQACAhcY2HXpnmw6zPlF876UHZO+lB8wy\\\nfc+l+2e3pdbIbdP7ZXDT59Ks6r/Xf1yw3Ddmu42mVU1y60q7z7cxA3PHKVgAAEAxAgQAYB5VKrM+\\\nSJCvLn+eZQgQAIC51KxuSlJXm0lTpjX2UJiPJk2ZltTVplnd5MYeyiLNNSAAAHOpSWV6Okx8I6M+\\\nmPFMklY1zeqfEs5XT6VSyaQp0zLqg4/SYeIbaVKpbewhLdIECADAPOgy8ZUkyajanskCenggBdXV\\\npsPEN+r/XFlwBAgAwDyoSrLMxFfSadKITKtu8YXLs3BrVjfZkY9CBAgAwJfQpFKbJrUTG3sY8JXh\\\nInQAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQj\\\nQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwA\\\nAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIE\\\nAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAA\\\nAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEFlOvvNLwMwBACQIEFlPXXTfj8/XXN+44\\\nAIDFiwCBxdQNNzT8DABQggCBxdDLLyfDhs36NQDAgiZAYDH0178mTZrM+Lq6esZrAIASBAgshq65\\\nJqmrm/F1Xd2M1wBfRbWVuvxj/Ig8MOaB/GP8iNRW6hp7SMAXaNrYA1gUPfDAAzn99NPz5JNP5r33\\\n3suNN96YHXfcsbGHxWJk8uTkqaeSSmXWeR99lDz3XNKy5X+nPftscsstyZJLzrp8VVWy5ppJixYL\\\nbrwA8+KGj17ID968I29PG58kOStJ92bt8rtlt85OS/Zt3MEBn0mALAATJ05M//79s99++2WnnXZq\\\n7OGwGLrkkuT73//s+dXVs77efvvPXv6cc5Ijjpg/YwOYH2746IXs8uq1+d9/Z3ln2vjs8uq1uT67\\\nihBYSAmQBWCbbbbJNtts09jDYDF2wAEzLiw/99wZRzD+90hIXd3nv07++74jjpixPoCFRW2lLj94\\\n845Z4iNJKkmqkhz51h3ZYYk+aVLlbHNY2AiQhcCUKVMyZcqU+tfjx884lDxt2rRMmzatsYbFV1iT\\\nJslZZyVbbJEcfHAycWIyfXrDZVq2nNbg86c1bZq0bp1cdFEys6X9KDLPKn4BZP76x/g36k+7mp1K\\\nkremjs9949/KJu16lRsYi7Rp/i6bbwTIQuDUU0/NSSedNMv0IUOGpFWrVo0wIhYlF174+fMvu+yu\\\nz5xXqSS33TafB8RiqF9jD4BFzANTxs7RcrdPWSITp/v5Y/6YNH1SYw9hkSFAFgLHHntsjjrqqPrX\\\n48ePT48ePbLVVlulXbt2jTgyFhW1tclvf5ucfPJ/T8dq2XJaLrvsruy335b55JNmSWacdnX88cmR\\\nR/73Nr3wpQ35dWOPgEVM65oxOWsOltumZkw2afrcAh8Pi4fxTSc39hAWGQJkIVBTU5OamppZpjdr\\\n1izNmjVrhBGxqGnWLNlvv+SnP531epBPPmnWIED2398dr5jPqtwWlflr03Y90r1Zu7wzbfxsrwOp\\\nStK9ebts2q5Hmvj5Yz5p5mdpvnEyGywmbrpp/i4H0FiaVFXnd8tunWRGbHzazNe/7bG1C9BhIeW/\\\nzAXg448/zjPPPJNnnnkmSTJixIg888wzefPNNxt3YCzWrrtuxhGOmZo2bfg5mXE73muvLTsugHmx\\\n05J9c/0Ku6Zbs4anKndv3i7Xr+AWvLAwcwrWAvDEE09k0003rX898/qOvffeO5dffnkjjYrF2Ucf\\\nJffd99/b7VZXJyuvPOPr3r2TZ56ZMa+2dsZyY8YkSyzRaMMFmCM7Ldk3OyzRJ/eNfyu3T1ki29SM\\\n+c9pV/59FRZm/gtdAAYOHJhKpTLLh/igsdx884y4mHkE5Igjkvvvn/H1P/7x34cMVlXNWO7mmxtl\\\nmABzrUlVdTZp1ysbL7FxNmnXS3zAV4D/SmExcN11Mz536JDceuuMO2I1bz5jWk3NjNe33DJj/qeX\\\nBwCY3wQILAaGDk0GDkyefz7ZdtvZL/ONb8xYbpNNZnwGAFgQXAMCi4Hnn5/xZPOq/71dzP/o2nXG\\\nNSATJ5YZFwCw+BEgsBho02bOl62qmrvlAQDmhlOwAACAYgQIAABQjAABAACKESAAAEAxAgQAAChG\\\ngAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgB\\\nAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQI\\\nAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAA\\\nAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAA\\\nAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAA\\\nFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQ\\\njAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAx\\\nAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUI\\\nEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNA\\\nAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAAB\\\nAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQA\\\nAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAA\\\noBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACA\\\nYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFNO0sQfA5xhyatKqRWOPgkVV\\\npTpJv2TIr5OqusYeDYuwXz3e2CNgUVddnazSLznzqaTOX2csIJMnN/YIFh2OgAAAAMUIEAAAoBgB\\\nAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQI\\\nAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAA\\\nAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAA\\\nAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAA\\\nFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQ\\\njAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAx\\\nAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUI\\\nEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAi5X2Y8Zkjccfb+xhwGJL\\\ngAAAi5X2Y8dmzSeeaOxhwGKraWMPAABgpqbTpuUbN92UTiNHprZJk0xs3TpX77VX+j39dNb55z+T\\\nJHXV1blh110zboklstqzz2b9Rx5JTYsWWbV589y23Xb5uF27rP7001nppZfy1+98J0my4ssvZ71H\\\nHsmf990329x6a9qNG5f9L7gg49q3z/Xf/W5j7jIsdgQIALDQWP6VV9Ji8uRcfPjhSZIWkyZl2REj\\\n8rV//CN/3H//TGzbNk2nTk2SdBw5MpsNGZLLDzkkPb72tXT4/e+z7c0355o99vjcbdz+jW9kyzvu\\\nyKWHHLLA9weYlVOwAICFxqjOnbPU6NEZdOutWWXo0NQ1aZIVhw/P0H79MrFt2yTJ9ObNM7158/R8\\\n/fW8tuKK+bhduyTJU+uum54jRqSqrq4xdwH4AgIEAFhojF1yyVx82GF5bcUV0/3NN3PA+eenZvLk\\\nuV5PXXV1qiuV+tdNp0+fn8MEvgQBAgAsNNqOG5dUVWV4nz65Z6utkkolQ/v1y6rPPZfWEyYkSZpO\\\nnZqmU6fmjeWWy/KvvJI248cnSdZ4/PG83qtXKtXVGbPkkuk0cmSaTpuWqtrarPrvf9dvY0pNTWqm\\\nTGmU/QNcAwIALEQ6jhqVTe++O0lSXVeXof37563llstDAwfmO1dckUpVVWqbNMkNu+6a0Z07596t\\\ntsquV1yRmuuuy+jmzXPb9tsnSd7t0SOv9u6dA84/Px+3aZO3l102Xd9+O8mM07xGd+yYA847L2OW\\\nWMJF6FBYVaXyqeOTLBTGjx+f9u3bZ9x1P0m7Vi0aezgsoqZVqnPb9H4Z3PS5NKtyvjQLzq88boEF\\\nrLq6Oqv065cXn3suda7/YAGZPHlyTv71rzNu3Li0+891R8wbp2ABAADFCBAAAKAYAQIAABQjQAAA\\\ngGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAA\\\nihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAo\\\nRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAY\\\nAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIE\\\nCAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEg\\\nAABAMQIEAAAoRoAAAADFCBAAAKCYpo09AACAeVVVV5dl33gj3caOzcQxY/JGjx6pVPv3VViYCZAF\\\n5Lzzzsvpp5+e999/P/3798/vf//7rLvuuo09LABYZKz8wgvZ8o470m78+CTJ2knGt2uXu7beOi/3\\\n7du4gwM+k38iWACuueaaHHXUUTnxxBPz1FNPpX///hk0aFBGjRrV2EMDgEXCyi+8kJ2uvTZt/xMf\\\nM7UdPz47XXttVn7hhUYaGfBFBMgCcNZZZ+WAAw7Ivvvum759++bCCy9Mq1atctlllzX20ADgK6+q\\\nri5b3nHHjK//d95/Pm9xxx2pqqsrOi5gzjgFaz6bOnVqnnzyyRx77LH106qrq7PFFlvk0Ucfne17\\\npkyZkilTptS/Hv+ff82ZVqnOtIpGZMGY+bPlZ4wFzen4zG/LvvFG/WlXs1OVpP348en51lt5s1ev\\\ncgNjkVbtL7P5RoDMZx988EFqa2vTuXPnBtM7d+6cl156abbvOfXUU3PSSSfNMn3I9NXSanqrBTJO\\\nmOmu2tUaewgs4lbp19gjYFHTbezYOVpupSWWSOt+fgCZPyZNmtTYQ1hkCJCFwLHHHpujjjqq/vX4\\\n8ePTo0ePbNV0aNo1bdGII2NRNq1SnbtqV8uWTYamWZXTFFhwznyqsUfAombimDFZew6WGzZmTN58\\\n7rkFPh4WD5MnT27sISwyBMh8tvTSS6dJkyYZOXJkg+kjR45Mly5dZvuempqa1NTUzDK9WVWdXwxZ\\\n4PycsaA5DZ/57Y0ePTK+Xbu0HT9+lmtAkqSSGXfDeqNHj1T8ADKf1PlZmm+czDafNW/ePGuttVbu\\\nueee+ml1dXW55557ssEGGzTiyABg0VCprs5dW2894+v/nfefz3dvvbXngcBCyn+ZC8BRRx2VSy65\\\nJH/84x/z4osv5pBDDsnEiROz7777NvbQAGCR8HLfvrlh110zoV27BtPHt2uXG3bd1XNAYCHmFKwF\\\n4Nvf/nZGjx6dE044Ie+//34GDBiQO+64Y5YL0wGAefdy374Z1qdPer71VlZaYokM8yR0+EoQIAvI\\\n4YcfnsMPP7yxhwEAi7RKdXXe7NUrrfv1y5vPPeeaD/gK8E8EAABAMQIEAAAoRoAAAADFCBAAAKAY\\\nAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIE\\\nCAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEg\\\nAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAA\\\nAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIA\\\nABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAA\\\nUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABA\\\nMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADF\\\nCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQj\\\nQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwA\\\nAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIE\\\nAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAA\\\nAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAA\\\ngGIECAAAUIwAAQAAihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAA\\\nihEgAABAMQIEAAAoRoAAAADFCBAAAKAYAQIAABQjQAAAgGIECAAAUIwAAQAAihEgAABAMQIEAAAo\\\npmljD4BZVSqVJMn4SVMaeSQsyqZVqjNp+qSMbzo5zarqGns4LMImT27sEbCoq66uzqRJkzJ58uTU\\\n1fn7jAVjypQZv5fN/D2NeVdV8V1c6Lz99tvp0aNHYw8DAID/8dZbb6V79+6NPYyvNAGyEKqrq8u7\\\n776btm3bpqqqqrGHwyJq/Pjx6dGjR9566620a9eusYcDMM/8fUYJlUolEyZMSNeuXVNd7SqGL8Mp\\\nWAuh6upqZU0x7dq18z9sYJHg7zMWtPbt2zf2EBYJ8g0AAChGgAAAAMUIEFhM1dTU5MQTT0xNTU1j\\\nDwXgS/H3GXy1uAgdAAAoxhEQAACgGAECAAAUI0AAAIBiBAgAAFCMAIHF0HnnnZflllsuLVq0yHrr\\\nrZd//etfjT0kgLn2wAMPZLvttkvXrl1TVVWVm266qbGHBMwBAQKLmWuuuSZHHXVUTjzxxDz11FPp\\\n379/Bg0alFGjRjX20ADmysSJE9O/f/+cd955jT0UYC64DS8sZtZbb72ss846Offcc5MkdXV16dGj\\\nR4444oj85Cc/aeTRAcybqqqq3Hjjjdlxxx0beyjAF3AEBBYjU6dOzZNPPpktttiiflp1dXW22GKL\\\nPProo404MgBgcSFAYDHywQcfpLa2Np07d24wvXPnznn//fcbaVQAwOJEgAAAAMUIEFiMLL300mnS\\\npElGjhzZYPrIkSPTpUuXRhoVALA4ESCwGGnevHnWWmut3HPPPfXT6urqcs8992SDDTZoxJEBAIuL\\\npo09AKCso446KnvvvXfWXnvtrLvuuvntb3+biRMnZt99923soQHMlY8//jivvPJK/esRI0bkmWee\\\nyZJLLplll122EUcGfB634YXF0LnnnpvTTz8977//fgYMGJBzzjkn6623XmMPC2Cu3H///dl0001n\\\nmb733nvn8ssvLz8gYI4IEAAAoBjXgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAA\\\nAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAA\\\nAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAA\\\nFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQ\\\njAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAx\\\nAgQAAChGgAAAAMUIEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUI\\\nEAAAoBgBAgAAFCNAAACAYgQIAABQjAABAACKESAAAEAxAgQAAChGgAAAAMUIEAAAoJj/D3dPDo34\\\nFOSMAAAAAElFTkSuQmCC\\\n\"\n  frames[1] = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAyAAAAMgCAYAAADbcAZoAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90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\\\nQmCC\\\n\"\n\n\n    /* set a timeout to make sure all the above elements are created before\n       the object is initialized. */\n    setTimeout(function() {\n        animf4556bc6aa314543bb7500385c371eeb = new Animation(frames, img_id, slider_id, 200.0,\n                                 loop_select_id);\n    }, 0);\n  })()\n</script>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "36eb284f1e684cdf9ad5dc00a480839c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VJdTN3MMheR",
        "outputId": "c91feb9c-f35a-4292-8b84-70d5674972f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:   2%|▏         | 19/1000 [00:00<00:05, 169.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 1// Epsilon: 0.900, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n",
            "//Episode 21// Epsilon: 0.860, Steps: 8, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:   7%|▋         | 73/1000 [00:00<00:05, 167.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 41// Epsilon: 0.820, Steps: 5, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 33.0\n",
            "//Episode 61// Epsilon: 0.780, Steps: 5, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 23.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  11%|█         | 107/1000 [00:00<00:05, 156.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 81// Epsilon: 0.740, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 43.0\n",
            "//Episode 101// Epsilon: 0.700, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Episodes:  12%|█▏        | 123/1000 [00:00<00:05, 151.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 121// Epsilon: 0.660, Steps: 3, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 32.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  16%|█▌        | 158/1000 [00:01<00:06, 121.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 141// Epsilon: 0.620, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n",
            "//Episode 161// Epsilon: 0.580, Steps: 5, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 33.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  20%|██        | 205/1000 [00:01<00:05, 133.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 181// Epsilon: 0.540, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n",
            "//Episode 201// Epsilon: 0.500, Steps: 6, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  24%|██▍       | 243/1000 [00:01<00:04, 154.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 221// Epsilon: 0.460, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 44.0\n",
            "//Episode 241// Epsilon: 0.420, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Episodes:  27%|██▋       | 273/1000 [00:01<00:03, 188.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 261// Epsilon: 0.380, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n",
            "//Episode 281// Epsilon: 0.340, Steps: 6, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 27.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  31%|███▏      | 314/1000 [00:02<00:04, 170.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 301// Epsilon: 0.300, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n",
            "//Episode 321// Epsilon: 0.260, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Episodes:  33%|███▎      | 332/1000 [00:02<00:05, 129.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 341// Epsilon: 0.220, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  36%|███▌      | 358/1000 [00:05<00:34, 18.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 361// Epsilon: 0.180, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  38%|███▊      | 385/1000 [00:08<00:42, 14.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 381// Epsilon: 0.140, Steps: 3, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  40%|████      | 403/1000 [00:09<00:32, 18.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 401// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  42%|████▏     | 424/1000 [00:10<00:27, 21.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 421// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 44.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  44%|████▍     | 445/1000 [00:11<00:27, 20.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 441// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  46%|████▋     | 463/1000 [00:12<00:25, 21.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 461// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  48%|████▊     | 484/1000 [00:13<00:24, 20.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 481// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  50%|█████     | 505/1000 [00:14<00:23, 21.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 501// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  52%|█████▏    | 523/1000 [00:15<00:23, 20.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 521// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  54%|█████▍    | 544/1000 [00:15<00:21, 21.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 541// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  56%|█████▋    | 565/1000 [00:17<00:21, 20.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 561// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  58%|█████▊    | 582/1000 [00:18<00:31, 13.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 581// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  60%|██████    | 602/1000 [00:19<00:27, 14.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 601// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  62%|██████▏   | 622/1000 [00:21<00:28, 13.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 621// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  64%|██████▍   | 644/1000 [00:22<00:16, 21.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 641// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  66%|██████▋   | 665/1000 [00:23<00:15, 21.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 661// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  68%|██████▊   | 683/1000 [00:23<00:14, 21.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 681// Epsilon: 0.100, Steps: 3, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 44.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  70%|███████   | 704/1000 [00:24<00:14, 20.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 701// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  72%|███████▎  | 725/1000 [00:25<00:12, 21.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 721// Epsilon: 0.100, Steps: 3, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 44.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  74%|███████▍  | 744/1000 [00:26<00:12, 19.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 741// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  76%|███████▋  | 764/1000 [00:27<00:11, 20.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 761// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 54.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  78%|███████▊  | 785/1000 [00:28<00:10, 20.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 781// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  80%|████████  | 803/1000 [00:29<00:09, 20.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 801// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  82%|████████▏ | 824/1000 [00:30<00:09, 19.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 821// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 54.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  84%|████████▍ | 843/1000 [00:32<00:10, 14.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 841// Epsilon: 0.100, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  86%|████████▋ | 863/1000 [00:33<00:09, 13.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 861// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  88%|████████▊ | 883/1000 [00:35<00:08, 13.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 881// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 54.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  90%|█████████ | 904/1000 [00:36<00:05, 18.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 901// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  92%|█████████▏| 924/1000 [00:37<00:04, 18.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 921// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  94%|█████████▍| 943/1000 [00:38<00:02, 19.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 941// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  96%|█████████▋| 963/1000 [00:39<00:01, 20.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 961// Epsilon: 0.100, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  98%|█████████▊| 984/1000 [00:40<00:00, 20.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 981// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes: 100%|██████████| 1000/1000 [00:40<00:00, 24.45it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import gym\n",
        "from gym import spaces\n",
        "import copy\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "class MultiAgentActionSpace(list):\n",
        "    def __init__(self, agents_action_space):\n",
        "        for x in agents_action_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_action_space)\n",
        "        self._agents_action_space = agents_action_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_action_space.sample() for agent_action_space in self._agents_action_space]\n",
        "\n",
        "class MultiAgentObservationSpace(list):\n",
        "    def __init__(self, agents_observation_space):\n",
        "        for x in agents_observation_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_observation_space)\n",
        "        self._agents_observation_space = agents_observation_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_observation_space.sample() for agent_observation_space in self._agents_observation_space]\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append((np.ones(len(done)) - done).tolist())\n",
        "\n",
        "        return (torch.tensor(np.array(s_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(a_lst), dtype=torch.long),\n",
        "                torch.tensor(np.array(r_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(s_prime_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(done_mask_lst), dtype=torch.float))\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        super(QNet, self).__init__()\n",
        "        self.num_agents = len(observation_space)\n",
        "        for agent_i in range(self.num_agents):\n",
        "            n_obs = observation_space[agent_i].shape[0]\n",
        "            setattr(self, f'agent_{agent_i}', nn.Sequential(\n",
        "                nn.Linear(n_obs, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, action_space[agent_i].n)\n",
        "            ))\n",
        "\n",
        "    def forward(self, obs):\n",
        "        q_values = [torch.empty(obs.shape[0], )] * self.num_agents\n",
        "        for agent_i in range(self.num_agents):\n",
        "            q_values[agent_i] = getattr(self, f'agent_{agent_i}')(obs[:, agent_i, :]).unsqueeze(1)\n",
        "        return torch.cat(q_values, dim=1)\n",
        "\n",
        "    def sample_action(self, obs, epsilon, roles):\n",
        "        out = self.forward(obs)\n",
        "        actions = torch.empty((out.shape[0], out.shape[1],), dtype=torch.long)\n",
        "        for agent_i in range(out.shape[1]):\n",
        "            if random.random() < epsilon:\n",
        "                if roles[agent_i] == 'relay':\n",
        "                    actions[:, agent_i] = 8  # STAY\n",
        "                else:\n",
        "                    actions[:, agent_i] = torch.randint(0, out.shape[2], (1,)).long()\n",
        "            else:\n",
        "                actions[:, agent_i] = out[:, agent_i].argmax().long()\n",
        "        return actions\n",
        "\n",
        "def train(q, q_target, memory, optimizer, gamma, batch_size, update_iter=10):\n",
        "    for _ in range(update_iter):\n",
        "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n",
        "        q_out = q(s)\n",
        "        a = a.unsqueeze(-1)\n",
        "        q_a = q_out.gather(2, a).squeeze(-1)\n",
        "        max_q_prime = q_target(s_prime).max(dim=2)[0]\n",
        "        target = r + gamma * max_q_prime * done_mask\n",
        "        loss = F.smooth_l1_loss(q_a, target.detach())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "class GridMultiAgent(gym.Env):\n",
        "    metadata = {'render.modes': ['console']}\n",
        "    XM, XP, YM, YP, XMYM, XMYP, XPYM, XPYP, STAY = range(9)\n",
        "    OOE, OBS, POI, MAP, AGT, SURVIVOR = -2, -1, 0, 1, 2, 3\n",
        "\n",
        "    def __init__(self, x_size=2, y_size=2, fov_x=3, fov_y=3, n_agents=2, n_survivors=1):\n",
        "        super().__init__()\n",
        "        self.x_size = x_size\n",
        "        self.y_size = y_size\n",
        "        self.n_agents = n_agents\n",
        "        self.idx_agents = list(range(n_agents))\n",
        "        self.n_survivors = n_survivors\n",
        "        self.fov_x = fov_x\n",
        "        self.fov_y = fov_y\n",
        "\n",
        "        self.confidence_map = np.zeros((x_size, y_size))\n",
        "        self.pheromone_map = np.zeros((x_size, y_size))\n",
        "        self.confidence_decay = 0.95\n",
        "        self.pheromone_decay = 0.9\n",
        "        self.agent_roles = ['scout'] * n_agents\n",
        "        self.stuck_counts = [0] * n_agents\n",
        "        self.survivor_pos = []\n",
        "        self.agent_paths = [[] for _ in range(n_agents)]\n",
        "\n",
        "        n_actions = 9\n",
        "        self.action_space = MultiAgentActionSpace([spaces.Discrete(n_actions) for _ in range(n_agents)])\n",
        "        obs_size = (fov_x * fov_y - 1) + 1 + 1 + 1 + 3\n",
        "        self.obs_low = np.concatenate([np.ones(fov_x * fov_y - 1) * self.OOE, [0, 0, 0, 0, 0, 0]], dtype=np.float32)\n",
        "        self.obs_high = np.concatenate([np.ones(fov_x * fov_y - 1) * self.SURVIVOR, [1, 1, 1, 1, 1, 1]], dtype=np.float32)\n",
        "        self.observation_space = MultiAgentObservationSpace([\n",
        "            spaces.Box(self.obs_low, self.obs_high, dtype=np.float32) for _ in range(n_agents)\n",
        "        ])\n",
        "\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "\n",
        "    def init_grid(self):\n",
        "        self.grid_status = np.zeros((self.x_size, self.y_size))\n",
        "        self.grid_counts = np.tile(self.grid_status, (self.n_agents, 1, 1)).reshape(self.n_agents, self.x_size, self.y_size)\n",
        "        self.n_poi = self.x_size * self.y_size - np.count_nonzero(self.grid_status)\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "\n",
        "    def init_agent(self):\n",
        "        self.agent_pos = []\n",
        "        self.agent_paths = [[] for _ in range(self.n_agents)]\n",
        "        for i in range(self.n_agents):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_agents_status[x, y] == self.POI:\n",
        "                    self.agent_pos.append([x, y])\n",
        "                    self.grid_agents_status[x, y] = self.AGT\n",
        "                    self.agent_paths[i].append([x, y])\n",
        "                    break\n",
        "        self.stuck_counts = [0] * n_agents\n",
        "\n",
        "    def init_survivors(self):\n",
        "        self.survivor_pos = []\n",
        "        for _ in range(self.n_survivors):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_status[x, y] == self.POI and [x, y] not in self.agent_pos:\n",
        "                    self.survivor_pos.append([x, y])\n",
        "                    self.grid_status[x, y] = self.SURVIVOR\n",
        "                    self.confidence_map[x, y] = 1.0\n",
        "                    break\n",
        "\n",
        "    def grid_overlay(self):\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            self.grid_agents_status[x, y] = self.AGT\n",
        "\n",
        "    def get_rf_signal(self, agent_pos, survivor_pos):\n",
        "        distance = np.sqrt((agent_pos[0] - survivor_pos[0])**2 + (agent_pos[1] - survivor_pos[1])**2)\n",
        "        return min(1.0, 1.0 / (distance + 1)) if distance > 0 else 1.0\n",
        "\n",
        "    def get_agent_obs(self):\n",
        "        self.agent_obs = []\n",
        "        for agent in range(self.n_agents):\n",
        "            single_obs = np.ones((self.fov_x, self.fov_y)) * self.OOE\n",
        "            x, y = self.agent_pos[agent]\n",
        "            for i in range(self.fov_x):\n",
        "                for j in range(self.fov_y):\n",
        "                    obs_x = x + (i - 1)\n",
        "                    obs_y = y + (j - 1)\n",
        "                    if 0 <= obs_x < self.x_size and 0 <= obs_y < self.y_size:\n",
        "                        single_obs[i][j] = self.grid_agents_status[obs_x][obs_y]\n",
        "            single_obs_flat = single_obs.flatten()\n",
        "            single_obs_flat = np.array([v for k, v in enumerate(single_obs_flat) if k != math.floor(self.fov_x * self.fov_y / 2)])\n",
        "\n",
        "            rf_signal = max([self.get_rf_signal(self.agent_pos[agent], s) for s in self.survivor_pos], default=0.0)\n",
        "            confidence_level = self.confidence_map[x, y]\n",
        "            pheromone_level = self.pheromone_map[x, y]\n",
        "            role_encoding = {'scout': [1, 0, 0], 'verification': [0, 1, 0], 'relay': [0, 0, 1]}\n",
        "            role_vec = role_encoding[self.agent_roles[agent]]\n",
        "\n",
        "            obs = np.concatenate([single_obs_flat, [confidence_level, pheromone_level, rf_signal], role_vec])\n",
        "            self.agent_obs.append(obs.astype(np.float32))\n",
        "        return self.agent_obs\n",
        "\n",
        "    def update_roles(self):\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            confidence = self.confidence_map[x, y]\n",
        "            if confidence > 0.7 and self.agent_roles[i] != 'verification':\n",
        "                self.agent_roles[i] = 'verification'\n",
        "            elif confidence > 0.5 and self.agent_roles[i] != 'relay':\n",
        "                self.agent_roles[i] = 'relay'\n",
        "            elif self.agent_roles[i] != 'scout':\n",
        "                self.agent_roles[i] = 'scout'\n",
        "\n",
        "    def update_confidence_and_pheromones(self):\n",
        "        self.confidence_map *= self.confidence_decay\n",
        "        self.pheromone_map *= self.pheromone_decay\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            rf_signal = max([self.get_rf_signal(self.agent_pos[i], s) for s in self.survivor_pos], default=0.0)\n",
        "            confidence = rf_signal\n",
        "            self.confidence_map[x, y] = min(1.0, self.confidence_map[x, y] + confidence)\n",
        "            if confidence > 0.5:\n",
        "                self.pheromone_map[x, y] = min(1.0, self.pheromone_map[x, y] + 0.5)\n",
        "\n",
        "    def get_coverage(self):\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        return mapped_poi / self.n_poi\n",
        "\n",
        "    def get_survivor_detection_rate(self):\n",
        "        detected = sum(1 for s in self.survivor_pos if self.confidence_map[s[0], s[1]] > 0.8)\n",
        "        return detected / self.n_survivors if self.n_survivors > 0 else 0.0\n",
        "\n",
        "    def step(self, action, i):\n",
        "        org_x, org_y = self.agent_pos[i]\n",
        "        reward = 0\n",
        "        action = int(action)\n",
        "        if action == self.XM:\n",
        "            self.agent_pos[i][0] -= 1\n",
        "        elif action == self.XP:\n",
        "            self.agent_pos[i][0] += 1\n",
        "        elif action == self.YM:\n",
        "            self.agent_pos[i][1] -= 1\n",
        "        elif action == self.YP:\n",
        "            self.agent_pos[i][1] += 1\n",
        "        elif action == self.XMYM:\n",
        "            self.agent_pos[i][0] -= 1\n",
        "            self.agent_pos[i][1] -= 1\n",
        "        elif action == self.XMYP:\n",
        "            self.agent_pos[i][0] -= 1\n",
        "            self.agent_pos[i][1] += 1\n",
        "        elif action == self.XPYM:\n",
        "            self.agent_pos[i][0] += 1\n",
        "            self.agent_pos[i][1] -= 1\n",
        "        elif action == self.XPYP:\n",
        "            self.agent_pos[i][0] += 1\n",
        "            self.agent_pos[i][1] += 1\n",
        "        elif action == self.STAY:\n",
        "            reward = 5 if self.agent_roles[i] == 'relay' and self.confidence_map[org_x, org_y] > 0.5 else -1\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid action={action}\")\n",
        "\n",
        "        if (self.agent_pos[i][0] >= self.x_size or self.agent_pos[i][0] < 0 or\n",
        "            self.agent_pos[i][1] >= self.y_size or self.agent_pos[i][1] < 0 or\n",
        "            self.grid_status[self.agent_pos[i][0], self.agent_pos[i][1]] in [self.OBS, self.AGT]):\n",
        "            self.agent_pos[i] = [org_x, org_y]\n",
        "            self.grid_counts[i][org_x, org_y] += 1\n",
        "            reward = -2\n",
        "        else:\n",
        "            prev_status = self.grid_status[self.agent_pos[i][0], self.agent_pos[i][1]]\n",
        "            if prev_status == self.POI:\n",
        "                self.grid_status[self.agent_pos[i][0], self.agent_pos[i][1]] = self.MAP\n",
        "                self.grid_counts[i][self.agent_pos[i][0], self.agent_pos[i][1]] += 1\n",
        "                reward = 5\n",
        "            elif prev_status == self.SURVIVOR:\n",
        "                self.grid_counts[i][self.agent_pos[i][0], self.agent_pos[i][1]] += 1\n",
        "                reward = 50\n",
        "            elif prev_status == self.MAP:\n",
        "                self.grid_counts[i][self.agent_pos[i][0], self.agent_pos[i][1]] += 1\n",
        "                reward = -1\n",
        "\n",
        "        self.agent_paths[i].append(self.agent_pos[i][:])\n",
        "\n",
        "        nearby_agents = sum(1 for j in range(self.n_agents) if j != i and\n",
        "                           np.sqrt((self.agent_pos[i][0] - self.agent_pos[j][0])**2 +\n",
        "                                   (self.agent_pos[i][1] - self.agent_pos[j][1])**2) < 2)\n",
        "        if nearby_agents > 0:\n",
        "            reward -= 2 * nearby_agents\n",
        "\n",
        "        if org_x == self.agent_pos[i][0] and org_y == self.agent_pos[i][1]:\n",
        "            self.stuck_counts[i] += 1\n",
        "        else:\n",
        "            self.stuck_counts[i] = 0\n",
        "\n",
        "        self.update_confidence_and_pheromones()\n",
        "        self.update_roles()\n",
        "        self.grid_overlay()\n",
        "\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        done = mapped_poi == self.n_poi or self.get_survivor_detection_rate() == 1.0\n",
        "        info = {'survivor_detection_rate': self.get_survivor_detection_rate()}\n",
        "\n",
        "        return self.get_agent_obs(), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "        self.confidence_map = np.zeros((self.x_size, self.y_size))\n",
        "        self.pheromone_map = np.zeros((self.x_size, self.y_size))\n",
        "        self.agent_roles = ['scout'] * self.n_agents\n",
        "        while True:\n",
        "            obs = self.get_agent_obs()\n",
        "            obs_tf = [obs[i][0] != 0 and obs[i][1] != 0 and obs[i][2] != 0 and obs[i][3] != 0\n",
        "                      for i in range(self.n_agents)]\n",
        "            if any(obs_tf):\n",
        "                self.init_grid()\n",
        "                self.init_agent()\n",
        "                self.init_survivors()\n",
        "            else:\n",
        "                break\n",
        "        return self.get_agent_obs()\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# Training\n",
        "size = 2\n",
        "fov = 3\n",
        "n_agents = 2\n",
        "n_survivors = 1\n",
        "train_episodes = 1000\n",
        "max_steps = 8\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "buffer_limit = 50000\n",
        "log_interval = 20\n",
        "max_epsilon = 0.9\n",
        "min_epsilon = 0.1\n",
        "warm_up_steps = 1000\n",
        "update_iter = 10\n",
        "coverage_threshold = 0.95\n",
        "lr = 0.001\n",
        "\n",
        "env = GridMultiAgent(x_size=size, y_size=size, fov_x=fov, fov_y=fov, n_agents=n_agents, n_survivors=n_survivors)\n",
        "memory = ReplayBuffer(buffer_limit)\n",
        "q = QNet(env.observation_space, env.action_space)\n",
        "q_target = QNet(env.observation_space, env.action_space)\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "optimizer = optim.Adam(q.parameters(), lr=lr)\n",
        "\n",
        "time_steps, epsilons, coverage, survivor_detection, total_reward, all_agent_paths = [], [], [], [], [], []\n",
        "\n",
        "for episode in tqdm(range(train_episodes), desc=\"Training Episodes\"):\n",
        "    score = np.zeros(n_agents)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_step = 0\n",
        "    epsilon = max(min_epsilon, max_epsilon - (max_epsilon - min_epsilon) * (episode / (0.4 * train_episodes)))\n",
        "\n",
        "    while not done:\n",
        "        state_np = np.array(state, dtype=np.float32)\n",
        "        action = q.sample_action(torch.tensor(state_np).unsqueeze(0), epsilon, env.agent_roles)[0].data.cpu().numpy()\n",
        "        rewards = np.zeros(n_agents)\n",
        "        next_state = state\n",
        "        for agent_i in env.idx_agents:\n",
        "            next_state, reward, done, info = env.step(action[agent_i], agent_i)\n",
        "            rewards[agent_i] = reward\n",
        "            if done:\n",
        "                break\n",
        "        memory.put((state, action, rewards, next_state, [done] * n_agents))\n",
        "        score += rewards\n",
        "        state = next_state\n",
        "        episode_step += 1\n",
        "        if episode_step >= max_steps or done:\n",
        "            break\n",
        "\n",
        "    if memory.size() > warm_up_steps:\n",
        "        train(q, q_target, memory, optimizer, gamma, batch_size, update_iter)\n",
        "\n",
        "    if episode % log_interval == 0:\n",
        "        q_target.load_state_dict(q.state_dict())\n",
        "\n",
        "    all_agent_paths.append([path[:] for path in env.agent_paths])\n",
        "    time_steps.append(episode_step)\n",
        "    epsilons.append(epsilon)\n",
        "    coverage.append(env.get_coverage())\n",
        "    survivor_detection.append(info['survivor_detection_rate'])\n",
        "    total_reward.append(score.sum())\n",
        "\n",
        "    if episode % log_interval == 0:\n",
        "        print(f'//Episode {episode+1}// Epsilon: {epsilon:.3f}, Steps: {episode_step}, '\n",
        "              f'Coverage (%): {coverage[-1]:.3f}, Survivor Detection (%): {survivor_detection[-1]:.3f}, '\n",
        "              f'Total Reward: {total_reward[-1]}')\n",
        "\n",
        "# Metrics Visualization\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(coverage, label='Coverage')\n",
        "plt.title('Coverage Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Coverage (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(survivor_detection, label='Survivor Detection', color='orange')\n",
        "plt.title('Survivor Detection Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Detection Rate (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(total_reward, label='Total Reward', color='green')\n",
        "plt.title('Total Reward Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('metrics_original1000.png')\n",
        "plt.close()\n",
        "\n",
        "# Drone Path Animation (Last Episode)\n",
        "def animate_drone_paths(episode_idx=-1):\n",
        "    paths = all_agent_paths[episode_idx]\n",
        "    final_grid = env.grid_status\n",
        "    final_confidence = env.confidence_map\n",
        "    final_survivors = env.survivor_pos\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xlim(-0.5, env.x_size - 0.5)\n",
        "    ax.set_ylim(-0.5, env.y_size - 0.5)\n",
        "    ax.set_xticks(range(env.x_size))\n",
        "    ax.set_yticks(range(env.y_size))\n",
        "    ax.grid(True)\n",
        "    ax.set_title(f'Drone Paths (Episode {episode_idx + 1})')\n",
        "\n",
        "    obs_x, obs_y = np.where(final_grid == env.OBS)\n",
        "    ax.scatter(obs_x, obs_y, c='black', marker='s', s=100, label='Obstacles')\n",
        "    heatmap = ax.imshow(final_confidence.T, cmap='hot', alpha=0.5, origin='lower',\n",
        "                        extent=(-0.5, env.x_size - 0.5, -0.5, env.y_size - 0.5))\n",
        "    survivor_x, survivor_y = zip(*final_survivors) if final_survivors else ([], [])\n",
        "    survivors = ax.scatter(survivor_x, survivor_y, c='blue', marker='*', s=200, label='Survivors')\n",
        "    colors = ['red', 'green']\n",
        "    agent_plots = [ax.plot([], [], c=colors[i], marker='o', linestyle='-', label=f'Agent {i}')[0]\n",
        "                   for i in range(n_agents)]\n",
        "    ax.legend()\n",
        "\n",
        "    def update(frame):\n",
        "        for i, plot in enumerate(agent_plots):\n",
        "            if frame < len(paths[i]):\n",
        "                x, y = zip(*paths[i][:frame + 1]) if paths[i] else ([], [])\n",
        "                plot.set_data(x, y)\n",
        "        return agent_plots + [heatmap, survivors]\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=min(len(p) for p in paths), interval=200, blit=True)\n",
        "    plt.close(fig)\n",
        "    return ani\n",
        "\n",
        "# Save animation\n",
        "ani = animate_drone_paths(-1)\n",
        "with open('animation_original.html', 'w') as f:\n",
        "    f.write(ani.to_jshtml())\n",
        "\n",
        "# Save metrics for comparison\n",
        "np.savez('metrics_original.npz', coverage=coverage, survivor_detection=survivor_detection, total_reward=total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import gym\n",
        "from gym import spaces\n",
        "import copy\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "import ipywidgets as widgets\n",
        "\n",
        "class MultiAgentActionSpace(list):\n",
        "    def __init__(self, agents_action_space):\n",
        "        for x in agents_action_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_action_space)\n",
        "        self._agents_action_space = agents_action_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_action_space.sample() for agent_action_space in self._agents_action_space]\n",
        "\n",
        "class MultiAgentObservationSpace(list):\n",
        "    def __init__(self, agents_observation_space):\n",
        "        for x in agents_observation_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_observation_space)\n",
        "        self._agents_observation_space = agents_observation_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_observation_space.sample() for agent_observation_space in self._agents_observation_space]\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append((np.ones(len(done)) - done).tolist())\n",
        "\n",
        "        return (torch.tensor(np.array(s_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(a_lst), dtype=torch.long),\n",
        "                torch.tensor(np.array(r_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(s_prime_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(done_mask_lst), dtype=torch.float))\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        super(QNet, self).__init__()\n",
        "        self.num_agents = len(observation_space)\n",
        "        for agent_i in range(self.num_agents):\n",
        "            n_obs = observation_space[agent_i].shape[0]\n",
        "            setattr(self, f'agent_{agent_i}', nn.Sequential(\n",
        "                nn.Linear(n_obs, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, action_space[agent_i].n)\n",
        "            ))\n",
        "\n",
        "    def forward(self, obs):\n",
        "        q_values = [torch.empty(obs.shape[0], )] * self.num_agents\n",
        "        for agent_i in range(self.num_agents):\n",
        "            q_values[agent_i] = getattr(self, f'agent_{agent_i}')(obs[:, agent_i, :]).unsqueeze(1)\n",
        "        return torch.cat(q_values, dim=1)\n",
        "\n",
        "    def sample_action(self, obs, epsilon, roles):\n",
        "        out = self.forward(obs)\n",
        "        actions = torch.empty((out.shape[0], out.shape[1],), dtype=torch.long)\n",
        "        for agent_i in range(out.shape[1]):\n",
        "            if random.random() < epsilon:\n",
        "                if roles[agent_i] == 'relay':\n",
        "                    actions[:, agent_i] = 8  # STAY\n",
        "                else:\n",
        "                    actions[:, agent_i] = torch.randint(0, out.shape[2], (1,)).long()\n",
        "            else:\n",
        "                actions[:, agent_i] = out[:, agent_i].argmax().long()\n",
        "        return actions\n",
        "\n",
        "def train(q, q_target, memory, optimizer, gamma, batch_size, update_iter=10):\n",
        "    for _ in range(update_iter):\n",
        "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n",
        "        q_out = q(s)\n",
        "        a = a.unsqueeze(-1)\n",
        "        q_a = q_out.gather(2, a).squeeze(-1)\n",
        "        max_q_prime = q_target(s_prime).max(dim=2)[0]\n",
        "        target = r + gamma * max_q_prime * done_mask\n",
        "        loss = F.smooth_l1_loss(q_a, target.detach())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "class GridMultiAgent(gym.Env):\n",
        "    metadata = {'render.modes': ['console']}\n",
        "    XM, XP, YM, YP, XMYM, XMYP, XPYM, XPYP, STAY = range(9)\n",
        "    OOE, OBS, POI, MAP, AGT, SURVIVOR = -2, -1, 0, 1, 2, 3\n",
        "\n",
        "    def __init__(self, x_size=2, y_size=2, fov_x=3, fov_y=3, n_agents=2, n_survivors=1):\n",
        "        super().__init__()\n",
        "        self.x_size = x_size\n",
        "        self.y_size = y_size\n",
        "        self.n_agents = n_agents\n",
        "        self.idx_agents = list(range(n_agents))\n",
        "        self.n_survivors = n_survivors\n",
        "        self.fov_x = fov_x\n",
        "        self.fov_y = fov_y\n",
        "\n",
        "        self.confidence_map = np.zeros((x_size, y_size))\n",
        "        self.pheromone_map = np.zeros((x_size, y_size))\n",
        "        self.confidence_decay = 0.95\n",
        "        self.pheromone_decay = 0.9\n",
        "        self.agent_roles = ['scout'] * n_agents\n",
        "        self.stuck_counts = [0] * n_agents\n",
        "        self.survivor_pos = []\n",
        "        self.agent_paths = [[] for _ in range(n_agents)]\n",
        "        self.agent_roles_history = [[] for _ in range(n_agents)]\n",
        "\n",
        "        n_actions = 9\n",
        "        self.action_space = MultiAgentActionSpace([spaces.Discrete(n_actions) for _ in range(n_agents)])\n",
        "        obs_size = (fov_x * fov_y - 1) + 1 + 1 + 1 + 3\n",
        "        self.obs_low = np.concatenate([np.ones(fov_x * fov_y - 1) * self.OOE, [0, 0, 0, 0, 0, 0]], dtype=np.float32)\n",
        "        self.obs_high = np.concatenate([np.ones(fov_x * fov_y - 1) * self.SURVIVOR, [1, 1, 1, 1, 1, 1]], dtype=np.float32)\n",
        "        self.observation_space = MultiAgentObservationSpace([\n",
        "            spaces.Box(self.obs_low, self.obs_high, dtype=np.float32) for _ in range(n_agents)\n",
        "        ])\n",
        "\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "\n",
        "    def init_grid(self):\n",
        "        self.grid_status = np.zeros((self.x_size, self.y_size))\n",
        "        self.grid_counts = np.tile(self.grid_status, (self.n_agents, 1, 1)).reshape(self.n_agents, self.x_size, self.y_size)\n",
        "        self.n_poi = self.x_size * self.y_size - np.count_nonzero(self.grid_status)\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "\n",
        "    def init_agent(self):\n",
        "        self.agent_pos = []\n",
        "        self.agent_paths = [[] for _ in range(self.n_agents)]\n",
        "        self.agent_roles_history = [[] for _ in range(self.n_agents)]\n",
        "        for i in range(self.n_agents):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_agents_status[x, y] == self.POI:\n",
        "                    self.agent_pos.append([x, y])\n",
        "                    self.grid_agents_status[x, y] = self.AGT\n",
        "                    self.agent_paths[i].append([x, y])\n",
        "                    self.agent_roles_history[i].append(self.agent_roles[i])\n",
        "                    break\n",
        "        self.stuck_counts = [0] * n_agents\n",
        "\n",
        "    def init_survivors(self):\n",
        "        self.survivor_pos = []\n",
        "        for _ in range(self.n_survivors):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_status[x, y] == self.POI and [x, y] not in self.agent_pos:\n",
        "                    self.survivor_pos.append([x, y])\n",
        "                    self.grid_status[x, y] = self.SURVIVOR\n",
        "                    self.confidence_map[x, y] = 1.0\n",
        "                    break\n",
        "\n",
        "    def grid_overlay(self):\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            self.grid_agents_status[x, y] = self.AGT\n",
        "\n",
        "    def get_rf_signal(self, agent_pos, survivor_pos):\n",
        "        distance = np.sqrt((agent_pos[0] - survivor_pos[0])**2 + (agent_pos[1] - survivor_pos[1])**2)\n",
        "        return min(1.0, 1.0 / (distance + 1)) if distance > 0 else 1.0\n",
        "\n",
        "    def get_agent_obs(self):\n",
        "        self.agent_obs = []\n",
        "        for agent in range(self.n_agents):\n",
        "            single_obs = np.ones((self.fov_x, self.fov_y)) * self.OOE\n",
        "            x, y = self.agent_pos[agent]\n",
        "            for i in range(self.fov_x):\n",
        "                for j in range(self.fov_y):\n",
        "                    obs_x = x + (i - 1)\n",
        "                    obs_y = y + (j - 1)\n",
        "                    if 0 <= obs_x < self.x_size and 0 <= obs_y < self.y_size:\n",
        "                        single_obs[i][j] = self.grid_agents_status[obs_x][obs_y]\n",
        "            single_obs_flat = single_obs.flatten()\n",
        "            single_obs_flat = np.array([v for k, v in enumerate(single_obs_flat) if k != math.floor(self.fov_x * self.fov_y / 2)])\n",
        "\n",
        "            rf_signal = max([self.get_rf_signal(self.agent_pos[agent], s) for s in self.survivor_pos], default=0.0)\n",
        "            confidence_level = self.confidence_map[x, y]\n",
        "            pheromone_level = self.pheromone_map[x, y]\n",
        "            role_encoding = {'scout': [1, 0, 0], 'verification': [0, 1, 0], 'relay': [0, 0, 1]}\n",
        "            role_vec = role_encoding[self.agent_roles[agent]]\n",
        "\n",
        "            obs = np.concatenate([single_obs_flat, [confidence_level, pheromone_level, rf_signal], role_vec])\n",
        "            self.agent_obs.append(obs.astype(np.float32))\n",
        "        return self.agent_obs\n",
        "\n",
        "    def update_roles(self):\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            confidence = self.confidence_map[x, y]\n",
        "            if confidence > 0.7 and self.agent_roles[i] != 'verification':\n",
        "                self.agent_roles[i] = 'verification'\n",
        "            elif confidence > 0.5 and self.agent_roles[i] != 'relay':\n",
        "                self.agent_roles[i] = 'relay'\n",
        "            elif self.agent_roles[i] != 'scout':\n",
        "                self.agent_roles[i] = 'scout'\n",
        "            self.agent_roles_history[i].append(self.agent_roles[i])\n",
        "\n",
        "    def update_confidence_and_pheromones(self):\n",
        "        self.confidence_map *= self.confidence_decay\n",
        "        self.pheromone_map *= self.pheromone_decay\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            rf_signal = max([self.get_rf_signal(self.agent_pos[i], s) for s in self.survivor_pos], default=0.0)\n",
        "            confidence = rf_signal\n",
        "            self.confidence_map[x, y] = min(1.0, self.confidence_map[x, y] + confidence)\n",
        "            if confidence > 0.5:\n",
        "                self.pheromone_map[x, y] = min(1.0, self.pheromone_map[x, y] + 0.5)\n",
        "\n",
        "    def get_coverage(self):\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        return mapped_poi / self.n_poi\n",
        "\n",
        "    def get_survivor_detection_rate(self):\n",
        "        detected = sum(1 for s in self.survivor_pos if self.confidence_map[s[0], s[1]] > 0.8)\n",
        "        return detected / self.n_survivors if self.n_survivors > 0 else 0.0\n",
        "\n",
        "    def step(self, action, i):\n",
        "        org_x, org_y = self.agent_pos[i]\n",
        "        reward = 0\n",
        "        action = int(action)\n",
        "        if action == self.XM:\n",
        "            self.agent_pos[i][0] -= 1\n",
        "        elif action == self.XP:\n",
        "            self.agent_pos[i][0] += 1\n",
        "        elif action == self.YM:\n",
        "            self.agent_pos[i][1] -= 1\n",
        "        elif action == self.YP:\n",
        "            self.agent_pos[i][1] += 1\n",
        "        elif action == self.XMYM:\n",
        "            self.agent_pos[i][0] -= 1\n",
        "            self.agent_pos[i][1] -= 1\n",
        "        elif action == self.XMYP:\n",
        "            self.agent_pos[i][0] -= 1\n",
        "            self.agent_pos[i][1] += 1\n",
        "        elif action == self.XPYM:\n",
        "            self.agent_pos[i][0] += 1\n",
        "            self.agent_pos[i][1] -= 1\n",
        "        elif action == self.XPYP:\n",
        "            self.agent_pos[i][0] += 1\n",
        "            self.agent_pos[i][1] += 1\n",
        "        elif action == self.STAY:\n",
        "            reward = 5 if self.agent_roles[i] == 'relay' and self.confidence_map[org_x, org_y] > 0.5 else -1\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid action={action}\")\n",
        "\n",
        "        if (self.agent_pos[i][0] >= self.x_size or self.agent_pos[i][0] < 0 or\n",
        "            self.agent_pos[i][1] >= self.y_size or self.agent_pos[i][1] < 0 or\n",
        "            self.grid_status[self.agent_pos[i][0], self.agent_pos[i][1]] in [self.OBS, self.AGT]):\n",
        "            self.agent_pos[i] = [org_x, org_y]\n",
        "            self.grid_counts[i][org_x, org_y] += 1\n",
        "            reward = -2\n",
        "        else:\n",
        "            prev_status = self.grid_status[self.agent_pos[i][0], self.agent_pos[i][1]]\n",
        "            if prev_status == self.POI:\n",
        "                self.grid_status[self.agent_pos[i][0], self.agent_pos[i][1]] = self.MAP\n",
        "                self.grid_counts[i][self.agent_pos[i][0], self.agent_pos[i][1]] += 1\n",
        "                reward = 5\n",
        "            elif prev_status == self.SURVIVOR:\n",
        "                self.grid_counts[i][self.agent_pos[i][0], self.agent_pos[i][1]] += 1\n",
        "                reward = 50\n",
        "            elif prev_status == self.MAP:\n",
        "                self.grid_counts[i][self.agent_pos[i][0], self.agent_pos[i][1]] += 1\n",
        "                reward = -1\n",
        "\n",
        "        self.agent_paths[i].append(self.agent_pos[i][:])\n",
        "\n",
        "        nearby_agents = sum(1 for j in range(self.n_agents) if j != i and\n",
        "                           np.sqrt((self.agent_pos[i][0] - self.agent_pos[j][0])**2 +\n",
        "                                   (self.agent_pos[i][1] - self.agent_pos[j][1])**2) < 2)\n",
        "        if nearby_agents > 0:\n",
        "            reward -= 2 * nearby_agents\n",
        "\n",
        "        if org_x == self.agent_pos[i][0] and org_y == self.agent_pos[i][1]:\n",
        "            self.stuck_counts[i] += 1\n",
        "        else:\n",
        "            self.stuck_counts[i] = 0\n",
        "\n",
        "        self.update_confidence_and_pheromones()\n",
        "        self.update_roles()\n",
        "        self.grid_overlay()\n",
        "\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        done = mapped_poi == self.n_poi or self.get_survivor_detection_rate() == 1.0\n",
        "        info = {'survivor_detection_rate': self.get_survivor_detection_rate()}\n",
        "\n",
        "        return self.get_agent_obs(), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "        self.confidence_map = np.zeros((self.x_size, self.y_size))\n",
        "        self.pheromone_map = np.zeros((self.x_size, self.y_size))\n",
        "        self.agent_roles = ['scout'] * self.n_agents\n",
        "        while True:\n",
        "            obs = self.get_agent_obs()\n",
        "            obs_tf = [obs[i][0] != 0 and obs[i][1] != 0 and obs[i][2] != 0 and obs[i][3] != 0\n",
        "                      for i in range(self.n_agents)]\n",
        "            if any(obs_tf):\n",
        "                self.init_grid()\n",
        "                self.init_agent()\n",
        "                self.init_survivors()\n",
        "            else:\n",
        "                break\n",
        "        return self.get_agent_obs()\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# Training\n",
        "size = 2\n",
        "fov = 3\n",
        "n_agents = 2\n",
        "n_survivors = 1\n",
        "train_episodes = 1000\n",
        "max_steps = 8\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "buffer_limit = 50000\n",
        "log_interval = 20\n",
        "max_epsilon = 0.9\n",
        "min_epsilon = 0.1\n",
        "warm_up_steps = 1000\n",
        "update_iter = 10\n",
        "coverage_threshold = 0.95\n",
        "lr = 0.001\n",
        "\n",
        "env = GridMultiAgent(x_size=size, y_size=size, fov_x=fov, fov_y=fov, n_agents=n_agents, n_survivors=n_survivors)\n",
        "memory = ReplayBuffer(buffer_limit)\n",
        "q = QNet(env.observation_space, env.action_space)\n",
        "q_target = QNet(env.observation_space, env.action_space)\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "optimizer = optim.Adam(q.parameters(), lr=lr)\n",
        "\n",
        "time_steps, epsilons, coverage, survivor_detection, total_reward, all_agent_paths, all_agent_roles = [], [], [], [], [], [], []\n",
        "\n",
        "for episode in tqdm(range(train_episodes), desc=\"Training Episodes\"):\n",
        "    score = np.zeros(n_agents)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_step = 0\n",
        "    epsilon = max(min_epsilon, max_epsilon - (max_epsilon - min_epsilon) * (episode / (0.4 * train_episodes)))\n",
        "\n",
        "    while not done:\n",
        "        state_np = np.array(state, dtype=np.float32)\n",
        "        action = q.sample_action(torch.tensor(state_np).unsqueeze(0), epsilon, env.agent_roles)[0].data.cpu().numpy()\n",
        "        rewards = np.zeros(n_agents)\n",
        "        next_state = state\n",
        "        for agent_i in env.idx_agents:\n",
        "            next_state, reward, done, info = env.step(action[agent_i], agent_i)\n",
        "            rewards[agent_i] = reward\n",
        "            if done:\n",
        "                break\n",
        "        memory.put((state, action, rewards, next_state, [done] * n_agents))\n",
        "        score += rewards\n",
        "        state = next_state\n",
        "        episode_step += 1\n",
        "        if episode_step >= max_steps or done:\n",
        "            break\n",
        "\n",
        "    if memory.size() > warm_up_steps:\n",
        "        train(q, q_target, memory, optimizer, gamma, batch_size, update_iter)\n",
        "\n",
        "    if episode % log_interval == 0:\n",
        "        q_target.load_state_dict(q.state_dict())\n",
        "\n",
        "    all_agent_paths.append([path[:] for path in env.agent_paths])\n",
        "    all_agent_roles.append([roles[:] for roles in env.agent_roles_history])\n",
        "    time_steps.append(episode_step)\n",
        "    epsilons.append(epsilon)\n",
        "    coverage.append(env.get_coverage())\n",
        "    survivor_detection.append(info['survivor_detection_rate'])\n",
        "    total_reward.append(score.sum())\n",
        "\n",
        "    if episode % log_interval == 0:\n",
        "        print(f'//Episode {episode+1}// Epsilon: {epsilon:.3f}, Steps: {episode_step}, '\n",
        "              f'Coverage (%): {coverage[-1]:.3f}, Survivor Detection (%): {survivor_detection[-1]:.3f}, '\n",
        "              f'Total Reward: {total_reward[-1]}')\n",
        "\n",
        "# Metrics Visualization\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(coverage, label='Coverage')\n",
        "plt.title('Coverage Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Coverage (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(survivor_detection, label='Survivor Detection', color='orange')\n",
        "plt.title('Survivor Detection Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Detection Rate (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(total_reward, label='Total Reward', color='green')\n",
        "plt.title('Total Reward Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('metrics_upgraded1000.png')\n",
        "plt.close()\n",
        "\n",
        "# Drone Path Animation\n",
        "def animate_drone_paths(episode_idx):\n",
        "    if episode_idx >= len(all_agent_paths):\n",
        "        print(f\"Episode {episode_idx + 1} data not available.\")\n",
        "        return None\n",
        "\n",
        "    paths = all_agent_paths[episode_idx]\n",
        "    roles = all_agent_roles[episode_idx]\n",
        "    final_grid = env.grid_status.copy()\n",
        "    final_confidence = env.confidence_map.copy()\n",
        "    final_survivors = env.survivor_pos[:]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xlim(-0.5, env.x_size - 0.5)\n",
        "    ax.set_ylim(-0.5, env.y_size - 0.5)\n",
        "    ax.set_xticks(range(env.x_size))\n",
        "    ax.set_yticks(range(env.y_size))\n",
        "    ax.grid(True)\n",
        "    ax.set_title(f'Drone Paths (Episode {episode_idx + 1})')\n",
        "\n",
        "    obs_x, obs_y = np.where(final_grid == env.OBS)\n",
        "    ax.scatter(obs_x, obs_y, c='black', marker='s', s=100, label='Obstacles')\n",
        "    heatmap = ax.imshow(final_confidence.T, cmap='hot', alpha=0.5, origin='lower',\n",
        "                        extent=(-0.5, env.x_size - 0.5, -0.5, env.y_size - 0.5))\n",
        "    survivor_x, survivor_y = zip(*final_survivors) if final_survivors else ([], [])\n",
        "    survivors = ax.scatter(survivor_x, survivor_y, c='blue', marker='*', s=200, label='Survivors')\n",
        "    colors = ['red', 'green']\n",
        "    agent_plots = [ax.plot([], [], c=colors[i], marker='o', linestyle='-', label=f'Agent {i}')[0]\n",
        "                   for i in range(n_agents)]\n",
        "    role_texts = [ax.text(0, 0, '', fontsize=8, color=colors[i], ha='center', va='bottom')\n",
        "                  for i in range(n_agents)]\n",
        "    ax.legend()\n",
        "\n",
        "    max_frames = max(len(p) for p in paths) if paths else 0\n",
        "\n",
        "    def update(frame):\n",
        "        artists = [heatmap, survivors]\n",
        "        for i, plot in enumerate(agent_plots):\n",
        "            if i < len(paths):\n",
        "                if frame < len(paths[i]):\n",
        "                    x, y = zip(*paths[i][:frame + 1]) if paths[i][:frame + 1] else ([], [])\n",
        "                    plot.set_data(x, y)\n",
        "                    if frame < len(roles[i]):\n",
        "                        if x and y:\n",
        "                            role_texts[i].set_position((x[-1], y[-1] + 0.2))\n",
        "                            role_texts[i].set_text(roles[i][frame])\n",
        "                        else:\n",
        "                            role_texts[i].set_text('')\n",
        "                    else:\n",
        "                        role_texts[i].set_text('')\n",
        "                    artists.append(plot)\n",
        "                    artists.append(role_texts[i])\n",
        "                else:\n",
        "                    if paths[i]:\n",
        "                        last_x, last_y = paths[i][-1]\n",
        "                        plot.set_data(*zip(*paths[i]))\n",
        "                        role_texts[i].set_position((last_x, last_y + 0.2))\n",
        "                        if roles[i]:\n",
        "                            role_texts[i].set_text(roles[i][-1])\n",
        "                        else:\n",
        "                            role_texts[i].set_text('')\n",
        "                        artists.append(plot)\n",
        "                        artists.append(role_texts[i])\n",
        "        return artists\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=max_frames, interval=200, blit=True)\n",
        "    plt.close(fig)\n",
        "    return ani\n",
        "\n",
        "# Save animation\n",
        "ani = animate_drone_paths(train_episodes - 1)\n",
        "if ani is not None:\n",
        "    with open('animation_upgraded.html', 'w') as f:\n",
        "        f.write(ani.to_jshtml())\n",
        "\n",
        "# Save metrics for comparison\n",
        "np.savez('metrics_upgraded.npz', coverage=coverage, survivor_detection=survivor_detection, total_reward=total_reward)\n",
        "\n",
        "# Interactive widget\n",
        "episode_selector = widgets.IntSlider(\n",
        "    value=train_episodes - 1,\n",
        "    min=0,\n",
        "    max=train_episodes - 1,\n",
        "    step=1,\n",
        "    description='Episode:',\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "def on_episode_change(change):\n",
        "    episode_idx = change['new']\n",
        "    with output_widget:\n",
        "        output_widget.clear_output(wait=True)\n",
        "        ani = animate_drone_paths(episode_idx)\n",
        "        if ani is not None:\n",
        "            display(HTML(ani.to_jshtml()))\n",
        "\n",
        "episode_selector.observe(on_episode_change, names='value')\n",
        "display(episode_selector, output_widget)\n",
        "\n",
        "with output_widget:\n",
        "    ani = animate_drone_paths(episode_selector.value)\n",
        "    if ani is not None:\n",
        "        display(HTML(ani.to_jshtml()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "103f896e99c34773bde3bf5e19b67eb9",
            "114fbe1fa67d401aa1a356e8d67824db",
            "738c5d2a42b94dec9e1498ef2f1b9733",
            "10cf1cc989b048c78e0c66cf923d70f0",
            "36eb284f1e684cdf9ad5dc00a480839c"
          ]
        },
        "id": "K_IFUc7fN_Bb",
        "outputId": "8593c82a-44f2-409d-9b30-397e4a0a7287"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:   4%|▍         | 45/1000 [00:00<00:04, 226.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 1// Epsilon: 0.900, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n",
            "//Episode 21// Epsilon: 0.860, Steps: 7, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 23.0\n",
            "//Episode 41// Epsilon: 0.820, Steps: 8, Coverage (%): 0.750, Survivor Detection (%): 0.000, Total Reward: -38.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  10%|█         | 105/1000 [00:00<00:03, 278.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 61// Epsilon: 0.780, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n",
            "//Episode 81// Epsilon: 0.740, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n",
            "//Episode 101// Epsilon: 0.700, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n",
            "//Episode 121// Epsilon: 0.660, Steps: 8, Coverage (%): 0.500, Survivor Detection (%): 0.000, Total Reward: -45.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  19%|█▉        | 189/1000 [00:00<00:03, 266.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 141// Epsilon: 0.620, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n",
            "//Episode 161// Epsilon: 0.580, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n",
            "//Episode 181// Epsilon: 0.540, Steps: 4, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 34.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  25%|██▍       | 247/1000 [00:00<00:02, 267.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 201// Epsilon: 0.500, Steps: 4, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 43.0\n",
            "//Episode 221// Epsilon: 0.460, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n",
            "//Episode 241// Epsilon: 0.420, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Episodes:  28%|██▊       | 275/1000 [00:01<00:02, 270.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 261// Epsilon: 0.380, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 44.0\n",
            "//Episode 281// Epsilon: 0.340, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n",
            "//Episode 301// Epsilon: 0.300, Steps: 5, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 23.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  35%|███▍      | 347/1000 [00:01<00:03, 200.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 321// Epsilon: 0.260, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n",
            "//Episode 341// Epsilon: 0.220, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n",
            "//Episode 361// Epsilon: 0.180, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Episodes:  38%|███▊      | 375/1000 [00:02<00:05, 106.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 381// Epsilon: 0.140, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 44.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining Episodes:  40%|███▉      | 396/1000 [00:03<00:11, 51.75it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 401// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  42%|████▏     | 422/1000 [00:04<00:16, 34.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 421// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  44%|████▍     | 438/1000 [00:05<00:19, 28.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 441// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  46%|████▋     | 463/1000 [00:07<00:30, 17.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 461// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: -4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  48%|████▊     | 483/1000 [00:08<00:40, 12.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 481// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 54.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  50%|█████     | 504/1000 [00:09<00:26, 18.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 501// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  52%|█████▏    | 524/1000 [00:10<00:22, 20.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 521// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  55%|█████▍    | 545/1000 [00:11<00:22, 20.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 541// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  56%|█████▋    | 563/1000 [00:12<00:21, 20.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 561// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  58%|█████▊    | 585/1000 [00:13<00:19, 21.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 581// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  60%|██████    | 603/1000 [00:14<00:19, 20.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 601// Epsilon: 0.100, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  62%|██████▏   | 624/1000 [00:15<00:18, 19.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 621// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  64%|██████▍   | 643/1000 [00:17<00:30, 11.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 641// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  66%|██████▋   | 663/1000 [00:18<00:20, 16.20it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 661// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  68%|██████▊   | 682/1000 [00:19<00:21, 14.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 681// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  70%|███████   | 704/1000 [00:21<00:19, 15.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 701// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  72%|███████▏  | 722/1000 [00:22<00:20, 13.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 721// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  74%|███████▍  | 744/1000 [00:23<00:12, 21.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 741// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  76%|███████▋  | 765/1000 [00:24<00:11, 20.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 761// Epsilon: 0.100, Steps: 4, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 38.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  78%|███████▊  | 783/1000 [00:25<00:10, 21.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 781// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  80%|████████  | 804/1000 [00:26<00:09, 20.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 801// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  82%|████████▏ | 822/1000 [00:27<00:08, 21.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 821// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  84%|████████▍ | 843/1000 [00:28<00:09, 17.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 841// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  86%|████████▋ | 864/1000 [00:29<00:07, 19.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 861// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  88%|████████▊ | 885/1000 [00:30<00:05, 19.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 881// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  90%|█████████ | 904/1000 [00:31<00:04, 20.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 901// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  92%|█████████▎| 925/1000 [00:32<00:03, 21.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 921// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  94%|█████████▍| 942/1000 [00:33<00:03, 15.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 941// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  96%|█████████▌| 962/1000 [00:35<00:02, 13.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 961// Epsilon: 0.100, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  98%|█████████▊| 984/1000 [00:36<00:00, 18.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 981// Epsilon: 0.100, Steps: 3, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 42.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes: 100%|██████████| 1000/1000 [00:37<00:00, 26.69it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntSlider(value=999, continuous_update=False, description='Episode:', max=999)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "103f896e99c34773bde3bf5e19b67eb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10cf1cc989b048c78e0c66cf923d70f0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "import gym\n",
        "from gym import spaces\n",
        "import copy\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "\n",
        "class MultiAgentActionSpace(list):\n",
        "    def __init__(self, agents_action_space):\n",
        "        for x in agents_action_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_action_space)\n",
        "        self._agents_action_space = agents_action_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_action_space.sample() for agent_action_space in self._agents_action_space]\n",
        "\n",
        "class MultiAgentObservationSpace(list):\n",
        "    def __init__(self, agents_observation_space):\n",
        "        for x in agents_observation_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_observation_space)\n",
        "        self._agents_observation_space = agents_observation_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_observation_space.sample() for agent_observation_space in self._agents_observation_space]\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append((np.ones(len(done)) - done).tolist())\n",
        "\n",
        "        return (torch.tensor(np.array(s_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(a_lst), dtype=torch.long),\n",
        "                torch.tensor(np.array(r_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(s_prime_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(done_mask_lst), dtype=torch.float))\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, observation_space, action_space):\n",
        "        super(QNet, self).__init__()\n",
        "        self.num_agents = len(observation_space)\n",
        "        for agent_i in range(self.num_agents):\n",
        "            n_obs = observation_space[agent_i].shape[0]\n",
        "            setattr(self, f'agent_{agent_i}', nn.Sequential(\n",
        "                nn.Linear(n_obs, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, 32),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(32, action_space[agent_i].n)\n",
        "            ))\n",
        "\n",
        "    def forward(self, obs):\n",
        "        q_values = [torch.empty(obs.shape[0], )] * self.num_agents\n",
        "        for agent_i in range(self.num_agents):\n",
        "            q_values[agent_i] = getattr(self, f'agent_{agent_i}')(obs[:, agent_i, :]).unsqueeze(1)\n",
        "        return torch.cat(q_values, dim=1)\n",
        "\n",
        "    def sample_action(self, obs, epsilon):\n",
        "        out = self.forward(obs)\n",
        "        actions = torch.empty((out.shape[0], out.shape[1],), dtype=torch.long)\n",
        "        for agent_i in range(out.shape[1]):\n",
        "            if random.random() < epsilon:\n",
        "                actions[:, agent_i] = torch.randint(0, out.shape[2], (1,)).long()\n",
        "            else:\n",
        "                actions[:, agent_i] = out[:, agent_i].argmax().long()\n",
        "        return actions\n",
        "\n",
        "def train(q, q_target, memory, optimizer, gamma, batch_size, update_iter=10):\n",
        "    for _ in range(update_iter):\n",
        "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n",
        "        q_out = q(s)\n",
        "        a = a.unsqueeze(-1)\n",
        "        q_a = q_out.gather(2, a).squeeze(-1)\n",
        "        max_q_prime = q_target(s_prime).max(dim=2)[0]\n",
        "        target = r + gamma * max_q_prime * done_mask\n",
        "        loss = F.smooth_l1_loss(q_a, target.detach())\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "class GridMultiAgent(gym.Env):\n",
        "    metadata = {'render.modes': ['console']}\n",
        "    UP, DOWN, LEFT, RIGHT, STAY = range(5)\n",
        "    OOE, OBS, POI, MAP, AGT, SURVIVOR = -2, -1, 0, 1, 2, 3\n",
        "\n",
        "    def __init__(self, x_size=2, y_size=2, fov_x=3, fov_y=3, n_agents=2, n_survivors=1):\n",
        "        super().__init__()\n",
        "        self.x_size = x_size\n",
        "        self.y_size = y_size\n",
        "        self.n_agents = n_agents\n",
        "        self.idx_agents = list(range(n_agents))\n",
        "        self.n_survivors = n_survivors\n",
        "        self.fov_x = fov_x\n",
        "        self.fov_y = fov_y\n",
        "\n",
        "        self.confidence_map = np.zeros((x_size, y_size))\n",
        "        self.agent_paths = [[] for _ in range(n_agents)]\n",
        "\n",
        "        n_actions = 5\n",
        "        self.action_space = MultiAgentActionSpace([spaces.Discrete(n_actions) for _ in range(n_agents)])\n",
        "        obs_size = (fov_x * fov_y - 1) + 1\n",
        "        self.obs_low = np.concatenate([np.ones(fov_x * fov_y - 1) * self.OOE, [0]], dtype=np.float32)\n",
        "        self.obs_high = np.concatenate([np.ones(fov_x * fov_y - 1) * self.SURVIVOR, [1]], dtype=np.float32)\n",
        "        self.observation_space = MultiAgentObservationSpace([\n",
        "            spaces.Box(self.obs_low, self.obs_high, dtype=np.float32) for _ in range(n_agents)\n",
        "        ])\n",
        "\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "\n",
        "    def init_grid(self):\n",
        "        self.grid_status = np.zeros((self.x_size, self.y_size))\n",
        "        self.grid_counts = np.tile(self.grid_status, (self.n_agents, 1, 1)).reshape(self.n_agents, self.x_size, self.y_size)\n",
        "        self.n_poi = self.x_size * self.y_size - np.count_nonzero(self.grid_status)\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "\n",
        "    def init_agent(self):\n",
        "        self.agent_pos = []\n",
        "        self.agent_paths = [[] for _ in range(self.n_agents)]\n",
        "        for i in range(self.n_agents):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_agents_status[x, y] == self.POI:\n",
        "                    self.agent_pos.append([x, y])\n",
        "                    self.grid_agents_status[x, y] = self.AGT\n",
        "                    self.agent_paths[i].append([x, y])\n",
        "                    break\n",
        "\n",
        "    def init_survivors(self):\n",
        "        self.survivor_pos = []\n",
        "        for _ in range(self.n_survivors):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_status[x, y] == self.POI and [x, y] not in self.agent_pos:\n",
        "                    self.survivor_pos.append([x, y])\n",
        "                    self.grid_status[x, y] = self.SURVIVOR\n",
        "                    self.confidence_map[x, y] = 1.0\n",
        "                    break\n",
        "\n",
        "    def grid_overlay(self):\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            self.grid_agents_status[x, y] = self.AGT\n",
        "\n",
        "    def get_rf_signal(self, agent_pos, survivor_pos):\n",
        "        distance = np.sqrt((agent_pos[0] - survivor_pos[0])**2 + (agent_pos[1] - survivor_pos[1])**2)\n",
        "        return min(1.0, 1.0 / (distance + 1)) if distance > 0 else 1.0\n",
        "\n",
        "    def get_agent_obs(self):\n",
        "        self.agent_obs = []\n",
        "        for agent in range(self.n_agents):\n",
        "            single_obs = np.ones((self.fov_x, self.fov_y)) * self.OOE\n",
        "            x, y = self.agent_pos[agent]\n",
        "            for i in range(self.fov_x):\n",
        "                for j in range(self.fov_y):\n",
        "                    obs_x = x + (i - 1)\n",
        "                    obs_y = y + (j - 1)\n",
        "                    if 0 <= obs_x < self.x_size and 0 <= obs_y < self.y_size:\n",
        "                        single_obs[i][j] = self.grid_agents_status[obs_x][obs_y]\n",
        "            single_obs_flat = single_obs.flatten()\n",
        "            single_obs_flat = np.array([v for k, v in enumerate(single_obs_flat) if k != math.floor(self.fov_x * self.fov_y / 2)])\n",
        "            rf_signal = max([self.get_rf_signal(self.agent_pos[agent], s) for s in self.survivor_pos], default=0.0)\n",
        "            obs = np.concatenate([single_obs_flat, [rf_signal]])\n",
        "            self.agent_obs.append(obs.astype(np.float32))\n",
        "        return self.agent_obs\n",
        "\n",
        "    def update_confidence(self):\n",
        "        self.confidence_map *= 0.95\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            rf_signal = max([self.get_rf_signal(self.agent_pos[i], s) for s in self.survivor_pos], default=0.0)\n",
        "            self.confidence_map[x, y] = min(1.0, self.confidence_map[x, y] + rf_signal)\n",
        "\n",
        "    def get_coverage(self):\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        return mapped_poi / self.n_poi\n",
        "\n",
        "    def get_survivor_detection_rate(self):\n",
        "        detected = sum(1 for s in self.survivor_pos if self.confidence_map[s[0], s[1]] > 0.8)\n",
        "        return detected / self.n_survivors if self.n_survivors > 0 else 0.0\n",
        "\n",
        "    def step(self, action, i):\n",
        "        org_x, org_y = self.agent_pos[i]\n",
        "        reward = 0\n",
        "        action = int(action)\n",
        "        if action == self.UP:\n",
        "            self.agent_pos[i][1] += 1\n",
        "        elif action == self.DOWN:\n",
        "            self.agent_pos[i][1] -= 1\n",
        "        elif action == self.LEFT:\n",
        "            self.agent_pos[i][0] -= 1\n",
        "        elif action == self.RIGHT:\n",
        "            self.agent_pos[i][0] += 1\n",
        "        elif action == self.STAY:\n",
        "            reward = -1\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid action={action}\")\n",
        "\n",
        "        if (self.agent_pos[i][0] >= self.x_size or self.agent_pos[i][0] < 0 or\n",
        "            self.agent_pos[i][1] >= self.y_size or self.agent_pos[i][1] < 0 or\n",
        "            self.grid_status[self.agent_pos[i][0], self.agent_pos[i][1]] in [self.OBS, self.AGT]):\n",
        "            self.agent_pos[i] = [org_x, org_y]\n",
        "            self.grid_counts[i][org_x, org_y] += 1\n",
        "            reward = -2\n",
        "        else:\n",
        "            prev_status = self.grid_status[self.agent_pos[i][0], self.agent_pos[i][1]]\n",
        "            if prev_status == self.POI:\n",
        "                self.grid_status[self.agent_pos[i][0], self.agent_pos[i][1]] = self.MAP\n",
        "                self.grid_counts[i][self.agent_pos[i][0], self.agent_pos[i][1]] += 1\n",
        "                reward = 5\n",
        "            elif prev_status == self.SURVIVOR:\n",
        "                self.grid_counts[i][self.agent_pos[i][0], self.agent_pos[i][1]] += 1\n",
        "                reward = 50\n",
        "            elif prev_status == self.MAP:\n",
        "                self.grid_counts[i][self.agent_pos[i][0], self.agent_pos[i][1]] += 1\n",
        "                reward = -1\n",
        "\n",
        "        self.agent_paths[i].append(self.agent_pos[i][:])\n",
        "\n",
        "        nearby_agents = sum(1 for j in range(self.n_agents) if j != i and\n",
        "                           np.sqrt((self.agent_pos[i][0] - self.agent_pos[j][0])**2 +\n",
        "                                   (self.agent_pos[i][1] - self.agent_pos[j][1])**2) < 2)\n",
        "        if nearby_agents > 0:\n",
        "            reward -= 2 * nearby_agents\n",
        "\n",
        "        self.update_confidence()\n",
        "        self.grid_overlay()\n",
        "\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        done = mapped_poi == self.n_poi or self.get_survivor_detection_rate() == 1.0\n",
        "        info = {'survivor_detection_rate': self.get_survivor_detection_rate()}\n",
        "\n",
        "        return self.get_agent_obs(), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "        self.confidence_map = np.zeros((self.x_size, self.y_size))\n",
        "        return self.get_agent_obs()\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "# Training\n",
        "size = 2\n",
        "fov = 3\n",
        "n_agents = 2\n",
        "n_survivors = 1\n",
        "train_episodes = 1000\n",
        "max_steps = 8\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "buffer_limit = 50000\n",
        "log_interval = 20\n",
        "max_epsilon = 0.9\n",
        "min_epsilon = 0.1\n",
        "warm_up_steps = 1000\n",
        "update_iter = 10\n",
        "lr = 0.001\n",
        "\n",
        "env = GridMultiAgent(x_size=size, y_size=size, fov_x=fov, fov_y=fov, n_agents=n_agents, n_survivors=n_survivors)\n",
        "memory = ReplayBuffer(buffer_limit)\n",
        "q = QNet(env.observation_space, env.action_space)\n",
        "q_target = QNet(env.observation_space, env.action_space)\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "optimizer = optim.Adam(q.parameters(), lr=lr)\n",
        "\n",
        "time_steps, epsilons, coverage, survivor_detection, total_reward, all_agent_paths = [], [], [], [], [], []\n",
        "\n",
        "for episode in tqdm(range(train_episodes), desc=\"Training Episodes\"):\n",
        "    score = np.zeros(n_agents)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_step = 0\n",
        "    epsilon = max(min_epsilon, max_epsilon - (max_epsilon - min_epsilon) * (episode / (0.4 * train_episodes)))\n",
        "\n",
        "    while not done:\n",
        "        state_np = np.array(state, dtype=np.float32)\n",
        "        action = q.sample_action(torch.tensor(state_np).unsqueeze(0), epsilon)[0].data.cpu().numpy()\n",
        "        rewards = np.zeros(n_agents)\n",
        "        next_state = state\n",
        "        for agent_i in env.idx_agents:\n",
        "            next_state, reward, done, info = env.step(action[agent_i], agent_i)\n",
        "            rewards[agent_i] = reward\n",
        "            if done:\n",
        "                break\n",
        "        memory.put((state, action, rewards, next_state, [done] * n_agents))\n",
        "        score += rewards\n",
        "        state = next_state\n",
        "        episode_step += 1\n",
        "        if episode_step >= max_steps or done:\n",
        "            break\n",
        "\n",
        "    if memory.size() > warm_up_steps:\n",
        "        train(q, q_target, memory, optimizer, gamma, batch_size, update_iter)\n",
        "\n",
        "    if episode % log_interval == 0:\n",
        "        q_target.load_state_dict(q.state_dict())\n",
        "\n",
        "    all_agent_paths.append([path[:] for path in env.agent_paths])\n",
        "    time_steps.append(episode_step)\n",
        "    epsilons.append(epsilon)\n",
        "    coverage.append(env.get_coverage())\n",
        "    survivor_detection.append(info['survivor_detection_rate'])\n",
        "    total_reward.append(score.sum())\n",
        "\n",
        "    if episode % log_interval == 0:\n",
        "        print(f'//Episode {episode+1}// Epsilon: {epsilon:.3f}, Steps: {episode_step}, '\n",
        "              f'Coverage (%): {coverage[-1]:.3f}, Survivor Detection (%): {survivor_detection[-1]:.3f}, '\n",
        "              f'Total Reward: {total_reward[-1]}')\n",
        "\n",
        "# Metrics Visualization\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(coverage, label='Coverage')\n",
        "plt.title('Coverage Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Coverage (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(survivor_detection, label='Survivor Detection', color='orange')\n",
        "plt.title('Survivor Detection Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Detection Rate (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.plot(total_reward, label='Total Reward', color='green')\n",
        "plt.title('Total Reward Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('metrics_simple1000.png')\n",
        "plt.close()\n",
        "\n",
        "# Drone Path Animation (Last Episode)\n",
        "def animate_drone_paths(episode_idx=-1):\n",
        "    paths = all_agent_paths[episode_idx]\n",
        "    final_grid = env.grid_status\n",
        "    final_confidence = env.confidence_map\n",
        "    final_survivors = env.survivor_pos\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xlim(-0.5, env.x_size - 0.5)\n",
        "    ax.set_ylim(-0.5, env.y_size - 0.5)\n",
        "    ax.set_xticks(range(env.x_size))\n",
        "    ax.set_yticks(range(env.y_size))\n",
        "    ax.grid(True)\n",
        "    ax.set_title(f'Drone Paths (Episode {episode_idx + 1})')\n",
        "\n",
        "    obs_x, obs_y = np.where(final_grid == env.OBS)\n",
        "    ax.scatter(obs_x, obs_y, c='black', marker='s', s=100, label='Obstacles')\n",
        "    heatmap = ax.imshow(final_confidence.T, cmap='hot', alpha=0.5, origin='lower',\n",
        "                        extent=(-0.5, env.x_size - 0.5, -0.5, env.y_size - 0.5))\n",
        "    survivor_x, survivor_y = zip(*final_survivors) if final_survivors else ([], [])\n",
        "    survivors = ax.scatter(survivor_x, survivor_y, c='blue', marker='*', s=200, label='Survivors')\n",
        "    colors = ['red', 'green']\n",
        "    agent_plots = [ax.plot([], [], c=colors[i], marker='o', linestyle='-', label=f'Agent {i}')[0]\n",
        "                   for i in range(n_agents)]\n",
        "    ax.legend()\n",
        "\n",
        "    def update(frame):\n",
        "        for i, plot in enumerate(agent_plots):\n",
        "            if frame < len(paths[i]):\n",
        "                x, y = zip(*paths[i][:frame + 1]) if paths[i] else ([], [])\n",
        "                plot.set_data(x, y)\n",
        "        return agent_plots + [heatmap, survivors]\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=min(len(p) for p in paths), interval=200, blit=True)\n",
        "    plt.close(fig)\n",
        "    return ani\n",
        "\n",
        "# Save animation\n",
        "ani = animate_drone_paths(-1)\n",
        "with open('animation_simple.html', 'w') as f:\n",
        "    f.write(ani.to_jshtml())\n",
        "\n",
        "# Save metrics for comparison\n",
        "np.savez('metrics_simple.npz', coverage=coverage, survivor_detection=survivor_detection, total_reward=total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDGKeKowOuLf",
        "outputId": "df94e49d-37f8-4aed-fab3-5ced2f566099"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:   2%|▏         | 16/1000 [00:00<00:06, 154.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 1// Epsilon: 0.900, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n",
            "//Episode 21// Epsilon: 0.860, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:   5%|▍         | 46/1000 [00:00<00:07, 122.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 41// Epsilon: 0.820, Steps: 5, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 31.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:   7%|▋         | 70/1000 [00:00<00:11, 81.97it/s] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 61// Epsilon: 0.780, Steps: 5, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 36.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:   9%|▊         | 87/1000 [00:01<00:14, 64.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 81// Epsilon: 0.740, Steps: 5, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 37.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  10%|█         | 105/1000 [00:01<00:12, 69.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 101// Epsilon: 0.700, Steps: 3, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 39.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  12%|█▎        | 125/1000 [00:01<00:11, 74.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 121// Epsilon: 0.660, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  15%|█▍        | 148/1000 [00:02<00:15, 55.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 141// Epsilon: 0.620, Steps: 4, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 39.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  16%|█▋        | 164/1000 [00:02<00:15, 54.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 161// Epsilon: 0.580, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  19%|█▉        | 189/1000 [00:02<00:16, 49.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 181// Epsilon: 0.540, Steps: 5, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 39.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  22%|██▏       | 224/1000 [00:03<00:09, 84.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 201// Epsilon: 0.500, Steps: 3, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 39.0\n",
            "//Episode 221// Epsilon: 0.460, Steps: 8, Coverage (%): 0.750, Survivor Detection (%): 0.000, Total Reward: -39.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  25%|██▌       | 252/1000 [00:03<00:06, 109.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 241// Epsilon: 0.420, Steps: 3, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 47.0\n",
            "//Episode 261// Epsilon: 0.380, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  28%|██▊       | 280/1000 [00:05<00:38, 18.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 281// Epsilon: 0.340, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  30%|███       | 302/1000 [00:07<00:45, 15.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 301// Epsilon: 0.300, Steps: 8, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 18.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  32%|███▎      | 325/1000 [00:08<00:40, 16.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 321// Epsilon: 0.260, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 43.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  34%|███▍      | 344/1000 [00:09<00:30, 21.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 341// Epsilon: 0.220, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  36%|███▋      | 365/1000 [00:10<00:29, 21.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 361// Epsilon: 0.180, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  38%|███▊      | 383/1000 [00:11<00:30, 20.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 381// Epsilon: 0.140, Steps: 3, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  40%|████      | 404/1000 [00:12<00:29, 20.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 401// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  42%|████▎     | 425/1000 [00:13<00:27, 20.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 421// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  44%|████▍     | 443/1000 [00:14<00:27, 20.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 441// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  46%|████▋     | 464/1000 [00:15<00:26, 20.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 461// Epsilon: 0.100, Steps: 3, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 53.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  48%|████▊     | 485/1000 [00:16<00:23, 21.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 481// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  50%|█████     | 503/1000 [00:17<00:23, 21.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 501// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  52%|█████▏    | 524/1000 [00:18<00:22, 20.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 521// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  54%|█████▍    | 542/1000 [00:19<00:33, 13.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 541// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 54.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  56%|█████▌    | 562/1000 [00:21<00:33, 12.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 561// Epsilon: 0.100, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  58%|█████▊    | 584/1000 [00:22<00:24, 16.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 581// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  60%|██████    | 603/1000 [00:23<00:18, 21.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 601// Epsilon: 0.100, Steps: 3, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 46.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  62%|██████▏   | 624/1000 [00:24<00:16, 22.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 621// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  64%|██████▍   | 645/1000 [00:25<00:16, 21.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 641// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 50.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  66%|██████▋   | 663/1000 [00:26<00:14, 22.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 661// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  68%|██████▊   | 684/1000 [00:27<00:14, 21.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 681// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  70%|███████   | 705/1000 [00:28<00:13, 21.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 701// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  72%|███████▏  | 723/1000 [00:29<00:13, 21.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 721// Epsilon: 0.100, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 47.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  74%|███████▍  | 744/1000 [00:30<00:11, 21.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 741// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  76%|███████▋  | 765/1000 [00:31<00:11, 20.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 761// Epsilon: 0.100, Steps: 2, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 57.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  78%|███████▊  | 783/1000 [00:31<00:10, 21.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 781// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  80%|████████  | 804/1000 [00:33<00:12, 15.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 801// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  82%|████████▏ | 822/1000 [00:34<00:10, 16.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 821// Epsilon: 0.100, Steps: 2, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 57.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  84%|████████▍ | 842/1000 [00:35<00:11, 13.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 841// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  86%|████████▌ | 862/1000 [00:36<00:06, 21.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 861// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  88%|████████▊ | 883/1000 [00:37<00:05, 21.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 881// Epsilon: 0.100, Steps: 2, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  90%|█████████ | 904/1000 [00:38<00:04, 21.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 901// Epsilon: 0.100, Steps: 3, Coverage (%): 0.750, Survivor Detection (%): 1.000, Total Reward: 54.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  92%|█████████▎| 925/1000 [00:39<00:03, 20.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 921// Epsilon: 0.100, Steps: 1, Coverage (%): 0.250, Survivor Detection (%): 1.000, Total Reward: 51.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  94%|█████████▍| 943/1000 [00:40<00:02, 23.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 941// Epsilon: 0.100, Steps: 1, Coverage (%): 0.000, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  96%|█████████▋| 964/1000 [00:41<00:01, 21.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 961// Epsilon: 0.100, Steps: 3, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 48.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  98%|█████████▊| 985/1000 [00:42<00:00, 20.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 981// Epsilon: 0.100, Steps: 2, Coverage (%): 0.500, Survivor Detection (%): 1.000, Total Reward: 50.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes: 100%|██████████| 1000/1000 [00:43<00:00, 23.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Load metrics from all implementations\n",
        "metrics_original = np.load('metrics_original.npz')\n",
        "metrics_upgraded = np.load('metrics_upgraded.npz')\n",
        "metrics_simple = np.load('metrics_simple.npz')\n",
        "\n",
        "# Extract metrics\n",
        "coverage = {\n",
        "    'Original': metrics_original['coverage'],\n",
        "    'Upgraded': metrics_upgraded['coverage'],\n",
        "    'Simple': metrics_simple['coverage']\n",
        "}\n",
        "survivor_detection = {\n",
        "    'Original': metrics_original['survivor_detection'],\n",
        "    'Upgraded': metrics_upgraded['survivor_detection'],\n",
        "    'Simple': metrics_simple['survivor_detection']\n",
        "}\n",
        "total_reward = {\n",
        "    'Original': metrics_original['total_reward'],\n",
        "    'Upgraded': metrics_upgraded['total_reward'],\n",
        "    'Simple': metrics_simple['total_reward']\n",
        "}\n",
        "\n",
        "# Create table\n",
        "data = {\n",
        "    'Implementation': ['Original', 'Upgraded', 'Simple'],\n",
        "    'Avg. Coverage (%)': [np.mean(coverage['Original']), np.mean(coverage['Upgraded']), np.mean(coverage['Simple'])],\n",
        "    'Avg. Survivor Detection (%)': [np.mean(survivor_detection['Original']), np.mean(survivor_detection['Upgraded']), np.mean(survivor_detection['Simple'])],\n",
        "    'Avg. Total Reward': [np.mean(total_reward['Original']), np.mean(total_reward['Upgraded']), np.mean(total_reward['Simple'])]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df = df.round(2)\n",
        "table_html = df.to_html(index=False, classes='table table-striped', justify='center')\n",
        "\n",
        "# Plot metrics\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "for key in coverage:\n",
        "    plt.plot(coverage[key], label=key)\n",
        "plt.title('Coverage Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Coverage (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "for key in survivor_detection:\n",
        "    plt.plot(survivor_detection[key], label=key)\n",
        "plt.title('Survivor Detection Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Detection Rate (%)')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "for key in total_reward:\n",
        "    plt.plot(total_reward[key], label=key)\n",
        "plt.title('Total Reward Over Episodes')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('comparison_metrics.png')\n",
        "plt.close()\n",
        "\n",
        "# Display table\n",
        "display(HTML(table_html))\n",
        "\n",
        "# Instructions to view animations\n",
        "print(\"To view animations, open the following files in a browser:\")\n",
        "print(\"- Original: animation_original.html\")\n",
        "print(\"- Upgraded: animation_upgraded.html\")\n",
        "print(\"- Simple: animation_simple.html\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "NT79iUniP3_6",
        "outputId": "fe102f48-d9c6-43b3-f13a-5c704df90118"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe table table-striped\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: center;\">\n",
              "      <th>Implementation</th>\n",
              "      <th>Avg. Coverage (%)</th>\n",
              "      <th>Avg. Survivor Detection (%)</th>\n",
              "      <th>Avg. Total Reward</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Original</td>\n",
              "      <td>0.16</td>\n",
              "      <td>0.65</td>\n",
              "      <td>-10.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Upgraded</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.68</td>\n",
              "      <td>-4.97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Simple</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.60</td>\n",
              "      <td>5.14</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To view animations, open the following files in a browser:\n",
            "- Original: animation_original.html\n",
            "- Upgraded: animation_upgraded.html\n",
            "- Simple: animation_simple.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric\n",
        "!pip install torch-scatter torch-sparse torch-cluster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8FifoH_QfWb",
        "outputId": "ff15ff9f-74af-4140-e04e-db69eef94ab4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.4.26)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n",
            "Collecting torch-scatter\n",
            "  Downloading torch_scatter-2.1.2.tar.gz (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-sparse\n",
            "  Downloading torch_sparse-0.6.18.tar.gz (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch-cluster\n",
            "  Downloading torch_cluster-1.6.3.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Building wheels for collected packages: torch-scatter, torch-sparse, torch-cluster\n",
            "  Building wheel for torch-scatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-scatter: filename=torch_scatter-2.1.2-cp311-cp311-linux_x86_64.whl size=547368 sha256=91ebca7753117ec38de1ec0b87f8195bfceb756bf03a8de57ac0258211f2002a\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/d4/0e/a80af2465354ea7355a2c153b11af2da739cfcf08b6c0b28e2\n",
            "  Building wheel for torch-sparse (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0mTraceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 179, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/req_command.py\", line 67, in wrapper\n",
            "    return func(self, options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/commands/install.py\", line 423, in run\n",
            "    _, build_failures = build(\n",
            "                        ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/wheel_builder.py\", line 319, in build\n",
            "    wheel_file = _build_one(\n",
            "                 ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/wheel_builder.py\", line 193, in _build_one\n",
            "    wheel_path = _build_one_inside_env(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/wheel_builder.py\", line 240, in _build_one_inside_env\n",
            "    wheel_path = build_wheel_legacy(\n",
            "                 ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/operations/build/wheel_legacy.py\", line 83, in build_wheel_legacy\n",
            "    output = call_subprocess(\n",
            "             ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/subprocess.py\", line 151, in call_subprocess\n",
            "    line: str = proc.stdout.readline()\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen codecs>\", line 319, in decode\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/main.py\", line 80, in main\n",
            "    return command.main(cmd_args)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 100, in main\n",
            "    return self._main(args)\n",
            "           ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 232, in _main\n",
            "    return run(options, args)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/cli/base_command.py\", line 216, in exc_logging_wrapper\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1477, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1634, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1644, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1706, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 978, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.11/logging/handlers.py\", line 75, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1230, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 1110, in emit\n",
            "    msg = self.format(record)\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 953, in format\n",
            "    return fmt.format(record)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pip/_internal/utils/logging.py\", line 112, in format\n",
            "    formatted = super().format(record)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 695, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/logging/__init__.py\", line 645, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.11/traceback.py\", line 124, in print_exception\n",
            "    te = TracebackException(type(value), value, tb, limit=limit, compact=True)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/traceback.py\", line 728, in __init__\n",
            "    self.stack = StackSummary._extract_from_extended_frame_gen(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/traceback.py\", line 433, in _extract_from_extended_frame_gen\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.11/traceback.py\", line 318, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/linecache.py\", line 30, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/linecache.py\", line 46, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tokenize.py\", line 398, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tokenize.py\", line 367, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "            ^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/tokenize.py\", line 325, in read_or_stop\n",
            "    return readline()\n",
            "           ^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip install -q torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "!pip install -q numpy torch gym tqdm matplotlib ipython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39eHym8BXj1y",
        "outputId": "d0c3b1eb-5153-4b93-9f03-f4ab7aec39a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 19\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m989.8/989.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dependencies should be installed (torch-geometric, torch-scatter, torch-sparse, torch-cluster).\n",
        "# If session reset, reinstall with:\n",
        "# !pip cache purge\n",
        "# !pip install -q torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html\n",
        "# !pip install -q numpy torch gym tqdm matplotlib ipython\n",
        "\n",
        "import collections\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from gym import spaces\n",
        "import gym\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from IPython.display import HTML\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data, Batch\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Verify environment\n",
        "print(f\"PyTorch Version: {torch.__version__}, CUDA Available: {torch.cuda.is_available()}\")\n",
        "print(\"torch-geometric imported successfully!\")\n",
        "\n",
        "class MultiAgentActionSpace(list):\n",
        "    def __init__(self, agents_action_space):\n",
        "        for x in agents_action_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_action_space)\n",
        "        self._agents_action_space = agents_action_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_action_space.sample() for agent_action_space in self._agents_action_space]\n",
        "\n",
        "class MultiAgentObservationSpace(list):\n",
        "    def __init__(self, agents_observation_space):\n",
        "        for x in agents_observation_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_observation_space)\n",
        "        self._agents_observation_space = agents_observation_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_observation_space.sample() for agent_observation_space in self._agents_observation_space]\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        if len(transition) != 7:\n",
        "            raise ValueError(f\"Transition tuple must have exactly 7 elements, got {len(transition)}\")\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst, graph_s_lst, graph_s_prime_lst = [], [], [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done, graph_s, graph_s_prime = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append((np.ones(len(done)) - done).tolist())\n",
        "            graph_s_lst.append(graph_s)\n",
        "            graph_s_prime_lst.append(graph_s_prime)\n",
        "\n",
        "        return (torch.tensor(np.array(s_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(a_lst), dtype=torch.long),\n",
        "                torch.tensor(np.array(r_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(s_prime_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(done_mask_lst), dtype=torch.float),\n",
        "                graph_s_lst,\n",
        "                graph_s_prime_lst)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, observation_space, action_space, fov_x=3, fov_y=3):\n",
        "        super(QNet, self).__init__()\n",
        "        self.num_agents = len(observation_space)\n",
        "        self.fov_x, self.fov_y = fov_x, fov_y\n",
        "        self.n_actions = action_space[0].n\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * fov_x * fov_y, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.non_spatial_size = 1 + 3 + 3\n",
        "        self.fc_non_spatial = nn.Sequential(\n",
        "            nn.Linear(self.non_spatial_size, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_shared = nn.Sequential(\n",
        "            nn.Linear(64 + 16, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.heads = nn.ModuleList([nn.Linear(64, self.n_actions) for _ in range(self.num_agents)])\n",
        "\n",
        "    def forward(self, obs):\n",
        "        batch_size = obs.shape[0]\n",
        "        spatial_obs = torch.zeros(batch_size, self.num_agents, 1, self.fov_x, self.fov_y, device=obs.device)\n",
        "        for agent_i in range(self.num_agents):\n",
        "            grid_status = obs[:, agent_i, :self.fov_x * fov_y].reshape(-1, self.fov_x, self.fov_y)\n",
        "            spatial_obs[:, agent_i, 0] = grid_status\n",
        "        spatial_out = torch.zeros(batch_size, self.num_agents, 64, device=obs.device)\n",
        "        for agent_i in range(self.num_agents):\n",
        "            spatial_out[:, agent_i] = self.cnn(spatial_obs[:, agent_i])\n",
        "        non_spatial = obs[:, :, -self.non_spatial_size:]\n",
        "        non_spatial_out = self.fc_non_spatial(non_spatial.view(-1, self.non_spatial_size)).view(batch_size, self.num_agents, 16)\n",
        "        combined = torch.cat([spatial_out, non_spatial_out], dim=2)\n",
        "        shared_out = self.fc_shared(combined)\n",
        "        q_values = [self.heads[agent_i](shared_out[:, agent_i]).unsqueeze(1) for agent_i in range(self.num_agents)]\n",
        "        return torch.cat(q_values, dim=1)\n",
        "\n",
        "    def sample_action(self, obs, epsilon, roles, grid_status, agent_pos, x_size, y_size, stuck_counts, visited_counts):\n",
        "        out = self.forward(obs)\n",
        "        actions = torch.zeros((out.shape[1],), dtype=torch.long)\n",
        "        exploration_bonus = 50.0\n",
        "        scout_epsilon_boost = 0.4\n",
        "        for agent_i in range(out.shape[1]):\n",
        "            agent_epsilon = epsilon\n",
        "            if roles[agent_i] == 'scout' and stuck_counts[agent_i] > 2:\n",
        "                agent_epsilon = min(1.0, epsilon + scout_epsilon_boost)\n",
        "            if random.random() < agent_epsilon:\n",
        "                if roles[agent_i] == 'relay':\n",
        "                    actions[agent_i] = 4  # STAY\n",
        "                else:\n",
        "                    valid_actions = []\n",
        "                    poi_actions = []\n",
        "                    low_visit_actions = []\n",
        "                    x, y = agent_pos[agent_i]\n",
        "                    for a in range(4):  # UP, DOWN, LEFT, RIGHT\n",
        "                        new_x, new_y = x, y\n",
        "                        if a == 0: new_y += 1\n",
        "                        elif a == 1: new_y -= 1\n",
        "                        elif a == 2: new_x -= 1\n",
        "                        elif a == 3: new_x += 1\n",
        "                        if 0 <= new_x < x_size and 0 <= new_y < y_size and grid_status[new_x, new_y] in [0, 3]:\n",
        "                            valid_actions.append(a)\n",
        "                            if grid_status[new_x, new_y] == 0:\n",
        "                                poi_actions.append(a)\n",
        "                            if visited_counts[new_x, new_y] < 2:\n",
        "                                low_visit_actions.append(a)\n",
        "                    actions[agent_i] = random.choice(low_visit_actions or poi_actions or valid_actions) if valid_actions else 4\n",
        "            else:\n",
        "                q_values = out[0, agent_i].clone()\n",
        "                x, y = agent_pos[agent_i]\n",
        "                max_neighbor_rf = obs[0, agent_i, -2].item()\n",
        "                for a in range(4):  # UP, DOWN, LEFT, RIGHT\n",
        "                    new_x, new_y = x, y\n",
        "                    if a == 0: new_y += 1\n",
        "                    elif a == 1: new_y -= 1\n",
        "                    elif a == 2: new_x -= 1\n",
        "                    elif a == 3: new_x += 1\n",
        "                    if 0 <= new_x < x_size and 0 <= new_y < y_size:\n",
        "                        if grid_status[new_x, new_y] == 0:\n",
        "                            q_values[a] += exploration_bonus\n",
        "                        elif grid_status[new_x, new_y] == 3:\n",
        "                            q_values[a] += exploration_bonus * (2 + max_neighbor_rf)\n",
        "                        if visited_counts[new_x, new_y] < 2:\n",
        "                            q_values[a] += exploration_bonus * 0.5\n",
        "                actions[agent_i] = q_values.argmax().item()\n",
        "        return actions\n",
        "\n",
        "class QCentralGNN(nn.Module):\n",
        "    def __init__(self, node_feature_size, n_agents, n_actions):\n",
        "        super().__init__()\n",
        "        self.n_agents = n_agents\n",
        "        self.n_actions = n_actions\n",
        "        self.gat1 = GATConv(node_feature_size, 64, heads=4, concat=True)\n",
        "        self.gat2 = GATConv(64 * 4, 64, heads=1, concat=True)\n",
        "        self.fc = nn.Linear(64, n_actions)\n",
        "\n",
        "    def forward(self, graph_data):\n",
        "        x, edge_index = graph_data.x, graph_data.edge_index\n",
        "        x = F.relu(self.gat1(x, edge_index))\n",
        "        x = F.relu(self.gat2(x, edge_index))\n",
        "        x = self.fc(x)\n",
        "        return x.view(-1, self.n_agents, self.n_actions)\n",
        "\n",
        "class GridMultiAgent(gym.Env):\n",
        "    metadata = {'render.modes': ['console']}\n",
        "    UP, DOWN, LEFT, RIGHT, STAY = range(5)\n",
        "    OOE, OBS, POI, MAP, AGT, SURVIVOR = -2, -1, 0, 1, 2, 3\n",
        "\n",
        "    def __init__(self, x_size=2, y_size=2, fov_x=3, fov_y=3, n_agents=2, n_survivors=1, comm_range=2.0):\n",
        "        super().__init__()\n",
        "        self.x_size = x_size\n",
        "        self.y_size = y_size\n",
        "        self.n_agents = n_agents\n",
        "        self.idx_agents = list(range(n_agents))\n",
        "        self.n_survivors = n_survivors\n",
        "        self.fov_x = fov_x\n",
        "        self.fov_y = fov_y\n",
        "        self.comm_range = comm_range\n",
        "        self.max_relays = n_agents // 2\n",
        "        self.agent_roles = ['scout'] * n_agents\n",
        "        self.stuck_counts = [0] * n_agents\n",
        "        self.visited_counts = np.zeros((x_size, y_size))\n",
        "        self.survivor_pos = []\n",
        "        self.agent_paths = [[] for _ in range(n_agents)]\n",
        "        self.grid_status_history = []\n",
        "        self.adjacency_matrix = np.zeros((n_agents, n_agents))\n",
        "        self.detection_map = np.zeros((x_size, y_size))\n",
        "        n_actions = 5\n",
        "        self.action_space = MultiAgentActionSpace([spaces.Discrete(n_actions) for _ in range(n_agents)])\n",
        "        obs_size = (fov_x * fov_y) + 1 + 3 + 3\n",
        "        self.obs_low = np.concatenate([np.ones(fov_x * fov_y) * self.OOE, [0, 0, 0, 0, 0, 0, 0]], dtype=np.float32)\n",
        "        self.obs_high = np.concatenate([np.ones(fov_x * fov_y) * self.SURVIVOR, [1, 1, 1, 1, n_agents, 1, 3]], dtype=np.float32)\n",
        "        self.observation_space = MultiAgentObservationSpace([\n",
        "            spaces.Box(self.obs_low, self.obs_high, dtype=np.float32) for _ in range(n_agents)\n",
        "        ])\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "\n",
        "    def init_grid(self):\n",
        "        self.grid_status = np.zeros((self.x_size, self.y_size))\n",
        "        self.grid_counts = np.tile(self.grid_status, (self.n_agents, 1, 1)).reshape(self.n_agents, self.x_size, self.y_size)\n",
        "        self.n_poi = self.x_size * self.y_size - np.count_nonzero(self.grid_status)\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "        self.grid_status_history = [self.grid_status.copy()]\n",
        "        self.visited_counts = np.zeros((self.x_size, self.y_size))\n",
        "\n",
        "    def init_agent(self):\n",
        "        self.agent_pos = []\n",
        "        self.agent_paths = [[] for _ in range(self.n_agents)]\n",
        "        for i in range(self.n_agents):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_agents_status[x, y] == self.POI:\n",
        "                    self.agent_pos.append([x, y])\n",
        "                    self.grid_agents_status[x, y] = self.AGT\n",
        "                    self.agent_paths[i].append([x, y])\n",
        "                    self.visited_counts[x, y] += 1\n",
        "                    break\n",
        "        self.stuck_counts = [0] * n_agents\n",
        "\n",
        "    def init_survivors(self):\n",
        "        self.survivor_pos = []\n",
        "        for _ in range(self.n_survivors):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_status[x, y] == self.POI and [x, y] not in self.agent_pos:\n",
        "                    self.survivor_pos.append([x, y])\n",
        "                    self.grid_status[x, y] = self.SURVIVOR\n",
        "                    break\n",
        "\n",
        "    def compute_network(self):\n",
        "        self.adjacency_matrix = np.zeros((self.n_agents, self.n_agents))\n",
        "        for i in range(self.n_agents):\n",
        "            for j in range(i + 1, self.n_agents):\n",
        "                dist = np.sqrt((self.agent_pos[i][0] - self.agent_pos[j][0])**2 +\n",
        "                               (self.agent_pos[i][1] - self.agent_pos[j][1])**2)\n",
        "                if dist <= self.comm_range:\n",
        "                    self.adjacency_matrix[i, j] = 1\n",
        "                    self.adjacency_matrix[j, i] = 1\n",
        "\n",
        "    def get_neighbor_messages(self, agent_idx):\n",
        "        neighbors = np.where(self.adjacency_matrix[agent_idx] == 1)[0]\n",
        "        n_neighbors = len(neighbors)\n",
        "        max_rf = 0.0\n",
        "        neighbor_role = 0\n",
        "        for neighbor in neighbors:\n",
        "            rf_signals = [self.get_rf_signal(self.agent_pos[neighbor], s) for s in self.survivor_pos]\n",
        "            max_rf = max(max_rf, max(rf_signals, default=0.0))\n",
        "            role_map = {'scout': 0, 'verification': 1, 'relay': 2}\n",
        "            neighbor_role = max(neighbor_role, role_map[self.agent_roles[neighbor]])\n",
        "        return n_neighbors, max_rf, neighbor_role\n",
        "\n",
        "    def grid_overlay(self):\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            self.grid_agents_status[x, y] = self.AGT\n",
        "\n",
        "    def get_rf_signal(self, agent_pos, survivor_pos):\n",
        "        distance = np.sqrt((agent_pos[0] - survivor_pos[0])**2 + (agent_pos[1] - survivor_pos[1])**2)\n",
        "        return min(1.0, 1.0 / (distance + 0.5))\n",
        "\n",
        "    def get_graph_state(self):\n",
        "        agent_obs = self.get_agent_obs()\n",
        "        node_features = np.array(agent_obs, dtype=np.float32)\n",
        "        edge_index = []\n",
        "        for i in range(self.n_agents):\n",
        "            for j in range(self.n_agents):\n",
        "                if self.adjacency_matrix[i, j] == 1:\n",
        "                    edge_index.append([i, j])\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous() if edge_index else torch.empty((2, 0), dtype=torch.long)\n",
        "        graph_data = Data(\n",
        "            x=torch.tensor(node_features, dtype=torch.float),\n",
        "            edge_index=edge_index\n",
        "        )\n",
        "        return graph_data\n",
        "\n",
        "    def get_agent_obs(self):\n",
        "        self.compute_network()\n",
        "        self.agent_obs = []\n",
        "        for agent in range(self.n_agents):\n",
        "            single_obs = np.ones((self.fov_x, self.fov_y)) * self.OOE\n",
        "            x, y = self.agent_pos[agent]\n",
        "            for s in self.survivor_pos:\n",
        "                rf = self.get_rf_signal([x, y], s)\n",
        "                if rf > 0.6:\n",
        "                    sx, sy = s\n",
        "                    self.detection_map[sx, sy] += 1\n",
        "            for i in range(self.fov_x):\n",
        "                for j in range(self.fov_y):\n",
        "                    obs_x = x + (i - self.fov_x // 2)\n",
        "                    obs_y = y + (j - self.fov_y // 2)\n",
        "                    if 0 <= obs_x < self.x_size and 0 <= obs_y < self.y_size:\n",
        "                        single_obs[i][j] = self.grid_agents_status[obs_x][obs_y]\n",
        "            single_obs_flat = single_obs.flatten()\n",
        "            rf_signals = [self.get_rf_signal(self.agent_pos[agent], s) for s in self.survivor_pos]\n",
        "            rf_signal = max(rf_signals, default=0.0)\n",
        "            role_encoding = {'scout': [1, 0, 0], 'verification': [0, 1, 0], 'relay': [0, 0, 1]}\n",
        "            role_vec = role_encoding[self.agent_roles[agent]]\n",
        "            n_neighbors, max_neighbor_rf, neighbor_role = self.get_neighbor_messages(agent)\n",
        "            obs = np.concatenate([single_obs_flat, [rf_signal], role_vec, [n_neighbors, max_neighbor_rf, neighbor_role]])\n",
        "            self.agent_obs.append(obs.astype(np.float32))\n",
        "        return self.agent_obs\n",
        "\n",
        "    def update_roles(self):\n",
        "        relay_count = sum(1 for r in self.agent_roles if r == 'relay')\n",
        "        detected_survivors = sum(1 for s in self.survivor_pos if\n",
        "                                 max(self.get_rf_signal([ax, ay], s) for ax, ay in self.agent_pos) > 0.6)\n",
        "        for i in range(self.n_agents):\n",
        "            rf_signals = [self.get_rf_signal(self.agent_pos[i], s) for s in self.survivor_pos]\n",
        "            max_rf = max(rf_signals, default=0.0)\n",
        "            n_neighbors, max_neighbor_rf, _ = self.get_neighbor_messages(i)\n",
        "            if (max_rf > 0.6 or max_neighbor_rf > 0.6) and self.agent_roles[i] != 'verification':\n",
        "                self.agent_roles[i] = 'verification'\n",
        "            elif detected_survivors < self.n_survivors and self.agent_roles[i] != 'relay' and relay_count < self.max_relays:\n",
        "                self.agent_roles[i] = 'relay'\n",
        "                relay_count += 1\n",
        "            elif detected_survivors < self.n_survivors and self.agent_roles[i] != 'scout':\n",
        "                self.agent_roles[i] = 'scout'\n",
        "\n",
        "    def get_coverage(self):\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        return mapped_poi / self.n_poi if self.n_poi > 0 else 0.0\n",
        "\n",
        "    def get_survivor_detection_rate(self):\n",
        "        detected = sum(1 for s in self.survivor_pos if\n",
        "                       max(self.get_rf_signal([ax, ay], s) for ax, ay in self.agent_pos) > 0.6)\n",
        "        return detected / self.n_survivors if self.n_survivors > 0 else 0.0\n",
        "\n",
        "    def step(self, action, i):\n",
        "        org_x, org_y = self.agent_pos[i]\n",
        "        reward = 0\n",
        "        action = int(action)\n",
        "        new_x, new_y = org_x, org_y\n",
        "        if action == self.UP:\n",
        "            new_y += 1\n",
        "        elif action == self.DOWN:\n",
        "            new_y -= 1\n",
        "        elif action == self.LEFT:\n",
        "            new_x -= 1\n",
        "        elif action == self.RIGHT:\n",
        "            new_x += 1\n",
        "        elif action == self.STAY:\n",
        "            rf_signals = [self.get_rf_signal(self.agent_pos[i], s) for s in self.survivor_pos]\n",
        "            max_rf = max(rf_signals, default=0.0)\n",
        "            reward = 5 if self.agent_roles[i] == 'relay' and max_rf > 0.5 else -2\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid action={action}\")\n",
        "        if (0 <= new_x < self.x_size and 0 <= new_y < self.y_size and\n",
        "            self.grid_status[new_x, new_y] not in [self.OBS]):\n",
        "            collision = False\n",
        "            for j in range(self.n_agents):\n",
        "                if i != j and self.agent_pos[j][0] == new_x and self.agent_pos[j][1] == new_y:\n",
        "                    collision = True\n",
        "                    break\n",
        "            if not collision:\n",
        "                self.agent_pos[i] = [new_x, new_y]\n",
        "                self.visited_counts[new_x, new_y] += 1\n",
        "                prev_status = self.grid_status[new_x, new_y]\n",
        "                if prev_status == self.POI:\n",
        "                    self.grid_status[new_x, new_y] = self.MAP\n",
        "                    self.grid_counts[i][new_x, new_y] += 1\n",
        "                    reward = 15\n",
        "                elif prev_status == self.SURVIVOR:\n",
        "                    self.grid_counts[i][new_x, new_y] += 1\n",
        "                    reward = 100\n",
        "                    detected = self.get_survivor_detection_rate() * self.n_survivors\n",
        "                    reward += 50 * detected\n",
        "                elif prev_status == self.MAP:\n",
        "                    self.grid_counts[i][new_x, new_y] += 1\n",
        "                    reward = -1\n",
        "            else:\n",
        "                self.grid_counts[i][org_x, org_y] += 1\n",
        "                reward = -5\n",
        "        else:\n",
        "            self.grid_counts[i][org_x, org_y] += 1\n",
        "            reward = -2\n",
        "        self.agent_paths[i].append(self.agent_pos[i][:])\n",
        "        if org_x == self.agent_pos[i][0] and org_y == self.agent_pos[i][1]:\n",
        "            self.stuck_counts[i] += 1\n",
        "        else:\n",
        "            self.stuck_counts[i] = 0\n",
        "        self.update_roles()\n",
        "        self.grid_overlay()\n",
        "        self.grid_status_history.append(self.grid_status.copy())\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        done = mapped_poi >= self.n_poi * 0.95 or self.get_survivor_detection_rate() >= 1.0\n",
        "        info = {'survivor_detection_rate': self.get_survivor_detection_rate(),\n",
        "                'coverage': self.get_coverage(),\n",
        "                'adjacency_matrix': self.adjacency_matrix.copy()}\n",
        "        return self.get_agent_obs(), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "        self.agent_roles = ['scout'] * self.n_agents\n",
        "        self.grid_status_history = [self.grid_status.copy()]\n",
        "        self.adjacency_matrix = np.zeros((self.n_agents, self.n_agents))\n",
        "        self.detection_map = np.zeros((self.x_size, self.y_size))\n",
        "        while any(self.grid_status[pos[0], pos[1]] in [self.OBS, self.SURVIVOR] for pos in self.agent_pos):\n",
        "            self.init_grid()\n",
        "            self.init_agent()\n",
        "            self.init_survivors()\n",
        "        self.grid_overlay()\n",
        "        return self.get_agent_obs()\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "def plot_grid(env, agent_paths, agent_roles, episode, info_history):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    grid = env.grid_status_history[-1]\n",
        "    agent_pos = [path[-1] for path in agent_paths]\n",
        "    agent_roles_final = [roles[-1] for roles in agent_roles]\n",
        "    display_grid = np.zeros((env.x_size, env.y_size))\n",
        "    display_grid[grid == env.OBS] = -1\n",
        "    display_grid[grid == env.MAP] = 0.5\n",
        "    display_grid[grid == env.SURVIVOR] = 2\n",
        "    colors = plt.cm.RdYlBu(np.linspace(0, 1, 256))\n",
        "    colors[int(-1 + 127)] = [0, 0, 0, 1]\n",
        "    colors[int(0.5 + 127)] = [1, 0.8, 0.8, 1]\n",
        "    colors[int(2 + 127)] = [1, 1, 1, 1]\n",
        "    custom_cmap = plt.cm.colors.ListedColormap(colors)\n",
        "    ax.imshow(display_grid, cmap=custom_cmap, vmin=-1, vmax=3, origin='lower')\n",
        "    for sx, sy in env.survivor_pos:\n",
        "        ax.plot(sy, sx, 'b*', markersize=15, label='Survivor' if 'Survivor' not in [l.get_label() for l in ax.get_legend_handles_labels()[1]] else '')\n",
        "    colors = ['r', 'g']\n",
        "    for i in range(env.n_agents):\n",
        "        path = np.array(agent_paths[i])\n",
        "        ax.plot(path[:, 1], path[:, 0], color=colors[i], linewidth=2, label=f'Agent {i}')\n",
        "        ax.plot(path[-1, 1], path[-1, 0], 'o', color=colors[i], markersize=8)\n",
        "        ax.text(path[-1, 1] + 0.1, path[-1, 0], agent_roles_final[i], fontsize=10, color=colors[i])\n",
        "    ax.set_xticks(np.arange(env.x_size))\n",
        "    ax.set_yticks(np.arange(env.y_size))\n",
        "    ax.set_title(f'Drone Paths [Episode {episode + 1}]')\n",
        "    ax.grid(True)\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], color='k', marker='s', linestyle='None', markersize=10, label='Obstacles'),\n",
        "        plt.plot([], [], 'b*', markersize=10, label='Survivors')[0],\n",
        "        Line2D([0], [0], color='r', lw=2, label='Agent 0'),\n",
        "        Line2D([0], [0], color='g', lw=2, label='Agent 1'),\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='upper right')\n",
        "    plt.savefig(f'drone_paths_episode_{episode + 1}.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_detection_heatmap(env, episode):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    heatmap = ax.imshow(env.detection_map, cmap='hot', interpolation='nearest', origin='lower')\n",
        "    ax.set_title(f'Survivor Detection Heatmap [Episode {episode + 1}]')\n",
        "    ax.set_xticks(np.arange(env.x_size))\n",
        "    ax.set_yticks(np.arange(env.y_size))\n",
        "    ax.grid(True)\n",
        "    plt.colorbar(heatmap, ax=ax, label='Detection Count')\n",
        "    plt.savefig(f'survivor_detection_heatmap_episode_{episode + 1}.png')\n",
        "    plt.close()\n",
        "\n",
        "def plot_gnn_graph(env, agent_paths, info_history, episode):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    agent_pos = [path[-1] for path in agent_paths]\n",
        "    adjacency_matrix = info_history[-1]['adjacency_matrix']\n",
        "    colors = ['r', 'g']\n",
        "    for i, (x, y) in enumerate(agent_pos):\n",
        "        ax.plot(y, x, 'o', color=colors[i], markersize=10, label=f'Agent {i}')\n",
        "    for i in range(env.n_agents):\n",
        "        for j in range(i + 1, env.n_agents):\n",
        "            if adjacency_matrix[i, j] == 1:\n",
        "                ax.plot([agent_pos[i][1], agent_pos[j][1]],\n",
        "                        [agent_pos[i][0], agent_pos[j][0]], 'k-', alpha=0.5)\n",
        "    ax.set_xlim(-0.5, env.x_size - 0.5)\n",
        "    ax.set_ylim(-0.5, env.y_size - 0.5)\n",
        "    ax.set_xticks(np.arange(env.x_size))\n",
        "    ax.set_yticks(np.arange(env.y_size))\n",
        "    ax.set_title(f'GNN Communication Graph [Episode {episode + 1}]')\n",
        "    ax.grid(True)\n",
        "    ax.legend()\n",
        "    plt.savefig(f'gnn_communication_graph_episode_{episode + 1}.png')\n",
        "    plt.close()\n",
        "\n",
        "def animate_drone_paths(env, agent_paths, agent_roles, episode):\n",
        "    paths = agent_paths\n",
        "    roles = agent_roles\n",
        "    grid = env.grid_status_history[-1]\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xlim(-0.5, env.x_size - 0.5)\n",
        "    ax.set_ylim(-0.5, env.y_size - 0.5)\n",
        "    ax.set_xticks(range(env.x_size))\n",
        "    ax.set_yticks(range(env.y_size))\n",
        "    ax.grid(True)\n",
        "    ax.set_title(f'Drone Paths Animation [Episode {episode + 1}]')\n",
        "    display_grid = np.zeros((env.x_size, env.y_size))\n",
        "    display_grid[grid == env.OBS] = -1\n",
        "    display_grid[grid == env.MAP] = 0.5\n",
        "    display_grid[grid == env.SURVIVOR] = 2\n",
        "    colors = plt.cm.RdYlBu(np.linspace(0, 1, 256))\n",
        "    colors[int(-1 + 127)] = [0, 0, 0, 1]\n",
        "    colors[int(0.5 + 127)] = [1, 0.8, 0.8, 1]\n",
        "    colors[int(2 + 127)] = [1, 1, 1, 1]\n",
        "    custom_cmap = plt.cm.colors.ListedColormap(colors)\n",
        "    ax.imshow(display_grid, cmap=custom_cmap, vmin=-1, vmax=3, origin='lower')\n",
        "    survivor_x, survivor_y = zip(*env.survivor_pos) if env.survivor_pos else ([], [])\n",
        "    survivors = ax.scatter(survivor_y, survivor_x, c='blue', marker='*', s=200, label='Survivors')\n",
        "    colors = ['red', 'green']\n",
        "    agent_plots = [ax.plot([], [], c=colors[i], marker='o', linestyle='-', label=f'Agent {i}')[0]\n",
        "                   for i in range(env.n_agents)]\n",
        "    role_texts = [ax.text(0, 0, '', fontsize=8, color=colors[i], ha='center', va='bottom')\n",
        "                  for i in range(env.n_agents)]\n",
        "    ax.legend()\n",
        "    max_frames = max(len(p) for p in paths) if paths else 0\n",
        "\n",
        "    def update(frame):\n",
        "        artists = [survivors]\n",
        "        for i, plot in enumerate(agent_plots):\n",
        "            if frame < len(paths[i]):\n",
        "                x, y = zip(*paths[i][:frame + 1]) if paths[i][:frame + 1] else ([], [])\n",
        "                plot.set_data(y, x)\n",
        "                if frame < len(roles[i]):\n",
        "                    if x and y:\n",
        "                        role_texts[i].set_position((y[-1], x[-1] + 0.1))\n",
        "                        role_texts[i].set_text(roles[i][frame])\n",
        "                    else:\n",
        "                        role_texts[i].set_text('')\n",
        "                artists.append(plot)\n",
        "                artists.append(role_texts[i])\n",
        "        return artists\n",
        "\n",
        "    ani = FuncAnimation(fig, update, frames=max_frames, interval=200, blit=True)\n",
        "    plt.close(fig)\n",
        "    return ani\n",
        "\n",
        "def download_visualizations(episode):\n",
        "    files = [\n",
        "        f'drone_paths_episode_{episode + 1}.png',\n",
        "        f'survivor_detection_heatmap_episode_{episode + 1}.png',\n",
        "        f'gnn_communication_graph_episode_{episode + 1}.png',\n",
        "        f'animation_gnn_episode_{episode + 1}.html',\n",
        "        'swarm_drone_gnn_performance.png'\n",
        "    ]\n",
        "    zip_filename = 'gnn_visualizations.zip'\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for file in files:\n",
        "            if os.path.exists(file):\n",
        "                zipf.write(file)\n",
        "    from google.colab import files\n",
        "    files.download(zip_filename)\n",
        "\n",
        "def train(q, q_target, q_central, q_central_target, memory, optimizer, optimizer_central, gamma, batch_size, update_iter=5):\n",
        "    for _ in range(update_iter):\n",
        "        s, a, r, s_prime, done_mask, graph_s, graph_s_prime = memory.sample(batch_size)\n",
        "        q_out = q(s)\n",
        "        a = a.view(-1, q_out.shape[1], 1)\n",
        "        q_a = q_out.gather(2, a).squeeze(-1)\n",
        "        for g in graph_s_prime:\n",
        "            g.x = g.x.to(s.device)\n",
        "            g.edge_index = g.edge_index.to(s.device)\n",
        "        batch_s_prime = Batch.from_data_list(graph_s_prime).to(s.device)\n",
        "        with torch.no_grad():\n",
        "            next_actions = q(s_prime).argmax(dim=2, keepdim=True)\n",
        "            q_values = q_central_target(batch_s_prime)\n",
        "            q_selected = q_values.gather(2, next_actions).squeeze(-1)\n",
        "            max_q_prime = q_selected\n",
        "            target = r + gamma * max_q_prime * done_mask\n",
        "        loss_actor = F.smooth_l1_loss(q_a, target.detach())\n",
        "        optimizer.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        optimizer.step()\n",
        "        for g in graph_s:\n",
        "            g.x = g.x.to(s.device)\n",
        "            g.edge_index = g.edge_index.to(s.device)\n",
        "        batch_s = Batch.from_data_list(graph_s).to(s.device)\n",
        "        q_central_out = q_central(batch_s)\n",
        "        q_central_a = q_central_out.gather(2, a).squeeze(-1)\n",
        "        loss_central = F.smooth_l1_loss(q_central_a, target.detach())\n",
        "        optimizer_central.zero_grad()\n",
        "        loss_central.backward()\n",
        "        optimizer_central.step()\n",
        "\n",
        "size = 2\n",
        "fov = 3\n",
        "n_agents = 2\n",
        "n_survivors = 1\n",
        "train_episodes = 500  # Reduced for faster training\n",
        "max_steps = 8\n",
        "batch_size = 16  # Reduced to prevent memory issues\n",
        "gamma = 0.99\n",
        "buffer_limit = 10000\n",
        "log_interval = 100  # Adjusted for fewer episodes\n",
        "max_epsilon = 0.9\n",
        "min_epsilon = 0.1\n",
        "warm_up_steps = 500\n",
        "update_iter = 5\n",
        "lr = 0.001\n",
        "comm_range = 2.0\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "env = GridMultiAgent(x_size=size, y_size=size, fov_x=fov, fov_y=fov, n_agents=n_agents,\n",
        "                     n_survivors=n_survivors, comm_range=comm_range)\n",
        "memory = ReplayBuffer(buffer_limit)\n",
        "q = QNet(env.observation_space, env.action_space, fov_x=fov, fov_y=fov)\n",
        "q_target = QNet(env.observation_space, env.action_space, fov_x=fov, fov_y=fov)\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "agent_obs_size = (fov * fov) + 1 + 3 + 3\n",
        "node_feature_size = agent_obs_size\n",
        "q_central = QCentralGNN(node_feature_size, n_agents, env.action_space[0].n)\n",
        "q_central_target = QCentralGNN(node_feature_size, n_agents, env.action_space[0].n)\n",
        "q_central_target.load_state_dict(q_central.state_dict())\n",
        "optimizer = optim.Adam(q.parameters(), lr=lr)\n",
        "optimizer_central = optim.Adam(q_central.parameters(), lr=lr)\n",
        "time_steps, epsilons, coverage, survivor_detection, total_reward, all_agent_paths, all_agent_roles, all_info = [], [], [], [], [], [], [], []\n",
        "\n",
        "for episode in tqdm(range(train_episodes), desc=\"Training Episodes\"):\n",
        "    score = np.zeros(n_agents)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_step = 0\n",
        "    epsilon = max(min_epsilon, max_epsilon - (max_epsilon - min_epsilon) * (episode / (0.8 * train_episodes)))\n",
        "    episode_paths = [[] for _ in range(n_agents)]\n",
        "    episode_roles = [[] for _ in range(n_agents)]\n",
        "    episode_info = []\n",
        "    while not done and episode_step < max_steps:\n",
        "        state_np = np.array(state, dtype=np.float32)\n",
        "        graph_state = env.get_graph_state()\n",
        "        action = q.sample_action(\n",
        "            torch.tensor(state_np).unsqueeze(0),\n",
        "            epsilon, env.agent_roles, env.grid_status, env.agent_pos, env.x_size, env.y_size,\n",
        "            env.stuck_counts, env.visited_counts\n",
        "        ).data.cpu().numpy()\n",
        "        rewards = np.zeros(n_agents)\n",
        "        next_state = state\n",
        "        step_info = None\n",
        "        for agent_i in env.idx_agents:\n",
        "            if done:\n",
        "                break\n",
        "            step_state, reward, step_done, info = env.step(action[agent_i], agent_i)\n",
        "            rewards[agent_i] = reward\n",
        "            done = done or step_done\n",
        "            next_state = step_state\n",
        "            step_info = info\n",
        "            episode_paths[agent_i].append(env.agent_pos[agent_i][:])\n",
        "            episode_roles[agent_i].append(env.agent_roles[agent_i])\n",
        "        if step_info is not None:\n",
        "            episode_info.append(step_info)\n",
        "        next_graph_state = env.get_graph_state()\n",
        "        memory.put((state, action, rewards, next_state, [done] * n_agents, graph_state, next_graph_state))\n",
        "        score += rewards\n",
        "        state = next_state\n",
        "        episode_step += 1\n",
        "    if memory.size() > warm_up_steps:\n",
        "        train(q, q_target, q_central, q_central_target, memory, optimizer, optimizer_central, gamma, batch_size, update_iter)\n",
        "    if episode % log_interval == 0:\n",
        "        q_target.load_state_dict(q.state_dict())\n",
        "        q_central_target.load_state_dict(q_central.state_dict())\n",
        "    all_agent_paths.append([path[:] for path in episode_paths])\n",
        "    all_agent_roles.append([roles[:] for roles in episode_roles])\n",
        "    all_info.append(episode_info)\n",
        "    time_steps.append(episode_step)\n",
        "    epsilons.append(epsilon)\n",
        "    coverage.append(env.get_coverage())\n",
        "    survivor_detection.append(step_info['survivor_detection_rate'] if step_info else 0.0)\n",
        "    total_reward.append(score.sum())\n",
        "    if episode % log_interval == 0:\n",
        "        print(f'//Episode {episode+1}// Epsilon: {epsilon:.3f}, Steps: {episode_step}, '\n",
        "              f'Coverage (%): {coverage[-1]:.3f}, Survivor Detection (%): {survivor_detection[-1]:.3f}, '\n",
        "              f'Total Reward: {total_reward[-1]:.2f}')\n",
        "    if episode == 499:  # Adjusted for 500 episodes\n",
        "        plot_grid(env, episode_paths, episode_roles, episode, episode_info)\n",
        "        plot_detection_heatmap(env, episode)\n",
        "        plot_gnn_graph(env, episode_paths, episode_info, episode)\n",
        "        ani = animate_drone_paths(env, episode_paths, episode_roles, episode)\n",
        "        with open(f'animation_gnn_episode_{episode + 1}.html', 'w') as f:\n",
        "            f.write(ani.to_jshtml())\n",
        "        download_visualizations(episode)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(coverage)\n",
        "plt.title('Coverage Over Episodes (Swarm Drones with GNN)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Coverage (%)')\n",
        "plt.grid(True)\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(survivor_detection)\n",
        "plt.title('Survivor Detection Rate Over Episodes (Swarm Drones with GNN)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Detection Rate (%)')\n",
        "plt.grid(True)\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(total_reward)\n",
        "plt.title('Total Reward Over Episodes (Swarm Drones with GNN)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('swarm_drone_gnn_performance.png')\n",
        "plt.close()\n",
        "\n",
        "np.savez('metrics_gnn.npz', coverage=coverage, survivor_detection=survivor_detection, total_reward=total_reward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "hB7mDiVpcpkV",
        "outputId": "53720083-05ac-4130-ad34-fe2ab87b792b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZN2at4_ops16div__Tensor_mode4callERNS_6TensorERKS2_St8optionalIN3c1017basic_string_viewIcEEE\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_spline_conv/_basis_cuda.so: undefined symbol: _ZN5torch8autograd12VariableInfoC1ERKN2at6TensorE\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.11/dist-packages/torch_sparse/_spmm_cuda.so: undefined symbol: _ZN5torch8autograd12VariableInfoC1ERKN2at6TensorE\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124, CUDA Available: False\n",
            "torch-geometric imported successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:   0%|          | 0/500 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fov_y' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-6e8cb63eb5f6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0mstate_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mgraph_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_graph_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         action = q.sample_action(\n\u001b[0m\u001b[1;32m    658\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_np\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_roles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-6e8cb63eb5f6>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs, epsilon, roles, grid_status, agent_pos, x_size, y_size, stuck_counts, visited_counts)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_status\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstuck_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisited_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mexploration_bonus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-6e8cb63eb5f6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mspatial_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfov_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfov_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0magent_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mgrid_status\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfov_x\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mfov_y\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfov_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfov_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mspatial_obs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mspatial_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fov_y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should print True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKUWe30RdWHA",
        "outputId": "512ae3a7-56b8-4c59-c4c2-c08a90e21b96"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip cache purge\n",
        "!pip install -q torch==2.6.0+cu124\n",
        "!pip install -q torch-geometric torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n",
        "!pip install -q numpy gym tqdm matplotlib ipython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvqqUyaLdp2O",
        "outputId": "ca9a19a3-6153-4fda-8433-298df6676974"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files removed: 84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"PyTorch Version: {torch.__version__}, CUDA Available: {torch.cuda.is_available()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyMh007Bd_GT",
        "outputId": "0cbcb86b-31c2-4b8f-affb-4e01b718848f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch Version: 2.6.0+cu124, CUDA Available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import random\n",
        "import copy\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from gym import spaces\n",
        "import gym\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "from matplotlib.lines import Line2D\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.data import Data, Batch  # Added Batch import\n",
        "\n",
        "class MultiAgentActionSpace(list):\n",
        "    def __init__(self, agents_action_space):\n",
        "        for x in agents_action_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_action_space)\n",
        "        self._agents_action_space = agents_action_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_action_space.sample() for agent_action_space in self._agents_action_space]\n",
        "\n",
        "class MultiAgentObservationSpace(list):\n",
        "    def __init__(self, agents_observation_space):\n",
        "        for x in agents_observation_space:\n",
        "            assert isinstance(x, gym.spaces.Space)\n",
        "        super().__init__(agents_observation_space)\n",
        "        self._agents_observation_space = agents_observation_space\n",
        "\n",
        "    def sample(self):\n",
        "        return [agent_observation_space.sample() for agent_observation_space in self._agents_observation_space]\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, buffer_limit):\n",
        "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
        "\n",
        "    def put(self, transition):\n",
        "        if len(transition) != 7:\n",
        "            print(f\"Warning: Transition tuple has {len(transition)} elements, expected 7. Transition: {transition}\")\n",
        "            raise ValueError(f\"Transition tuple must have exactly 7 elements, got {len(transition)}\")\n",
        "        self.buffer.append(transition)\n",
        "\n",
        "    def sample(self, n):\n",
        "        mini_batch = random.sample(self.buffer, n)\n",
        "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst, graph_s_lst, graph_s_prime_lst = [], [], [], [], [], [], []\n",
        "\n",
        "        for transition in mini_batch:\n",
        "            s, a, r, s_prime, done, graph_s, graph_s_prime = transition\n",
        "            s_lst.append(s)\n",
        "            a_lst.append(a)\n",
        "            r_lst.append(r)\n",
        "            s_prime_lst.append(s_prime)\n",
        "            done_mask_lst.append((np.ones(len(done)) - done).tolist())\n",
        "            graph_s_lst.append(graph_s)\n",
        "            graph_s_prime_lst.append(graph_s_prime)\n",
        "\n",
        "        return (torch.tensor(np.array(s_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(a_lst), dtype=torch.long),\n",
        "                torch.tensor(np.array(r_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(s_prime_lst), dtype=torch.float),\n",
        "                torch.tensor(np.array(done_mask_lst), dtype=torch.float),\n",
        "                graph_s_lst,\n",
        "                graph_s_prime_lst)\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "class QNet(nn.Module):\n",
        "    def __init__(self, observation_space, action_space, fov_x=3, fov_y=3):\n",
        "        super(QNet, self).__init__()\n",
        "        self.num_agents = len(observation_space)\n",
        "        self.fov_x, self.fov_y = fov_x, fov_y\n",
        "        self.n_actions = action_space[0].n\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(32 * fov_x * fov_y, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.non_spatial_size = 1 + 3 + 3\n",
        "        self.fc_non_spatial = nn.Sequential(\n",
        "            nn.Linear(self.non_spatial_size, 16),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.fc_shared = nn.Sequential(\n",
        "            nn.Linear(64 + 16, 64),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.heads = nn.ModuleList([nn.Linear(64, self.n_actions) for _ in range(self.num_agents)])\n",
        "\n",
        "    def forward(self, obs):\n",
        "        batch_size = obs.shape[0]\n",
        "        spatial_obs = torch.zeros(batch_size, self.num_agents, 1, self.fov_x, self.fov_y, device=obs.device)\n",
        "        for agent_i in range(self.num_agents):\n",
        "            grid_status = obs[:, agent_i, :self.fov_x * self.fov_y].reshape(-1, self.fov_x, self.fov_y)\n",
        "            spatial_obs[:, agent_i, 0] = grid_status\n",
        "        spatial_out = torch.zeros(batch_size, self.num_agents, 64, device=obs.device)\n",
        "        for agent_i in range(self.num_agents):\n",
        "            spatial_out[:, agent_i] = self.cnn(spatial_obs[:, agent_i])\n",
        "        non_spatial = obs[:, :, -self.non_spatial_size:]\n",
        "        non_spatial_out = self.fc_non_spatial(non_spatial.view(-1, self.non_spatial_size)).view(batch_size, self.num_agents, 16)\n",
        "        combined = torch.cat([spatial_out, non_spatial_out], dim=2)\n",
        "        shared_out = self.fc_shared(combined)\n",
        "        q_values = [self.heads[agent_i](shared_out[:, agent_i]).unsqueeze(1) for agent_i in range(self.num_agents)]\n",
        "        return torch.cat(q_values, dim=1)\n",
        "\n",
        "    def sample_action(self, obs, epsilon, roles, grid_status, agent_pos, x_size, y_size, stuck_counts, visited_counts):\n",
        "        out = self.forward(obs)\n",
        "        actions = torch.zeros((out.shape[1],), dtype=torch.long)\n",
        "        exploration_bonus = 50.0\n",
        "        scout_epsilon_boost = 0.4\n",
        "        for agent_i in range(out.shape[1]):\n",
        "            agent_epsilon = epsilon\n",
        "            if roles[agent_i] == 'scout' and stuck_counts[agent_i] > 2:\n",
        "                agent_epsilon = min(1.0, epsilon + scout_epsilon_boost)\n",
        "            if random.random() < agent_epsilon:\n",
        "                if roles[agent_i] == 'relay':\n",
        "                    actions[agent_i] = 8\n",
        "                else:\n",
        "                    valid_actions = []\n",
        "                    poi_actions = []\n",
        "                    low_visit_actions = []\n",
        "                    x, y = agent_pos[agent_i]\n",
        "                    for a in range(8):\n",
        "                        new_x, new_y = x, y\n",
        "                        if a == 0: new_x -= 1\n",
        "                        elif a == 1: new_x += 1\n",
        "                        elif a == 2: new_y -= 1\n",
        "                        elif a == 3: new_y += 1\n",
        "                        elif a == 4: new_x, new_y = x-1, y-1\n",
        "                        elif a == 5: new_x, new_y = x-1, y+1\n",
        "                        elif a == 6: new_x, new_y = x+1, y-1\n",
        "                        elif a == 7: new_x, new_y = x+1, y+1\n",
        "                        if 0 <= new_x < x_size and 0 <= new_y < y_size and grid_status[new_x, new_y] in [0, 3]:\n",
        "                            valid_actions.append(a)\n",
        "                            if grid_status[new_x, new_y] == 0:\n",
        "                                poi_actions.append(a)\n",
        "                            if visited_counts[new_x, new_y] < 2:\n",
        "                                low_visit_actions.append(a)\n",
        "                    actions[agent_i] = random.choice(low_visit_actions or poi_actions or valid_actions) if valid_actions else 8\n",
        "            else:\n",
        "                q_values = out[0, agent_i].clone()\n",
        "                x, y = agent_pos[agent_i]\n",
        "                max_neighbor_rf = obs[0, agent_i, -2].item()\n",
        "                for a in range(8):\n",
        "                    new_x, new_y = x, y\n",
        "                    if a == 0: new_x -= 1\n",
        "                    elif a == 1: new_x += 1\n",
        "                    elif a == 2: new_y -= 1\n",
        "                    elif a == 3: new_y += 1\n",
        "                    elif a == 4: new_x, new_y = x-1, y-1\n",
        "                    elif a == 5: new_x, new_y = x-1, y+1\n",
        "                    elif a == 6: new_x, new_y = x+1, y-1\n",
        "                    elif a == 7: new_x, new_y = x+1, y+1\n",
        "                    if 0 <= new_x < x_size and 0 <= new_y < y_size:\n",
        "                        if grid_status[new_x, new_y] == 0:\n",
        "                            q_values[a] += exploration_bonus\n",
        "                        elif grid_status[new_x, new_y] == 3:\n",
        "                            q_values[a] += exploration_bonus * (2 + max_neighbor_rf)\n",
        "                        if visited_counts[new_x, new_y] < 2:\n",
        "                            q_values[a] += exploration_bonus * 0.5\n",
        "                actions[agent_i] = q_values.argmax().item()\n",
        "        return actions\n",
        "\n",
        "class QCentralGNN(nn.Module):\n",
        "    def __init__(self, node_feature_size, n_agents, n_actions):\n",
        "        super().__init__()\n",
        "        self.n_agents = n_agents\n",
        "        self.n_actions = n_actions\n",
        "        self.gat1 = GATConv(node_feature_size, 64, heads=4, concat=True)\n",
        "        self.gat2 = GATConv(64 * 4, 64, heads=1, concat=True)\n",
        "        self.fc = nn.Linear(64, n_actions)\n",
        "\n",
        "    def forward(self, graph_data):\n",
        "        x, edge_index = graph_data.x, graph_data.edge_index\n",
        "        x = F.relu(self.gat1(x, edge_index))\n",
        "        x = F.relu(self.gat2(x, edge_index))\n",
        "        x = self.fc(x)\n",
        "        return x.view(-1, self.n_agents, self.n_actions)\n",
        "\n",
        "class GridMultiAgent(gym.Env):\n",
        "    metadata = {'render.modes': ['console']}\n",
        "    XM, XP, YM, YP, XMYM, XMYP, XPYM, XPYP, STAY = range(9)\n",
        "    OOE, OBS, POI, MAP, AGT, SURVIVOR = -2, -1, 0, 1, 2, 3\n",
        "\n",
        "    def __init__(self, x_size=10, y_size=10, fov_x=3, fov_y=3, n_agents=3, n_survivors=5, comm_range=5.0):\n",
        "        super().__init__()\n",
        "        self.x_size = x_size\n",
        "        self.y_size = y_size\n",
        "        self.n_agents = n_agents\n",
        "        self.idx_agents = list(range(n_agents))\n",
        "        self.n_survivors = n_survivors\n",
        "        self.fov_x = fov_x\n",
        "        self.fov_y = fov_y\n",
        "        self.comm_range = comm_range\n",
        "        self.max_relays = n_agents // 2\n",
        "        self.agent_roles = ['scout'] * n_agents\n",
        "        self.stuck_counts = [0] * n_agents\n",
        "        self.visited_counts = np.zeros((x_size, y_size))\n",
        "        self.survivor_pos = []\n",
        "        self.agent_paths = [[] for _ in range(n_agents)]\n",
        "        self.obstacle_move_prob = 0.05\n",
        "        self.survivor_move_prob = 0.05\n",
        "        self.grid_status_history = []\n",
        "        self.adjacency_matrix = np.zeros((n_agents, n_agents))\n",
        "        n_actions = 9\n",
        "        self.action_space = MultiAgentActionSpace([spaces.Discrete(n_actions) for _ in range(n_agents)])\n",
        "        obs_size = (fov_x * fov_y) + 1 + 3 + 3\n",
        "        self.obs_low = np.concatenate([np.ones(fov_x * fov_y) * self.OOE, [0, 0, 0, 0, 0, 0, 0]], dtype=np.float32)\n",
        "        self.obs_high = np.concatenate([np.ones(fov_x * fov_y) * self.SURVIVOR, [1, 1, 1, 1, n_agents, 1, 3]], dtype=np.float32)\n",
        "        self.observation_space = MultiAgentObservationSpace([\n",
        "            spaces.Box(self.obs_low, self.obs_high, dtype=np.float32) for _ in range(n_agents)\n",
        "        ])\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "\n",
        "    def init_grid(self):\n",
        "        self.grid_status = np.zeros((self.x_size, self.y_size))\n",
        "        n_obstacle = random.randrange(0, int(self.x_size * self.y_size * 0.2))\n",
        "        for _ in range(n_obstacle):\n",
        "            x, y = random.randrange(1, self.x_size - 1), random.randrange(1, self.y_size - 1)\n",
        "            self.grid_status[x, y] = self.OBS\n",
        "        self.grid_counts = np.tile(self.grid_status, (self.n_agents, 1, 1)).reshape(self.n_agents, self.x_size, self.y_size)\n",
        "        self.n_poi = self.x_size * self.y_size - np.count_nonzero(self.grid_status)\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "        self.grid_status_history = [self.grid_status.copy()]\n",
        "        self.visited_counts = np.zeros((self.x_size, self.y_size))\n",
        "\n",
        "    def init_agent(self):\n",
        "        self.agent_pos = []\n",
        "        self.agent_paths = [[] for _ in range(self.n_agents)]\n",
        "        for i in range(self.n_agents):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_agents_status[x, y] == self.POI:\n",
        "                    self.agent_pos.append([x, y])\n",
        "                    self.grid_agents_status[x, y] = self.AGT\n",
        "                    self.agent_paths[i].append([x, y])\n",
        "                    self.visited_counts[x, y] += 1\n",
        "                    break\n",
        "        self.stuck_counts = [0] * n_agents\n",
        "\n",
        "    def init_survivors(self):\n",
        "        self.survivor_pos = []\n",
        "        for _ in range(self.n_survivors):\n",
        "            while True:\n",
        "                x, y = random.randrange(0, self.x_size), random.randrange(0, self.y_size)\n",
        "                if self.grid_status[x, y] == self.POI and [x, y] not in self.agent_pos:\n",
        "                    self.survivor_pos.append([x, y])\n",
        "                    self.grid_status[x, y] = self.SURVIVOR\n",
        "                    break\n",
        "\n",
        "    def compute_network(self):\n",
        "        self.adjacency_matrix = np.zeros((self.n_agents, self.n_agents))\n",
        "        for i in range(self.n_agents):\n",
        "            for j in range(i + 1, self.n_agents):\n",
        "                dist = np.sqrt((self.agent_pos[i][0] - self.agent_pos[j][0])**2 +\n",
        "                              (self.agent_pos[i][1] - self.agent_pos[j][1])**2)\n",
        "                if dist <= self.comm_range:\n",
        "                    self.adjacency_matrix[i, j] = 1\n",
        "                    self.adjacency_matrix[j, i] = 1\n",
        "\n",
        "    def get_neighbor_messages(self, agent_idx):\n",
        "        neighbors = np.where(self.adjacency_matrix[agent_idx] == 1)[0]\n",
        "        n_neighbors = len(neighbors)\n",
        "        max_rf = 0.0\n",
        "        neighbor_role = 0\n",
        "        for neighbor in neighbors:\n",
        "            rf_signals = [self.get_rf_signal(self.agent_pos[neighbor], s) for s in self.survivor_pos]\n",
        "            max_rf = max(max_rf, max(rf_signals, default=0.0))\n",
        "            role_map = {'scout': 0, 'verification': 1, 'relay': 2}\n",
        "            neighbor_role = max(neighbor_role, role_map[self.agent_roles[neighbor]])\n",
        "        return n_neighbors, max_rf, neighbor_role\n",
        "\n",
        "    def move_obstacles(self):\n",
        "        new_grid = self.grid_status.copy()\n",
        "        obstacle_pos = [(i, j) for i in range(self.x_size) for j in range(self.y_size) if self.grid_status[i, j] == self.OBS]\n",
        "        for x, y in obstacle_pos:\n",
        "            if random.random() < self.obstacle_move_prob:\n",
        "                directions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "                random.shuffle(directions)\n",
        "                for dx, dy in directions:\n",
        "                    new_x, new_y = x + dx, y + dy\n",
        "                    if (0 <= new_x < self.x_size and 0 <= new_y < self.y_size and\n",
        "                            new_grid[new_x, new_y] == self.POI and [new_x, new_y] not in self.agent_pos):\n",
        "                        new_grid[x, y] = self.POI\n",
        "                        new_grid[new_x, new_y] = self.OBS\n",
        "                        break\n",
        "        self.grid_status = new_grid\n",
        "\n",
        "    def move_survivors(self):\n",
        "        new_survivor_pos = []\n",
        "        for x, y in self.survivor_pos:\n",
        "            if random.random() < self.survivor_move_prob:\n",
        "                directions = [(-1, 0), (1, 0), (0, -1), (0, 1), (-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
        "                random.shuffle(directions)\n",
        "                moved = False\n",
        "                for dx, dy in directions:\n",
        "                    new_x, new_y = x + dx, y + dy\n",
        "                    if (0 <= new_x < self.x_size and 0 <= new_y < self.y_size and\n",
        "                            self.grid_status[new_x, new_y] == self.POI and\n",
        "                            [new_x, new_y] not in self.agent_pos):\n",
        "                        self.grid_status[x, y] = self.POI\n",
        "                        self.grid_status[new_x, new_y] = self.SURVIVOR\n",
        "                        new_survivor_pos.append([new_x, new_y])\n",
        "                        moved = True\n",
        "                        break\n",
        "                if not moved:\n",
        "                    new_survivor_pos.append([x, y])\n",
        "            else:\n",
        "                new_survivor_pos.append([x, y])\n",
        "        self.survivor_pos = new_survivor_pos\n",
        "\n",
        "    def grid_overlay(self):\n",
        "        self.grid_agents_status = copy.deepcopy(self.grid_status)\n",
        "        for i in range(self.n_agents):\n",
        "            x, y = self.agent_pos[i]\n",
        "            self.grid_agents_status[x, y] = self.AGT\n",
        "\n",
        "    def get_rf_signal(self, agent_pos, survivor_pos):\n",
        "        distance = np.sqrt((agent_pos[0] - survivor_pos[0])**2 + (agent_pos[1] - survivor_pos[1])**2)\n",
        "        return min(1.0, 1.0 / (distance + 0.5))\n",
        "\n",
        "    def get_graph_state(self):\n",
        "        agent_obs = self.get_agent_obs()\n",
        "        node_features = np.array(agent_obs, dtype=np.float32)\n",
        "        edge_index = []\n",
        "        for i in range(self.n_agents):\n",
        "            for j in range(self.n_agents):\n",
        "                if self.adjacency_matrix[i, j] == 1:\n",
        "                    edge_index.append([i, j])\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous() if edge_index else torch.empty((2, 0), dtype=torch.long)\n",
        "        graph_data = Data(\n",
        "            x=torch.tensor(node_features, dtype=torch.float),\n",
        "            edge_index=edge_index\n",
        "        )\n",
        "        return graph_data\n",
        "\n",
        "    def get_agent_obs(self):\n",
        "        self.compute_network()\n",
        "        self.agent_obs = []\n",
        "        for agent in range(self.n_agents):\n",
        "            single_obs = np.ones((self.fov_x, self.fov_y)) * self.OOE\n",
        "            x, y = self.agent_pos[agent]\n",
        "            for i in range(self.fov_x):\n",
        "                for j in range(self.fov_y):\n",
        "                    obs_x = x + (i - self.fov_x // 2)\n",
        "                    obs_y = y + (j - self.fov_y // 2)\n",
        "                    if 0 <= obs_x < self.x_size and 0 <= obs_y < self.y_size:\n",
        "                        single_obs[i][j] = self.grid_agents_status[obs_x][obs_y]\n",
        "            single_obs_flat = single_obs.flatten()\n",
        "            rf_signals = [self.get_rf_signal(self.agent_pos[agent], s) for s in self.survivor_pos]\n",
        "            rf_signal = max(rf_signals, default=0.0)\n",
        "            role_encoding = {'scout': [1, 0, 0], 'verification': [0, 1, 0], 'relay': [0, 0, 1]}\n",
        "            role_vec = role_encoding[self.agent_roles[agent]]\n",
        "            n_neighbors, max_neighbor_rf, neighbor_role = self.get_neighbor_messages(agent)\n",
        "            obs = np.concatenate([single_obs_flat, [rf_signal], role_vec, [n_neighbors, max_neighbor_rf, neighbor_role]])\n",
        "            self.agent_obs.append(obs.astype(np.float32))\n",
        "        return self.agent_obs\n",
        "\n",
        "    def update_roles(self):\n",
        "        relay_count = sum(1 for r in self.agent_roles if r == 'relay')\n",
        "        detected_survivors = sum(1 for s in self.survivor_pos if\n",
        "                                 max(self.get_rf_signal([ax, ay], s) for ax, ay in self.agent_pos) > 0.6)\n",
        "        for i in range(self.n_agents):\n",
        "            rf_signals = [self.get_rf_signal(self.agent_pos[i], s) for s in self.survivor_pos]\n",
        "            max_rf = max(rf_signals, default=0.0)\n",
        "            n_neighbors, max_neighbor_rf, _ = self.get_neighbor_messages(i)\n",
        "            if (max_rf > 0.6 or max_neighbor_rf > 0.6) and self.agent_roles[i] != 'verification':\n",
        "                self.agent_roles[i] = 'verification'\n",
        "            elif detected_survivors < self.n_survivors and self.agent_roles[i] != 'relay' and relay_count < self.max_relays:\n",
        "                self.agent_roles[i] = 'relay'\n",
        "                relay_count += 1\n",
        "            elif detected_survivors < self.n_survivors and self.agent_roles[i] != 'scout':\n",
        "                self.agent_roles[i] = 'scout'\n",
        "\n",
        "    def get_coverage(self):\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        return mapped_poi / self.n_poi if self.n_poi > 0 else 0.0\n",
        "\n",
        "    def get_survivor_detection_rate(self):\n",
        "        detected = 0\n",
        "        for s in self.survivor_pos:\n",
        "            for ax, ay in self.agent_pos:\n",
        "                if self.get_rf_signal([ax, ay], s) > 0.6:\n",
        "                    detected += 1\n",
        "                    break\n",
        "        return detected / self.n_survivors if self.n_survivors > 0 else 0.0\n",
        "\n",
        "    def step(self, action, i):\n",
        "        org_x, org_y = self.agent_pos[i][0], self.agent_pos[i][1]\n",
        "        reward = 0\n",
        "        action = int(action)\n",
        "        new_x, new_y = org_x, org_y\n",
        "        if action == self.XM:\n",
        "            new_x -= 1\n",
        "        elif action == self.XP:\n",
        "            new_x += 1\n",
        "        elif action == self.YM:\n",
        "            new_y -= 1\n",
        "        elif action == self.YP:\n",
        "            new_y += 1\n",
        "        elif action == self.XMYM:\n",
        "            new_x -= 1\n",
        "            new_y -= 1\n",
        "        elif action == self.XMYP:\n",
        "            new_x -= 1\n",
        "            new_y += 1\n",
        "        elif action == self.XPYM:\n",
        "            new_x += 1\n",
        "            new_y -= 1\n",
        "        elif action == self.XPYP:\n",
        "            new_x += 1\n",
        "            new_y += 1\n",
        "        elif action == self.STAY:\n",
        "            rf_signals = [self.get_rf_signal(self.agent_pos[i], s) for s in self.survivor_pos]\n",
        "            max_rf = max(rf_signals, default=0.0)\n",
        "            reward = 5 if self.agent_roles[i] == 'relay' and max_rf > 0.5 else -2\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid action={action}\")\n",
        "        if (0 <= new_x < self.x_size and 0 <= new_y < self.y_size and\n",
        "            self.grid_status[new_x, new_y] not in [self.OBS]):\n",
        "            collision = False\n",
        "            for j in range(self.n_agents):\n",
        "                if i != j and self.agent_pos[j][0] == new_x and self.agent_pos[j][1] == new_y:\n",
        "                    collision = True\n",
        "                    break\n",
        "            if not collision:\n",
        "                self.agent_pos[i] = [new_x, new_y]\n",
        "                self.visited_counts[new_x, new_y] += 1\n",
        "                prev_status = self.grid_status[new_x, new_y]\n",
        "                if prev_status == self.POI:\n",
        "                    self.grid_status[new_x, new_y] = self.MAP\n",
        "                    self.grid_counts[i][new_x, new_y] += 1\n",
        "                    reward = 15\n",
        "                elif prev_status == self.SURVIVOR:\n",
        "                    self.grid_counts[i][new_x, new_y] += 1\n",
        "                    reward = 100\n",
        "                    detected = self.get_survivor_detection_rate() * self.n_survivors\n",
        "                    reward += 50 * detected\n",
        "                elif prev_status == self.MAP:\n",
        "                    self.grid_counts[i][new_x, new_y] += 1\n",
        "                    reward = -1\n",
        "            else:\n",
        "                self.grid_counts[i][org_x, org_y] += 1\n",
        "                reward = -5\n",
        "        else:\n",
        "            self.grid_counts[i][org_x, org_y] += 1\n",
        "            reward = -2\n",
        "        self.agent_paths[i].append(self.agent_pos[i][:])\n",
        "        if org_x == self.agent_pos[i][0] and org_y == self.agent_pos[i][1]:\n",
        "            self.stuck_counts[i] += 1\n",
        "        else:\n",
        "            self.stuck_counts[i] = 0\n",
        "        self.move_obstacles()\n",
        "        self.move_survivors()\n",
        "        self.update_roles()\n",
        "        self.grid_overlay()\n",
        "        self.grid_status_history.append(self.grid_status.copy())\n",
        "        mapped_poi = np.count_nonzero(self.grid_status == self.MAP)\n",
        "        done = mapped_poi >= self.n_poi * 0.95 or self.get_survivor_detection_rate() >= 1.0\n",
        "        info = {'survivor_detection_rate': self.get_survivor_detection_rate(),\n",
        "                'coverage': self.get_coverage(),\n",
        "                'adjacency_matrix': self.adjacency_matrix.copy()}\n",
        "        return self.get_agent_obs(), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.init_grid()\n",
        "        self.init_agent()\n",
        "        self.init_survivors()\n",
        "        self.agent_roles = ['scout'] * self.n_agents\n",
        "        self.grid_status_history = [self.grid_status.copy()]\n",
        "        self.adjacency_matrix = np.zeros((self.n_agents, self.n_agents))\n",
        "        while any(self.grid_status[pos[0], pos[1]] in [self.OBS, self.SURVIVOR] for pos in self.agent_pos):\n",
        "            self.init_grid()\n",
        "            self.init_agent()\n",
        "            self.init_survivors()\n",
        "        self.grid_overlay()\n",
        "        return self.get_agent_obs()\n",
        "\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "def train(q, q_target, q_central, q_central_target, memory, optimizer, optimizer_central, gamma, batch_size, update_iter=10):\n",
        "    for _ in range(update_iter):\n",
        "        s, a, r, s_prime, done_mask, graph_s, graph_s_prime = memory.sample(batch_size)\n",
        "        q_out = q(s)\n",
        "        a = a.view(-1, q_out.shape[1], 1)\n",
        "        q_a = q_out.gather(2, a).squeeze(-1)\n",
        "\n",
        "        # Process batched graph data for GNN (s_prime)\n",
        "        for g in graph_s_prime:\n",
        "            g.x = g.x.to(s.device)\n",
        "            g.edge_index = g.edge_index.to(s.device)\n",
        "        batch_s_prime = Batch.from_data_list(graph_s_prime).to(s.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_actions = q(s_prime).argmax(dim=2, keepdim=True)\n",
        "            q_values = q_central_target(batch_s_prime)\n",
        "            q_selected = q_values.gather(2, next_actions).squeeze(-1)\n",
        "            max_q_prime = q_selected\n",
        "            target = r + gamma * max_q_prime * done_mask\n",
        "\n",
        "        loss_actor = F.smooth_l1_loss(q_a, target.detach())\n",
        "        optimizer.zero_grad()\n",
        "        loss_actor.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Train GNN (s)\n",
        "        for g in graph_s:\n",
        "            g.x = g.x.to(s.device)\n",
        "            g.edge_index = g.edge_index.to(s.device)\n",
        "        batch_s = Batch.from_data_list(graph_s).to(s.device)\n",
        "\n",
        "        q_central_out = q_central(batch_s)\n",
        "        q_central_a = q_central_out.gather(2, a).squeeze(-1)\n",
        "        loss_central = F.smooth_l1_loss(q_central_a, target.detach())\n",
        "        optimizer_central.zero_grad()\n",
        "        loss_central.backward()\n",
        "        optimizer_central.step()\n",
        "\n",
        "def animate_grid(env, agent_paths, episode, info_history, save_path='swarm_drone_gnn_animation.mp4'):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    cmap = plt.colormaps['tab10']\n",
        "    def update(frame):\n",
        "        ax.clear()\n",
        "        grid = env.grid_status_history[frame] if frame < len(env.grid_status_history) else env.grid_status_history[-1]\n",
        "        agent_pos = [path[frame] if frame < len(path) else path[-1] for path in agent_paths]\n",
        "        adjacency_matrix = info_history[frame]['adjacency_matrix'] if frame < len(info_history) else np.zeros((env.n_agents, env.n_agents))\n",
        "        display_grid = np.zeros((env.x_size, env.y_size))\n",
        "        display_grid[grid == env.OBS] = -1\n",
        "        display_grid[grid == env.MAP] = 1\n",
        "        display_grid[grid == env.SURVIVOR] = 2\n",
        "        for i, (x, y) in enumerate(agent_pos):\n",
        "            display_grid[x, y] = 3 + i\n",
        "        cax = ax.imshow(display_grid, cmap=cmap, vmin=-1, vmax=3 + env.n_agents)\n",
        "        for i in range(env.n_agents):\n",
        "            for j in range(i + 1, env.n_agents):\n",
        "                if adjacency_matrix[i, j] == 1:\n",
        "                    ax.plot([agent_pos[i][1], agent_pos[j][1]],\n",
        "                            [agent_pos[i][0], agent_pos[j][0]], 'k-', alpha=0.5)\n",
        "        ax.set_xticks(np.arange(env.x_size))\n",
        "        ax.set_yticks(np.arange(env.y_size))\n",
        "        ax.set_title(f'Episode {episode + 1}, Step {frame + 1}')\n",
        "        ax.grid(True)\n",
        "        legend_elements = [\n",
        "            Line2D([0], [0], color=cmap(-1), lw=4, label='Obstacle'),\n",
        "            Line2D([0], [0], color=cmap(1), lw=4, label='Mapped'),\n",
        "            Line2D([0], [0], color=cmap(2), lw=4, label='Survivor'),\n",
        "        ]\n",
        "        for i in range(env.n_agents):\n",
        "            legend_elements.append(Line2D([0], [0], color=cmap(3 + i), lw=4, label=f'Agent {i + 1}'))\n",
        "        legend_elements.append(Line2D([0], [0], color='k', lw=1, label='Comm Link'))\n",
        "        ax.legend(handles=legend_elements, loc='upper right')\n",
        "        return cax,\n",
        "    frames = min(len(env.grid_status_history), max(len(path) for path in agent_paths))\n",
        "    ani = FuncAnimation(fig, update, frames=frames, interval=200, blit=False)\n",
        "    ani.save(save_path, writer='ffmpeg')\n",
        "    plt.close()\n",
        "\n",
        "# Main training loop\n",
        "size = 10\n",
        "fov = 3\n",
        "n_agents = 3\n",
        "n_survivors = 5\n",
        "train_episodes = 2000\n",
        "max_steps = size * size * 3\n",
        "batch_size = 32\n",
        "gamma = 0.99\n",
        "buffer_limit = 50000\n",
        "log_interval = 200\n",
        "max_epsilon = 0.9\n",
        "min_epsilon = 0.1\n",
        "warm_up_steps = 1000\n",
        "update_iter = 10\n",
        "lr = 0.001\n",
        "comm_range = 5.0\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "env = GridMultiAgent(x_size=size, y_size=size, fov_x=fov, fov_y=fov, n_agents=n_agents,\n",
        "                     n_survivors=n_survivors, comm_range=comm_range)\n",
        "memory = ReplayBuffer(buffer_limit)\n",
        "q = QNet(env.observation_space, env.action_space, fov_x=fov, fov_y=fov)\n",
        "q_target = QNet(env.observation_space, env.action_space, fov_x=fov, fov_y=fov)\n",
        "q_target.load_state_dict(q.state_dict())\n",
        "agent_obs_size = (fov * fov) + 1 + 3 + 3\n",
        "node_feature_size = agent_obs_size\n",
        "q_central = QCentralGNN(node_feature_size, n_agents, env.action_space[0].n)\n",
        "q_central_target = QCentralGNN(node_feature_size, n_agents, env.action_space[0].n)\n",
        "q_central_target.load_state_dict(q_central.state_dict())\n",
        "optimizer = optim.Adam(q.parameters(), lr=lr)\n",
        "optimizer_central = optim.Adam(q_central.parameters(), lr=lr)\n",
        "time_steps, epsilons, coverage, survivor_detection, total_reward, all_agent_paths, all_agent_roles, all_info = [], [], [], [], [], [], [], []\n",
        "\n",
        "for episode in tqdm(range(train_episodes), desc=\"Training Episodes\"):\n",
        "    score = np.zeros(n_agents)\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    episode_step = 0\n",
        "    epsilon = max(min_epsilon, max_epsilon - (max_epsilon - min_epsilon) * (episode / (0.8 * train_episodes)))\n",
        "    episode_paths = [[] for _ in range(n_agents)]\n",
        "    episode_roles = [[] for _ in range(n_agents)]\n",
        "    episode_info = []\n",
        "    while not done and episode_step < max_steps:\n",
        "        state_np = np.array(state, dtype=np.float32)\n",
        "        graph_state = env.get_graph_state()\n",
        "        action = q.sample_action(\n",
        "            torch.tensor(state_np).unsqueeze(0),\n",
        "            epsilon, env.agent_roles, env.grid_status, env.agent_pos, env.x_size, env.y_size,\n",
        "            env.stuck_counts, env.visited_counts\n",
        "        ).data.cpu().numpy()\n",
        "        rewards = np.zeros(n_agents)\n",
        "        next_state = state\n",
        "        step_info = None\n",
        "        for agent_i in env.idx_agents:\n",
        "            if done:\n",
        "                break\n",
        "            step_state, reward, step_done, info = env.step(action[agent_i], agent_i)\n",
        "            rewards[agent_i] = reward\n",
        "            done = done or step_done\n",
        "            next_state = step_state\n",
        "            step_info = info\n",
        "            episode_paths[agent_i].append(env.agent_pos[agent_i][:])\n",
        "            episode_roles[agent_i].append(env.agent_roles[agent_i])\n",
        "        if step_info is not None:\n",
        "            episode_info.append(step_info)\n",
        "        next_graph_state = env.get_graph_state()\n",
        "        memory.put((state, action, rewards, next_state, [done] * n_agents, graph_state, next_graph_state))\n",
        "        score += rewards\n",
        "        state = next_state\n",
        "        episode_step += 1\n",
        "    if memory.size() > warm_up_steps:\n",
        "        train(q, q_target, q_central, q_central_target, memory, optimizer, optimizer_central, gamma, batch_size, update_iter)\n",
        "    if episode % log_interval == 0:\n",
        "        q_target.load_state_dict(q.state_dict())\n",
        "        q_central_target.load_state_dict(q_central.state_dict())\n",
        "        animate_grid(env, episode_paths, episode, episode_info, save_path=f'swarm_drone_gnn_animation_episode_{episode}.mp4')\n",
        "    all_agent_paths.append([path[:] for path in episode_paths])\n",
        "    all_agent_roles.append([roles[:] for roles in episode_roles])\n",
        "    all_info.append(episode_info)\n",
        "    time_steps.append(episode_step)\n",
        "    epsilons.append(epsilon)\n",
        "    coverage.append(env.get_coverage())\n",
        "    survivor_detection.append(step_info['survivor_detection_rate'] if step_info else 0.0)\n",
        "    total_reward.append(score.sum())\n",
        "    if episode % log_interval == 0:\n",
        "        print(f'//Episode {episode+1}// Epsilon: {epsilon:.3f}, Steps: {episode_step}, '\n",
        "              f'Coverage (%): {coverage[-1]:.3f}, Survivor Detection (%): {survivor_detection[-1]:.3f}, '\n",
        "              f'Total Reward: {total_reward[-1]:.2f}')\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(coverage)\n",
        "plt.title('Coverage Over Episodes (Swarm Drones with GNN)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Coverage (%)')\n",
        "plt.grid(True)\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(survivor_detection)\n",
        "plt.title('Survivor Detection Rate Over Episodes (Swarm Drones with GNN)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Detection Rate (%)')\n",
        "plt.grid(True)\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(total_reward)\n",
        "plt.title('Total Reward Over Episodes (Swarm Drones with GNN)')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Reward')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.savefig('swarm_drone_gnn_performance.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR6nYiOJeK_x",
        "outputId": "94a97c27-0a7f-4e09-b262-ed78fe0ea0a2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:   0%|          | 1/2000 [01:37<54:01:29, 97.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 1// Epsilon: 0.900, Steps: 300, Coverage (%): 0.849, Survivor Detection (%): 0.800, Total Reward: 97095.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  10%|█         | 201/2000 [09:50<15:29:22, 31.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 201// Epsilon: 0.800, Steps: 300, Coverage (%): 0.759, Survivor Detection (%): 0.600, Total Reward: 195142.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  20%|██        | 401/2000 [18:11<14:32:48, 32.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 401// Epsilon: 0.700, Steps: 300, Coverage (%): 0.700, Survivor Detection (%): 0.600, Total Reward: 111790.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  30%|███       | 601/2000 [26:52<12:51:09, 33.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 601// Epsilon: 0.600, Steps: 300, Coverage (%): 0.798, Survivor Detection (%): 0.400, Total Reward: 108372.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  40%|████      | 801/2000 [35:40<10:20:51, 31.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 801// Epsilon: 0.500, Steps: 300, Coverage (%): 0.707, Survivor Detection (%): 0.400, Total Reward: 111343.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  50%|█████     | 1001/2000 [44:37<8:34:12, 30.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 1001// Epsilon: 0.400, Steps: 300, Coverage (%): 0.604, Survivor Detection (%): 0.600, Total Reward: 204132.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  60%|██████    | 1201/2000 [53:42<6:55:04, 31.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 1201// Epsilon: 0.300, Steps: 300, Coverage (%): 0.670, Survivor Detection (%): 0.600, Total Reward: 137831.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  70%|███████   | 1401/2000 [1:02:54<5:10:42, 31.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 1401// Epsilon: 0.200, Steps: 300, Coverage (%): 0.619, Survivor Detection (%): 0.400, Total Reward: 99823.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  80%|████████  | 1601/2000 [1:11:51<3:25:51, 30.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 1601// Epsilon: 0.100, Steps: 300, Coverage (%): 0.429, Survivor Detection (%): 0.200, Total Reward: 52748.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes:  90%|█████████ | 1801/2000 [1:20:55<1:40:15, 30.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "//Episode 1801// Epsilon: 0.100, Steps: 300, Coverage (%): 0.711, Survivor Detection (%): 0.200, Total Reward: 60761.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Episodes: 100%|██████████| 2000/2000 [1:28:15<00:00,  2.65s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "otKXKoVtek91"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}